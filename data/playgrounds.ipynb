{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629aefd4",
   "metadata": {},
   "source": [
    "Midterm Report:\n",
    "Week 1\n",
    "1. Benchmarks: Gemma 3 (4 Billion) and Llama 3 (Llama 3.2 3B)\n",
    "2. Dataset: UCBerkely\n",
    "\n",
    "\n",
    "Week 2:\n",
    "1. Finetuning of Gemma 3 and Llama 3.2 3B with LoRA , and if necesasary QLoRA (maybe for endterm report). [Finetuning Benchmarks]\n",
    "2. Evaluation of other opensource models with more parameters (low priority)\n",
    "\n",
    "\n",
    "Week 3:\n",
    "TBD\n",
    "1. Start working on report and final exam.\n",
    "2. Prompting strategies for Gemma 3 and Llama 3.2 3B which can improve performance.\n",
    "3. Again perform evaluation on unfinetuned performed models with improved prompting strategies. (low priority)\n",
    "\n",
    "\n",
    "After Midterm Exam:\n",
    "1. We will focus on improving parameters of LoRA and QLoRA.\n",
    "2. Also work on improvements suggested in midterm report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d43d12",
   "metadata": {},
   "source": [
    "<!-- # Proposed Schema\n",
    "\n",
    "Input schema for finetuningb and ground truth:\n",
    "```\n",
    "{\n",
    "  \"overall\": {\"label\": \"none|abusive|hateful\", \"score\": 0.0},\n",
    "  \"facets\": {\n",
    "    \"insult\": 0, \"humiliate\": 0, \"dehumanize\": 0,\n",
    "    \"violence\": 0, \"genocide\": 0,\n",
    "    \"respect\": 0, \"status\": 0, \"attack_defend\": 0, \"sentiment\": 0\n",
    "  },\n",
    "  \"targets\": {\n",
    "    \"race_asian\": false, \"race_black\": false, \"race_white\": false, \"race_latinx\": false,\n",
    "    \"religion_muslim\": false, \"religion_jewish\": false, \"religion_hindu\": false, \"religion_buddhist\": false,\n",
    "    \"gender_men\": false, \"gender_women\": false, \"gender_transgender\": false, \"gender_non_binary\": false,\n",
    "    \"sexuality_gay\": false, \"sexuality_lesbian\": false, \"sexuality_bisexual\": false, \"sexuality_straight\": false,\n",
    "    \"disability_physical\": false, \"disability_cognitive\": false, \"origin_immigrant\": false, \"other\": false\n",
    "  },\n",
    "  \"explanation\" : \"text\",\n",
    "}``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb711d",
   "metadata": {},
   "source": [
    "# Gold Standard Dataset schema\n",
    "\n",
    "```\n",
    "{\n",
    "  // --- Core Identifiers ---\n",
    "  \"comment_id\": \"131169.0\", \n",
    "  \"text\": \"The original text content of the social media comment.\",\n",
    "\n",
    "  // --- Holistic Overall Assessment ---\n",
    "  \"overall\": {\n",
    "    \"label\": \"hateful\",             // A categorical label derived from the paper's official thresholds.\n",
    "                                    // - \"hateful\": If hate_speech_score > 0.5\n",
    "                                    // - \"supportive\": If hate_speech_score < -1.0\n",
    "                                    // - \"neutral\": If between -1.0 and 0.5 (inclusive)\n",
    "    \"hate_speech_score\": 1.52       // Ground truth \n",
    "  },\n",
    "\n",
    "  // --- Multi-Label Facet Analysis (The Core of the Task) ---\n",
    "  \"facets\": {\n",
    "    \"sentiment\": 3.0,               \n",
    "    \"respect\": 3.0,                 \n",
    "    \"insult\": 3.0,                  \n",
    "    \"humiliate\": 3.0,\n",
    "    \"status\": 0.0,\n",
    "    \"dehumanize\": 3.0,\n",
    "    \"violence\": 0.0,\n",
    "    \"genocide\": 0.0,\n",
    "    \"attack_defend\": 3.0,\n",
    "    \"hatespeech\": 1.0               \n",
    "  },\n",
    "\n",
    "  // --- Multi-Label Target Group Identification ---\n",
    "  \"targets\": {\n",
    "    \"race_asian\": true,             // A dictionary of booleans. `true` means the comment targets this group.\n",
    "    \"race_black\": false,            // This allows a single comment to target multiple groups simultaneously.\n",
    "    \"religion_muslim\": true,\n",
    "    \"gender_women\": false\n",
    "    // ... and so on for all other possible target groups.\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a5fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset into a pandas DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinh2\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 135556 rows and 131 columns.\n",
      "Applying the transformation to each row of the DataFrame...\n",
      "\n",
      "Example of a processed record:\n",
      "{\n",
      "  \"comment_id\": 47777,\n",
      "  \"text\": \"Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \\\"Titanic\\\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\",\n",
      "  \"overall\": {\n",
      "    \"label\": \"supportive\",\n",
      "    \"hate_speech_score\": -3.9\n",
      "  },\n",
      "  \"facets\": {\n",
      "    \"sentiment\": 0.0,\n",
      "    \"respect\": 0.0,\n",
      "    \"insult\": 0.0,\n",
      "    \"humiliate\": 0.0,\n",
      "    \"status\": 2.0,\n",
      "    \"dehumanize\": 0.0,\n",
      "    \"violence\": 0.0,\n",
      "    \"genocide\": 0.0,\n",
      "    \"attack_defend\": 0.0,\n",
      "    \"hatespeech\": 0.0\n",
      "  },\n",
      "  \"targets\": {\n",
      "    \"target_race_asian\": true,\n",
      "    \"target_race_black\": true,\n",
      "    \"target_race_latinx\": true,\n",
      "    \"target_race_middle_eastern\": true,\n",
      "    \"target_race_native_american\": true,\n",
      "    \"target_race_pacific_islander\": true,\n",
      "    \"target_race_white\": true,\n",
      "    \"target_race_other\": false,\n",
      "    \"target_religion_atheist\": false,\n",
      "    \"target_religion_buddhist\": false,\n",
      "    \"target_religion_christian\": false,\n",
      "    \"target_religion_hindu\": false,\n",
      "    \"target_religion_jewish\": false,\n",
      "    \"target_religion_mormon\": false,\n",
      "    \"target_religion_muslim\": false,\n",
      "    \"target_religion_other\": false,\n",
      "    \"target_origin_immigrant\": false,\n",
      "    \"target_origin_migrant_worker\": false,\n",
      "    \"target_origin_specific_country\": false,\n",
      "    \"target_origin_undocumented\": false,\n",
      "    \"target_origin_other\": false,\n",
      "    \"target_gender_men\": false,\n",
      "    \"target_gender_non_binary\": false,\n",
      "    \"target_gender_transgender_men\": false,\n",
      "    \"target_gender_transgender_unspecified\": false,\n",
      "    \"target_gender_transgender_women\": false,\n",
      "    \"target_gender_women\": false,\n",
      "    \"target_gender_other\": false,\n",
      "    \"target_sexuality_bisexual\": false,\n",
      "    \"target_sexuality_gay\": false,\n",
      "    \"target_sexuality_lesbian\": false,\n",
      "    \"target_sexuality_straight\": false,\n",
      "    \"target_sexuality_other\": false,\n",
      "    \"target_disability_physical\": false,\n",
      "    \"target_disability_cognitive\": false,\n",
      "    \"target_disability_neurological\": false,\n",
      "    \"target_disability_visually_impaired\": false,\n",
      "    \"target_disability_hearing_impaired\": false,\n",
      "    \"target_disability_unspecific\": false,\n",
      "    \"target_disability_other\": false\n",
      "  }\n",
      "}\n",
      "\n",
      "Saving the 135556 records to 'gold_benchmark_dataset.jsonl'...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Note: This script requires the following libraries to be installed:\n",
    "# pip install pandas pyarrow fsspec huggingface_hub\n",
    "\n",
    "# 1. Load the dataset into a pandas DataFrame (your preferred method)\n",
    "print(\"Loading the dataset into a pandas DataFrame...\")\n",
    "try:\n",
    "    df = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "    print(\"Please ensure you have run 'pip install pandas pyarrow fsspec huggingface_hub'\")\n",
    "    exit()\n",
    "\n",
    "# 2. Define the columns we want to extract\n",
    "# These are the actual facet and target columns present in the dataset\n",
    "FACET_COLUMNS = [\n",
    "    'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', \n",
    "    'violence', 'genocide', 'attack_defend', 'hatespeech'\n",
    "]\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    'target_race_asian', 'target_race_black', 'target_race_latinx', \n",
    "    'target_race_middle_eastern', 'target_race_native_american', \n",
    "    'target_race_pacific_islander', 'target_race_white', 'target_race_other',\n",
    "    'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian',\n",
    "    'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon',\n",
    "    'target_religion_muslim', 'target_religion_other', 'target_origin_immigrant',\n",
    "    'target_origin_migrant_worker', 'target_origin_specific_country',\n",
    "    'target_origin_undocumented', 'target_origin_other', 'target_gender_men',\n",
    "    'target_gender_non_binary', 'target_gender_transgender_men',\n",
    "    'target_gender_transgender_unspecified', 'target_gender_transgender_women',\n",
    "    'target_gender_women', 'target_gender_other', 'target_sexuality_bisexual',\n",
    "    'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight',\n",
    "    'target_sexuality_other', 'target_disability_physical', 'target_disability_cognitive',\n",
    "    'target_disability_neurological', 'target_disability_visually_impaired',\n",
    "    'target_disability_hearing_impaired', 'target_disability_unspecific',\n",
    "    'target_disability_other'\n",
    "]\n",
    "\n",
    "# Helper function to classify the score based on the paper's logic\n",
    "def classify_score(score):\n",
    "    if score > 0.5:\n",
    "        return \"hateful\"\n",
    "    if score < -1.0:\n",
    "        return \"supportive\"\n",
    "    return \"neutral\"\n",
    "\n",
    "# 3. Create the function to transform each row of the DataFrame\n",
    "def create_gold_standard_record(row):\n",
    "    # Create the 'overall' object\n",
    "    overall = {\n",
    "        \"label\": classify_score(row['hate_speech_score']),\n",
    "        \"hate_speech_score\": row['hate_speech_score']\n",
    "    }\n",
    "    \n",
    "    # Create the 'facets' object\n",
    "    facets = {col: row[col] for col in FACET_COLUMNS}\n",
    "    \n",
    "    # Create the 'targets' object (ensuring columns exist)\n",
    "    targets = {col: bool(row[col]) for col in TARGET_COLUMNS if col in row}\n",
    "    \n",
    "    # Assemble the final record as a dictionary\n",
    "    return {\n",
    "        \"comment_id\": row['comment_id'],\n",
    "        \"text\": row['text'],\n",
    "        \"overall\": overall,\n",
    "        \"facets\": facets,\n",
    "        \"targets\": targets\n",
    "    }\n",
    "\n",
    "# 4. Apply the function to each row of the DataFrame\n",
    "print(\"Applying the transformation to each row of the DataFrame...\")\n",
    "gold_records = df.apply(create_gold_standard_record, axis=1).tolist()\n",
    "\n",
    "print(\"\\nExample of a processed record:\")\n",
    "print(json.dumps(gold_records[0], indent=2))\n",
    "\n",
    "output_file = \"gold_benchmark_dataset.jsonl\"\n",
    "print(f\"\\nSaving the {len(gold_records)} records to '{output_file}'...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for record in gold_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86013981",
   "metadata": {},
   "source": [
    "# Next Steps: Benchmarking with unfinetuned models and calculate metrics against gold standard\n",
    "\n",
    "Standardized prompt:\n",
    "\n",
    "```You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly adheres to the schema below. Do not include explanations, markdown, or any other text outside of the JSON object.\n",
    "\n",
    "--- IMPORTANT INSTRUCTIONS (READ CAREFULLY) ---\n",
    "1. FIRST produce a single numeric signed float value named \"score\" (e.g. -1.52, 0.00, 1.23).  \n",
    "   - Use a NEGATIVE float for supportive content (e.g. -1.5).  \n",
    "   - Use a POSITIVE float for hateful content (e.g. 1.52).  \n",
    "   - Use a value CLOSE TO ZERO for neutral content (e.g. -0.10, 0.00, 0.25).  \n",
    "   - The score must be a standard JSON number (not a string).\n",
    "\n",
    "2. AFTER you produce the numeric \"score\", set the \"label\" based *only* on these exact thresholds (do not override or invent other thresholds):  \n",
    "   - \"hateful\": If score > 0.5  \n",
    "   - \"supportive\": If score < -1.0  \n",
    "   - \"neutral\": If score is between -1.0 and 0.5 (inclusive)\n",
    "\n",
    "3. The JSON MUST reflect the score and the label consistent with the thresholds above. Do not contradict the thresholds.\n",
    "\n",
    "4. Return ONLY valid JSON. No surrounding text, no markdown, no explanation.\n",
    "\n",
    "--- SCORING RUBRIC ---\n",
    "Use integers from 0 to 4 for all facets based on this scale:\n",
    "0 = Absent\n",
    "1 = Mild\n",
    "2 = Clear\n",
    "3 = Severe\n",
    "4 = Extreme\n",
    "\n",
    "--- SCHEMA DEFINITIONS ---\n",
    "1. overall.score: A signed continuous float (negative for supportive, positive for hateful, near-zero for neutral). This must appear first in the overall object before the label.\n",
    "2. overall.label: A categorical label derived exactly from the score using the thresholds listed above.\n",
    "3. facets: 10 integer fields (0-4) as listed below.\n",
    "4. targets: booleans for all target groups; set true only if that group is explicitly targeted in the text.\n",
    "\n",
    "--- JSON FORMAT TO RETURN (MUST MATCH EXACTLY) ---\n",
    "{\n",
    "  \"overall\": {\n",
    "    \"score\": 0.00,\n",
    "    \"label\": \"neutral\"\n",
    "  },\n",
    "  \"facets\": {\n",
    "    \"sentiment\": 0,\n",
    "    \"respect\": 0,\n",
    "    \"insult\": 0,\n",
    "    \"humiliate\": 0,\n",
    "    \"status\": 0,\n",
    "    \"dehumanize\": 0,\n",
    "    \"violence\": 0,\n",
    "    \"genocide\": 0,\n",
    "    \"attack_defend\": 0,\n",
    "    \"hatespeech\": 0\n",
    "  },\n",
    "  \"targets\": {\n",
    "    \"target_race_asian\": false,\n",
    "    \"target_race_black\": false,\n",
    "    \"target_race_latinx\": false,\n",
    "    \"target_race_middle_eastern\": false,\n",
    "    \"target_race_native_american\": false,\n",
    "    \"target_race_pacific_islander\": false,\n",
    "    \"target_race_white\": false,\n",
    "    \"target_race_other\": false,\n",
    "    \"target_religion_atheist\": false,\n",
    "    \"target_religion_buddhist\": false,\n",
    "    \"target_religion_christian\": false,\n",
    "    \"target_religion_hindu\": false,\n",
    "    \"target_religion_jewish\": false,\n",
    "    \"target_religion_mormon\": false,\n",
    "    \"target_religion_muslim\": false,\n",
    "    \"target_religion_other\": false,\n",
    "    \"target_origin_immigrant\": false,\n",
    "    \"target_origin_migrant_worker\": false,\n",
    "    \"target_origin_specific_country\": false,\n",
    "    \"target_origin_undocumented\": false,\n",
    "    \"target_origin_other\": false,\n",
    "    \"target_gender_men\": false,\n",
    "    \"target_gender_non_binary\": false,\n",
    "    \"target_gender_transgender_men\": false,\n",
    "    \"target_gender_transgender_unspecified\": false,\n",
    "    \"target_gender_transgender_women\": false,\n",
    "    \"target_gender_women\": false,\n",
    "    \"target_gender_other\": false,\n",
    "    \"target_sexuality_bisexual\": false,\n",
    "    \"target_sexuality_gay\": false,\n",
    "    \"target_sexuality_lesbian\": false,\n",
    "    \"target_sexuality_straight\": false,\n",
    "    \"target_sexuality_other\": false,\n",
    "    \"target_age_children\": false,\n",
    "    \"target_age_teenagers\": false,\n",
    "    \"target_age_young_adults\": false,\n",
    "    \"target_age_middle_aged\": false,\n",
    "    \"target_age_seniors\": false,\n",
    "    \"target_age_other\": false,\n",
    "    \"target_disability_physical\": false,\n",
    "    \"target_disability_cognitive\": false,\n",
    "    \"target_disability_neurological\": false,\n",
    "    \"target_disability_visually_impaired\": false,\n",
    "    \"target_disability_hearing_impaired\": false,\n",
    "    \"target_disability_unspecific\": false,\n",
    "    \"target_disability_other\": false\n",
    "  }\n",
    "}\n",
    "\n",
    "--- TEXT TO ANALYZE ---\n",
    "{text}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e3a7c",
   "metadata": {},
   "source": [
    "# Benchmarks:\n",
    "\n",
    "Overall Assessment:\n",
    "Label: We'll calculate the Macro F1-Score and Accuracy for the overall.label field (\"hateful\", \"neutral\", \"supportive\")\n",
    "\n",
    "Facet Analysis (Ordinal):\n",
    "Accuracy: We'll calculate the Mean Absolute Error (MAE) for each of the 10 facets by comparing our gold integer values to the model's predicted integer values.\n",
    "\n",
    "Target Identification (Multi-Label):\n",
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439748a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
