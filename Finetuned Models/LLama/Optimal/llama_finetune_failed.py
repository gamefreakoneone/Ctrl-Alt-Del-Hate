# -*- coding: utf-8 -*-
"""LLama_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zCwf5nlDnH39HuSVKzQ7xlteC5qqNA1p
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Dataset Creation"""

import pandas as pd
import json

# Note: This script requires the following libraries to be installed:
# pip install pandas pyarrow fsspec huggingface_hub

# 1. Load the dataset into a pandas DataFrame (your preferred method)
print("Loading the dataset into a pandas DataFrame...")
try:
    df = pd.read_parquet("hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet")
    print(f"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns.")
except Exception as e:
    print(f"An error occurred while loading the data: {e}")
    print("Please ensure you have run 'pip install pandas pyarrow fsspec huggingface_hub'")
    exit()

# 2. Define the columns we want to extract
# These are the actual facet and target columns present in the dataset
FACET_COLUMNS = [
    'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize',
    'violence', 'genocide', 'attack_defend', 'hatespeech'
]

TARGET_COLUMNS = [
    "target_race_asian", "target_race_black", "target_race_latinx", "target_race_middle_eastern",
    "target_race_native_american", "target_race_pacific_islander", "target_race_white", "target_race_other",
    "target_religion_atheist", "target_religion_buddhist", "target_religion_christian", "target_religion_hindu",
    "target_religion_jewish", "target_religion_mormon", "target_religion_muslim", "target_religion_other",
    "target_origin_immigrant",  "target_origin_migrant_worker", "target_origin_specific_country",
    "target_origin_undocumented", "target_origin_other", "target_gender_men", "target_gender_non_binary",
    "target_gender_transgender_men", "target_gender_transgender_unspecified", "target_gender_transgender_women",
    "target_gender_women", "target_gender_other", "target_sexuality_bisexual", "target_sexuality_gay",
    "target_sexuality_lesbian", "target_sexuality_straight", "target_sexuality_other",
    "target_age_children", "target_age_teenagers", "target_age_young_adults", "target_age_middle_aged", "target_age_seniors",
    "target_age_other",  "target_disability_physical", "target_disability_cognitive",
    "target_disability_neurological", "target_disability_visually_impaired",
    "target_disability_hearing_impaired", "target_disability_unspecific", "target_disability_other"
]

# Helper function to classify the score based on the paper's logic
def classify_score(score):
    if score > 0.5:
        return "hateful"
    if score < -1.0:
        return "supportive"
    return "neutral"

# 3. Create the function to transform each row of the DataFrame
def create_gold_standard_record(row):
    # Create the 'overall' object
    overall = {
        "label": classify_score(row['hate_speech_score']),
        "hate_speech_score": row['hate_speech_score']
    }

    # Create the 'facets' object
    facets = {col: row[col] for col in FACET_COLUMNS}

    # Create the 'targets' object (ensuring columns exist)
    targets = {col: bool(row[col]) for col in TARGET_COLUMNS if col in row}

    # Assemble the final record as a dictionary
    return {
        "comment_id": row['comment_id'],
        "text": row['text'],
        "overall": overall,
        "facets": facets,
        "targets": targets
    }

# 4. Apply the function to each row of the DataFrame
print("Applying the transformation to each row of the DataFrame...")
gold_records = df.apply(create_gold_standard_record, axis=1).tolist()

print("\nExample of a processed record:")
print(json.dumps(gold_records[0], indent=2))

output_file = "gold_benchmark_dataset.jsonl"
print(f"\nSaving the {len(gold_records)} records to '{output_file}'...")

with open(output_file, 'w') as f:
    for record in gold_records:
        f.write(json.dumps(record) + '\n')

"""# Concantenated dataset"""

import json
import pandas as pd
from sklearn.model_selection import train_test_split

INPUT_FILE = "gold_benchmark_dataset.jsonl"
TRAIN_FILE = "train_aggregated.jsonl"
VAL_FILE = "val_aggregated.jsonl"
TEST_FILE = "test_aggregated.jsonl"

def load_data(path):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

def save_jsonl(data, path):
    with open(path, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")

def aggregate_annotations(records):
    """Aggregate multiple annotations for the same comment into one record."""
    if len(records) == 1:
        return records[0]

    aggregated = {
        "comment_id": records[0]["comment_id"],
        "text": records[0]["text"],
        "overall": {},
        "facets": {},
        "targets": {}
    }

    # Aggregate hate_speech_score (mean)
    hate_scores = [r["overall"]["hate_speech_score"] for r in records]
    avg_hate_score = sum(hate_scores) / len(hate_scores)

    # Classify based on averaged score
    if avg_hate_score > 0.5:
        label = "hateful"
    elif avg_hate_score < -1.0:
        label = "supportive"
    else:
        label = "neutral"

    aggregated["overall"] = {
        "label": label,
        "hate_speech_score": avg_hate_score
    }

    # Aggregate facets (mean then round)
    facet_keys = records[0]["facets"].keys()
    for key in facet_keys:
        values = [r["facets"][key] for r in records]
        aggregated["facets"][key] = round(sum(values) / len(values))

    # Aggregate targets (OR logic)
    target_keys = records[0]["targets"].keys()
    for key in target_keys:
        aggregated["targets"][key] = any(r["targets"][key] for r in records)

    return aggregated

def main():
    # Load all data
    data = load_data(INPUT_FILE)
    print(f"Loaded {len(data)} total records")

    # Group by comment_id
    comment_groups = {}
    for record in data:
        comment_id = record["comment_id"]
        if comment_id not in comment_groups:
            comment_groups[comment_id] = []
        comment_groups[comment_id].append(record)

    unique_comment_ids = list(comment_groups.keys())
    print(f"Found {len(unique_comment_ids)} unique comments")

    # Split comment_ids: 80% train, 10% val, 10% test
    train_ids, temp_ids = train_test_split(unique_comment_ids, test_size=0.2, random_state=42)
    val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)

    # ✅ CHANGE: Aggregate ALL splits (not just val/test)
    train_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in train_ids]
    val_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in val_ids]
    test_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in test_ids]

    # Save datasets
    save_jsonl(train_data, TRAIN_FILE)
    save_jsonl(val_data, VAL_FILE)
    save_jsonl(test_data, TEST_FILE)

    print(f"\n✅ Data split complete:")
    print(f"Train: {len(train_data)} aggregated records ({len(train_ids)} unique comments)")
    print(f"Val: {len(val_data)} aggregated records ({len(val_ids)} unique comments)")
    print(f"Test: {len(test_data)} aggregated records ({len(test_ids)} unique comments)")
    print(f"Total: {len(train_data) + len(val_data) + len(test_data)} records")
    print(f"Reduction: {len(data)} → {len(train_data) + len(val_data) + len(test_data)} (~{100*(1 - (len(train_data) + len(val_data) + len(test_data))/len(data)):.1f}% reduction)")

    # Verify no overlap
    assert len(set(train_ids) & set(val_ids)) == 0, "Train/Val overlap detected!"
    assert len(set(train_ids) & set(test_ids)) == 0, "Train/Test overlap detected!"
    assert len(set(val_ids) & set(test_ids)) == 0, "Val/Test overlap detected!"
    print("\n✅ Verified: No comment_id overlap between splits")

if __name__ == "__main__":
    main()

"""# Finetuning task"""

!pip install -U bitsandbytes

# Commented out IPython magic to ensure Python compatibility.
# %pip install "torch>=2.4.0" tensorboard
# %pip install tensorboard

# %pip install "transformers>=4.51.3"

# %pip install  --upgrade \
  "datasets==3.3.2" \
  "accelerate==1.4.0" \
  "evaluate==0.4.3" \
  "bitsandbytes==0.45.3" \
  "trl==0.21.0" \
  "peft==0.14.0" \
  protobuf \
  sentencepiece

# COMMENT IN: if you are running on a GPU that supports BF16 data type and flash attn, such as NVIDIA L4 or NVIDIA A100
# %pip install flash-attn
# %pip install vllm

from google.colab import userdata
from huggingface_hub import login
import json
import torch

hf_token = userdata.get('HF_TOKEN')
login(hf_token)

"""# Gemma PROMPT DONT USE FOR LLAMA"""

# INSTRUCTION = """
# You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.
# Do not include any explanations, markdown formatting, or text outside of the JSON object.

# =========================
# IMPORTANT INSTRUCTIONS
# =========================
# 1. Output must be **valid JSON only** - no markdown, no commentary.
# 2. Use exact field names and types from the schema.
# 3. Type requirements:
#    - `overall.label` → string ("hateful", "supportive", or "neutral")
#    - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)
#    - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)
#    - All `targets` values → booleans (`true` or `false`)

# =========================
# OVERALL ASSESSMENT
# =========================
# Provide two values in the `"overall"` object:

# 1. **`"hate_speech_score"`** - A float representing hate speech intensity:
#    - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)
#    - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)
#    - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)

# 2. **`"label"`** - A categorical string derived from the hate_speech_score:
#    - `"hateful"` if hate_speech_score > 0.5
#    - `"neutral"` if hate_speech_score is between -1.0 and 0.5 (inclusive)
#    - `"supportive"` if hate_speech_score < -1.0

# =========================
# FACETS (0-4 SCALE)
# =========================
# Each facet is an **integer** from 0 to 4:
# - 0 = Absent
# - 1 = Mild
# - 2 = Clear
# - 3 = Severe
# - 4 = Extreme

# Example: `"insult": 2` (not `2.0` or `"2"`)

# =========================
# TARGETS (BOOLEAN FLAGS)
# =========================
# Set to `true` only if that group is explicitly targeted in the text.

# =========================
# JSON SCHEMA (MUST MATCH EXACTLY)
# =========================
# {{
#   "overall": {{
#     "label": "neutral",
#     "hate_speech_score": 0.00
#   }},
#   "facets": {{
#     "sentiment": 0,
#     "respect": 0,
#     "insult": 0,
#     "humiliate": 0,
#     "status": 0,
#     "dehumanize": 0,
#     "violence": 0,
#     "genocide": 0,
#     "attack_defend": 0,
#     "hatespeech": 0
#   }},
#   "targets": {{
#     "target_race_asian": false,
#     "target_race_black": false,
#     "target_race_latinx": false,
#     "target_race_middle_eastern": false,
#     "target_race_native_american": false,
#     "target_race_pacific_islander": false,
#     "target_race_white": false,
#     "target_race_other": false,
#     "target_religion_atheist": false,
#     "target_religion_buddhist": false,
#     "target_religion_christian": false,
#     "target_religion_hindu": false,
#     "target_religion_jewish": false,
#     "target_religion_mormon": false,
#     "target_religion_muslim": false,
#     "target_religion_other": false,
#     "target_origin_immigrant": false,
#     "target_origin_migrant_worker": false,
#     "target_origin_specific_country": false,
#     "target_origin_undocumented": false,
#     "target_origin_other": false,
#     "target_gender_men": false,
#     "target_gender_non_binary": false,
#     "target_gender_transgender_men": false,
#     "target_gender_transgender_unspecified": false,
#     "target_gender_transgender_women": false,
#     "target_gender_women": false,
#     "target_gender_other": false,
#     "target_sexuality_bisexual": false,
#     "target_sexuality_gay": false,
#     "target_sexuality_lesbian": false,
#     "target_sexuality_straight": false,
#     "target_sexuality_other": false,
#     "target_age_children": false,
#     "target_age_teenagers": false,
#     "target_age_young_adults": false,
#     "target_age_middle_aged": false,
#     "target_age_seniors": false,
#     "target_age_other": false,
#     "target_disability_physical": false,
#     "target_disability_cognitive": false,
#     "target_disability_neurological": false,
#     "target_disability_visually_impaired": false,
#     "target_disability_hearing_impaired": false,
#     "target_disability_unspecific": false,
#     "target_disability_other": false
#   }}
# }}

# =========================
# TEXT TO ANALYZE
# =========================
# {text}
# """

"""# LLAMA Prompt"""

INSTRUCTION = """
You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.
Do not include any explanations, markdown formatting, or text outside of the JSON object.

=========================
IMPORTANT INSTRUCTIONS
=========================
1. Output must be **valid JSON only** - no markdown, no commentary.
2. Use exact field names and types from the schema.
3. Type requirements:
   - `overall.label` → string ("hateful", "supportive", or "neutral")
   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)
   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)
   - All `targets` values → booleans (`true` or `false`)

=========================
OVERALL ASSESSMENT
=========================
Provide two values in the `"overall"` object:

1. **`"hate_speech_score"`** - A float representing hate speech intensity:
   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)
   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)
   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)

2. **`"label"`** - A categorical string derived from the hate_speech_score:
   - `"hateful"` if hate_speech_score > 0.5
   - `"neutral"` if hate_speech_score is between -1.0 and 0.5 (inclusive)
   - `"supportive"` if hate_speech_score < -1.0

=========================
FACETS (0-4 SCALE)
=========================
Each facet is an **integer** from 0 to 4:
- 0 = Absent
- 1 = Mild
- 2 = Clear
- 3 = Severe
- 4 = Extreme

Example: `"insult": 2` (not `2.0` or `"2"`)

=========================
TARGETS (BOOLEAN FLAGS)
=========================
Set to `true` only if that group is explicitly targeted in the text.

=========================
JSON SCHEMA (MUST MATCH EXACTLY)
=========================
{{
  "overall": {{
    "label": "neutral",
    "hate_speech_score": 0.00
  }},
  "facets": {{
    "sentiment": 0,
    "respect": 0,
    "insult": 0,
    "humiliate": 0,
    "status": 0,
    "dehumanize": 0,
    "violence": 0,
    "genocide": 0,
    "attack_defend": 0,
    "hatespeech": 0
  }},
  "targets": {{
    "target_race_asian": false,
    "target_race_black": false,
    "target_race_latinx": false,
    "target_race_middle_eastern": false,
    "target_race_native_american": false,
    "target_race_pacific_islander": false,
    "target_race_white": false,
    "target_race_other": false,
    "target_religion_atheist": false,
    "target_religion_buddhist": false,
    "target_religion_christian": false,
    "target_religion_hindu": false,
    "target_religion_jewish": false,
    "target_religion_mormon": false,
    "target_religion_muslim": false,
    "target_religion_other": false,
    "target_origin_immigrant": false,
    "target_origin_migrant_worker": false,
    "target_origin_specific_country": false,
    "target_origin_undocumented": false,
    "target_origin_other": false,
    "target_gender_men": false,
    "target_gender_non_binary": false,
    "target_gender_transgender_men": false,
    "target_gender_transgender_unspecified": false,
    "target_gender_transgender_women": false,
    "target_gender_women": false,
    "target_gender_other": false,
    "target_sexuality_bisexual": false,
    "target_sexuality_gay": false,
    "target_sexuality_lesbian": false,
    "target_sexuality_straight": false,
    "target_sexuality_other": false,
    "target_age_children": false,
    "target_age_teenagers": false,
    "target_age_young_adults": false,
    "target_age_middle_aged": false,
    "target_age_seniors": false,
    "target_age_other": false,
    "target_disability_physical": false,
    "target_disability_cognitive": false,
    "target_disability_neurological": false,
    "target_disability_visually_impaired": false,
    "target_disability_hearing_impaired": false,
    "target_disability_unspecific": false,
    "target_disability_other": false
  }}
}}
"""

"""# THIS IS MEANT FOR LLAMA"""

# def create_conversation(sample):
#     system_msg = {"role": "system", "content": INSTRUCTION.strip()}

#     user_msg = {"role": "user", "content": sample["text"]}

#     assistant_msg = {
#         "role": "assistant",
#         "content": json.dumps({
#             "overall": sample["overall"],
#             "facets": sample["facets"],
#             "targets": sample["targets"],
#         }, ensure_ascii=False)
#     }

#     return {"messages": [system_msg, user_msg, assistant_msg]}

"""# THIS IS MEANT ONLY FOR GEMMA"""

# def create_conversation(sample):
#     assistant_response = json.dumps({
#         "overall": sample["overall"],
#         "facets": sample["facets"],
#         "targets": sample["targets"]
#     }, ensure_ascii=False)

#     user_content = f"{INSTRUCTION}\n\n<text>\n{sample['text']}\n</text>"

#     return {
#         "messages": [
#             {"role": "user",  "content": user_content},
#             {"role": "model", "content": assistant_response}
#         ]
#     }



"""Do this only once. We are formatting the dataset to suit the LLama formmater"""

# from datasets import load_dataset

# train = load_dataset("json", data_files="train_aggregated.jsonl", split="train")
# val   = load_dataset("json", data_files="val_aggregated.jsonl",   split="train")

# train_msgs = train.map(create_conversation, remove_columns=train.column_names)
# val_msgs   = val.map(create_conversation,   remove_columns=val.column_names)

# train_msgs.to_json("train_messages.jsonl")
# val_msgs.to_json("val_messages.jsonl")

"""# I am attempting ICL"""

from datasets import load_dataset
train_dataset = load_dataset("json", data_files="train_aggregated.jsonl", split="train")
val_dataset = load_dataset("json", data_files="val_aggregated.jsonl", split="train")

from collections import Counter
import numpy as np
print("Computing inverse frequency weights...")

target_keys = list(train_dataset[0]['targets'].keys())
target_counts = Counter()

for row in train_dataset:
    for k, v in row['targets'].items():
        if v:
            target_counts[k] += 1

total_samples = len(train_dataset)
inverse_freq = {k: total_samples / (target_counts[k] + 1) for k in target_keys}

# Compute per-row weight using MAX of active target weights
row_weights = []
for row in train_dataset:
    active_targets = [k for k, v in row['targets'].items() if v]
    if active_targets:
        weight = max(inverse_freq[t] for t in active_targets)
    else:
        weight = 1.0
    row_weights.append(weight)

row_weights = np.array(row_weights)
row_probs = row_weights / row_weights.sum()

print(f"Weights computed. {len(train_dataset)} samples ready for weighted sampling.")

def format_example(sample):
    """Format a single sample as INPUT/OUTPUT for few-shot."""
    output = json.dumps({
        "overall": sample["overall"],
        "facets": sample["facets"],
        "targets": sample["targets"]
    }, ensure_ascii=False)
    return f'INPUT: "{sample["text"]}"\nOUTPUT:\n{output}'


def sample_few_shot_examples(exclude_idx=None, k=5):
    """Sample k examples with inverse frequency weighting."""
    probs = row_probs.copy()

    if exclude_idx is not None:
        probs[exclude_idx] = 0
        probs = probs / probs.sum()

    indices = np.random.choice(len(train_dataset), size=k, replace=False, p=probs)
    return [train_dataset[int(i)] for i in indices]

def format_example(sample):
    """Format a single sample as INPUT/OUTPUT for few-shot."""
    output = json.dumps({
        "overall": sample["overall"],
        "facets": sample["facets"],
        "targets": sample["targets"]
    }, ensure_ascii=False)
    return f'INPUT: "{sample["text"]}"\nOUTPUT:\n{output}'


def sample_few_shot_examples(exclude_idx=None):
    """Sample 5 examples with inverse frequency weighting."""
    probs = row_probs.copy()

    if exclude_idx is not None:
        probs[exclude_idx] = 0
        probs = probs / probs.sum()

    indices = np.random.choice(len(train_dataset), size=5, replace=False, p=probs)
    return [train_dataset[int(i)] for i in indices]


def build_system_prompt(few_shot_examples):
    """Build system prompt with few-shot examples."""
    examples_text = "\n\n".join([
        f"EXAMPLE {i+1}:\n{format_example(ex)}"
        for i, ex in enumerate(few_shot_examples)
    ])

    return INSTRUCTION.strip() + f"\n\n=========================\nEXAMPLES\n=========================\n{examples_text}"

def create_conversation(sample, idx):
    """Create conversation for TRAINING data with 5-shot examples."""
    few_shot_examples = sample_few_shot_examples(exclude_idx=idx)
    system_prompt = build_system_prompt(few_shot_examples)

    return {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": sample["text"]},
            {"role": "assistant", "content": json.dumps({
                "overall": sample["overall"],
                "facets": sample["facets"],
                "targets": sample["targets"]
            }, ensure_ascii=False)}
        ]
    }


def create_conversation_val(sample, idx):
    """Create conversation for VALIDATION data - samples from train, no exclusion."""
    few_shot_examples = sample_few_shot_examples(exclude_idx=None)
    system_prompt = build_system_prompt(few_shot_examples)

    return {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": sample["text"]},
            {"role": "assistant", "content": json.dumps({
                "overall": sample["overall"],
                "facets": sample["facets"],
                "targets": sample["targets"]
            }, ensure_ascii=False)}
        ]
    }

print("Generating training conversations...")
train_msgs = train_dataset.map(
    create_conversation,
    with_indices=True,
    remove_columns=train_dataset.column_names
)

print("Generating validation conversations...")
np.random.seed(42)  # Reproducible validation
val_msgs = val_dataset.map(
    create_conversation_val,
    with_indices=True,
    remove_columns=val_dataset.column_names
)

print("Saving...")
train_msgs.to_json("train_messages.jsonl")
val_msgs.to_json("val_messages.jsonl")

print(f"Done. Train: {len(train_msgs)}, Val: {len(val_msgs)}")

"""# NExt step"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForImageTextToText, BitsAndBytesConfig

model_id = "meta-llama/Llama-3.2-1B-Instruct"
model_class = AutoModelForCausalLM

if torch.cuda.get_device_capability()[0] >= 8:
    torch_dtype = torch.bfloat16
else:
    torch_dtype = torch.float16

# torch_dtype = torch.bfloat16

model_kwargs = dict( # Explain the parameters we are using here
    # attn_implementation="flash_attention_2", # Use "flash_attention_2" when running on Ampere or newer GPU or eager if not A100 .
    # attn_implementation="eager",
    attn_implementation="sdpa",
    torch_dtype=torch_dtype,
    device_map="auto",
)

# This is meant for QLoRA if I am understanding correctly. Also forma  video I am wathcing they said that the rank of QLoRA is 64 with a scaling parameter of 16
# model_kwargs["quantization_config"] = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_quant_type='nf4',
#     bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],
#     bnb_4bit_quant_storage=model_kwargs['torch_dtype'],
# )

# !pip uninstall flash-attn -y
# !pip install flash-attn --no-build-isolation

# import torch
# print("Torch:", torch.__version__)
# print("CUDA:", torch.version.cuda)
# print("GPU:", torch.cuda.get_device_name(0))
# print("SM capability:", torch.cuda.get_device_capability(0))

# !pip uninstall -y torch torchvision torchaudio flash-attn flash_attn

# !pip install ninja
# !pip install packaging

# !pip install flash-attn --no-build-isolation

# # Uninstall the broken version first
# !pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu121torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

model = model_class.from_pretrained(model_id, **model_kwargs)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

# import torch
# print(torch.__version__)

from peft import LoraConfig
from datasets import load_dataset

train_data = load_dataset("json", data_files="train_messages.jsonl", split="train")
val_data = load_dataset("json", data_files="val_messages.jsonl", split="train")

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.05,
    r=32,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM",
    # modules_to_save=["lm_head", "embed_tokens"] # make sure to save the lm_head and embed_tokens as you train the special tokens. I dont think we need it since we are not adding new tokens??
)

from trl import SFTConfig

args = SFTConfig(
    output_dir="llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL",         # directory to save and repository id
    max_length=4096,                         # max sequence length for model and packing of the dataset. None this
    packing=False,                           # False since our sample are long as hell
    num_train_epochs=2,                     # number of training epochs
    per_device_train_batch_size=4,          # batch size per device during training
    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass
    gradient_checkpointing=False,            # use gradient checkpointing to save memory
    optim="adamw_torch_fused",              # use fused adamw optimizer, or maybe paged_adamw_32bit for QLoRA
    logging_steps=10,                       # log every 10 steps
    save_strategy="epoch",                  # save checkpoint every epoch
    learning_rate=2e-4,
    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision
    bf16=True if torch_dtype == torch.bfloat16 else False,   # use bfloat16 precision
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",           # try contant later if I  want to see difference
    # push_to_hub=True,
    report_to="tensorboard",                # reporting metrics to tensorboard
    dataset_kwargs={
        "add_special_tokens": False,
        "append_concat_token": True,
    },
    eval_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_data,
    eval_dataset=val_data,
    peft_config=peft_config,
    processing_class=tokenizer
)

"""# TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensorBoard extension
# %load_ext tensorboard

# Point to YOUR output directory
# %tensorboard --logdir llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL

# torch.cuda.empty_cache()



"""Training"""

trainer.train()

# Remember to save this on drive!
trainer.save_model()

# !mkdir -p "/content/drive/MyDrive/colab_exports"
# !zip -r "/content/drive/MyDrive/colab_exports/llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha32-model.zip" "llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha32"

"""# UPgraded Testing"""

# !unzip "/content/drive/MyDrive/colab_exports/llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha64.zip" -d "/content/"

# Free memory from training
del model
del trainer
torch.cuda.empty_cache()

# !unzip llama-1B-ctrl-alt-del-h8-r32-epochs5.zip

# Commented out IPython magic to ensure Python compatibility.
# %pip install vllm

#!/usr/bin/env python3
"""
Llama LoRA Fine-tuned Model - Testing and Inference
Adapted from Gemma pipeline for Llama-3.2-1B-Instruct
"""

import json
import torch
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from vllm import LLM, SamplingParams



# ============================================================================
# HELPER FUNCTIONS FOR JSON PARSING AND NORMALIZATION
# ============================================================================

def extract_outer_json(text: str) -> str:
    """Extract the outermost JSON object from generated text"""
    s = text.find("{")
    e = text.rfind("}")
    if s == -1 or e == -1 or e <= s:
        raise ValueError("No JSON object found in output")
    return text[s:e+1]

def to_bool(x):
    """Convert various types to boolean"""
    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)):
        return bool(int(x))
    if isinstance(x, str):
        return x.strip().lower() in {"1", "true", "yes", "y", "t"}
    return False

def derive_label_from_score(score: float) -> str:
    """Derive label from hate_speech_score"""
    if score > 0.5:
        return "hateful"
    elif score < -1.0:
        return "supportive"
    else:
        return "neutral"

def normalize_schema(data):
    """Normalize the JSON schema to ensure type consistency"""
    if isinstance(data, str):
        data = json.loads(data)

    overall = data.get("overall", {})

    if "hate_speech_score" not in overall and "score" in overall:
        overall["hate_speech_score"] = float(overall.pop("score"))
    if "hate_speech_score" in overall:
        overall["hate_speech_score"] = float(overall["hate_speech_score"])

    if "label" not in overall and "hate_speech_score" in overall:
        overall["label"] = derive_label_from_score(overall["hate_speech_score"])

    data["overall"] = overall

    facets = data.get("facets", {})
    fixed_facets = {}
    for key, value in facets.items():
        try:
            fixed_facets[key] = max(0, min(4, int(float(value))))
        except Exception:
            fixed_facets[key] = 0
    data["facets"] = fixed_facets

    targets = data.get("targets", {})
    data["targets"] = {key: to_bool(value) for key, value in targets.items()}

    return data

INSTRUCTION = """
You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.
Do not include any explanations, markdown formatting, or text outside of the JSON object.

=========================
IMPORTANT INSTRUCTIONS
=========================
1. Output must be **valid JSON only** - no markdown, no commentary.
2. Use exact field names and types from the schema.
3. Type requirements:
   - `overall.label` → string ("hateful", "supportive", or "neutral")
   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)
   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)
   - All `targets` values → booleans (`true` or `false`)

=========================
OVERALL ASSESSMENT
=========================
Provide two values in the `"overall"` object:

1. **`"hate_speech_score"`** - A float representing hate speech intensity:
   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)
   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)
   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)

2. **`"label"`** - A categorical string derived from the hate_speech_score:
   - `"hateful"` if hate_speech_score > 0.5
   - `"neutral"` if hate_speech_score is between -1.0 and 0.5 (inclusive)
   - `"supportive"` if hate_speech_score < -1.0

=========================
FACETS (0-4 SCALE)
=========================
Each facet is an **integer** from 0 to 4:
- 0 = Absent
- 1 = Mild
- 2 = Clear
- 3 = Severe
- 4 = Extreme

Example: `"insult": 2` (not `2.0` or `"2"`)

=========================
TARGETS (BOOLEAN FLAGS)
=========================
Set to `true` only if that group is explicitly targeted in the text.

=========================
JSON SCHEMA (MUST MATCH EXACTLY)
=========================
{{
  "overall": {{
    "label": "neutral",
    "hate_speech_score": 0.00
  }},
  "facets": {{
    "sentiment": 0,
    "respect": 0,
    "insult": 0,
    "humiliate": 0,
    "status": 0,
    "dehumanize": 0,
    "violence": 0,
    "genocide": 0,
    "attack_defend": 0,
    "hatespeech": 0
  }},
  "targets": {{
    "target_race_asian": false,
    "target_race_black": false,
    "target_race_latinx": false,
    "target_race_middle_eastern": false,
    "target_race_native_american": false,
    "target_race_pacific_islander": false,
    "target_race_white": false,
    "target_race_other": false,
    "target_religion_atheist": false,
    "target_religion_buddhist": false,
    "target_religion_christian": false,
    "target_religion_hindu": false,
    "target_religion_jewish": false,
    "target_religion_mormon": false,
    "target_religion_muslim": false,
    "target_religion_other": false,
    "target_origin_immigrant": false,
    "target_origin_migrant_worker": false,
    "target_origin_specific_country": false,
    "target_origin_undocumented": false,
    "target_origin_other": false,
    "target_gender_men": false,
    "target_gender_non_binary": false,
    "target_gender_transgender_men": false,
    "target_gender_transgender_unspecified": false,
    "target_gender_transgender_women": false,
    "target_gender_women": false,
    "target_gender_other": false,
    "target_sexuality_bisexual": false,
    "target_sexuality_gay": false,
    "target_sexuality_lesbian": false,
    "target_sexuality_straight": false,
    "target_sexuality_other": false,
    "target_age_children": false,
    "target_age_teenagers": false,
    "target_age_young_adults": false,
    "target_age_middle_aged": false,
    "target_age_seniors": false,
    "target_age_other": false,
    "target_disability_physical": false,
    "target_disability_cognitive": false,
    "target_disability_neurological": false,
    "target_disability_visually_impaired": false,
    "target_disability_hearing_impaired": false,
    "target_disability_unspecific": false,
    "target_disability_other": false
  }}
}}

=========================
EXAMPLES
=========================
EXAMPLE 1:
INPUT: "Some old white man accidentally FaceTimed me yesterday, He had this racist ass look on his face so we just stared at each other for like 37 seconds then I called him a nigga and hung up"
OUTPUT:
{"overall":{"label": "neutral", "hate_speech_score": -0.64},
"facets": {"sentiment": 4.0, "respect": 4.0, "insult": 3.0, "humiliate": 3.0, "status": 3.0, "dehumanize": 2.0, "violence": 1.0, "genocide": 0.0, "attack_defend": 2.0, "hatespeech": 2.0},
"targets": {"target_race_asian": false, "target_race_black": true, "target_race_latinx": false, "target_race_middle_eastern": false, "target_race_native_american": false,
"target_race_pacific_islander": false, "target_race_white": true, "target_race_other": false, "target_religion_atheist": false, "target_religion_buddhist": false, "target_religion_christian": false,
 "target_religion_hindu": false, "target_religion_jewish": false, "target_religion_mormon": false, "target_religion_muslim": false, "target_religion_other": false, "target_origin_immigrant": false,
 "target_origin_migrant_worker": false, "target_origin_specific_country": false, "target_origin_undocumented": false, "target_origin_other": false, "target_gender_men": true, "target_gender_non_binary": false,
 "target_gender_transgender_men": false, "target_gender_transgender_unspecified": false, "target_gender_transgender_women": false, "target_gender_women": false, "target_gender_other": false,
 "target_sexuality_bisexual": false, "target_sexuality_gay": false, "target_sexuality_lesbian": false, "target_sexuality_straight": false, "target_sexuality_other": false, "target_age_children": false,
 "target_age_teenagers": false, "target_age_young_adults": false, "target_age_middle_aged": true, "target_age_seniors": true, "target_age_other": true, "target_disability_physical": false,
 "target_disability_cognitive": false, "target_disability_neurological": false, "target_disability_visually_impaired": false, "target_disability_hearing_impaired": false,
 "target_disability_unspecific": false, "target_disability_other": false}}

EXAMPLE 2:
INPUT:"Hmm... all I can say, really, is go with your heart! If that leads you to living the life of a cis person, then go ahead, you're valid either way.  If that means transitioning, then do that! It's about what you want to do, theres reason to be fearful about whether you are or aren't, just be you.  Of course it will take a while to discover who you are, but it takes everyone a while.  I wish you the best of luck!"
OUTPUT:
{"overall": {"label": "supportive", "hate_speech_score": -6.23},
"facets": {"sentiment": 0.0, "respect": 0.0, "insult": 0.0, "humiliate": 0.0, "status": 2.0, "dehumanize": 0.0, "violence": 0.0, "genocide": 0.0, "attack_defend": 1.0, "hatespeech": 0.0},
"targets": {"target_race_asian": true, "target_race_black": true, "target_race_latinx": true, "target_race_middle_eastern": true, "target_race_native_american": true, "target_race_pacific_islander": true,
"target_race_white": true, "target_race_other": false, "target_religion_atheist": true, "target_religion_buddhist": true, "target_religion_christian": true, "target_religion_hindu": true,
"target_religion_jewish": true, "target_religion_mormon": true, "target_religion_muslim": true, "target_religion_other": false, "target_origin_immigrant": true, "target_origin_migrant_worker": true,
"target_origin_specific_country": true, "target_origin_undocumented": true, "target_origin_other": false, "target_gender_men": true, "target_gender_non_binary": true, "target_gender_transgender_men": true,
"target_gender_transgender_unspecified": true, "target_gender_transgender_women": true, "target_gender_women": true, "target_gender_other": false, "target_sexuality_bisexual": true, "target_sexuality_gay": true,
"target_sexuality_lesbian": true, "target_sexuality_straight": true, "target_sexuality_other": true, "target_age_children": true, "target_age_teenagers": true, "target_age_young_adults": true,
"target_age_middle_aged": true, "target_age_seniors": true, "target_age_other": false, "target_disability_physical": true, "target_disability_cognitive": true, "target_disability_neurological": true,
"target_disability_visually_impaired": true, "target_disability_hearing_impaired": true, "target_disability_unspecific": true, "target_disability_other": false}}

EXAMPLE 3:
INPUT:"Hey ,muslim dickhead !!!!!!!! Why don;t you go and shove Allah or whatever the fuck you call the retard up your stupid backwards ass !!!!!!!!! You fucking muslims are a bunch of jealous west hating mongs, Islam is a peaceful religion, BULLSHIT !!!!!!! There are many times when it refers to violence. So go strap on your suicide bomb and blow yourself up. yey, one less feeble minded prick ion the world."
OUTPUT:
{"overall": {"label": "hateful", "hate_speech_score": 3.16},
"facets": {"sentiment": 4.0, "respect": 4.0, "insult": 4.0, "humiliate": 4.0, "status": 4.0, "dehumanize": 3.0, "violence": 3.0, "genocide": 2.0, "attack_defend": 4.0, "hatespeech": 2.0},
"targets": {"target_race_asian": true, "target_race_black": true, "target_race_latinx": true, "target_race_middle_eastern": true, "target_race_native_american": true, "target_race_pacific_islander": true,
"target_race_white": true, "target_race_other": true, "target_religion_atheist": false, "target_religion_buddhist": false, "target_religion_christian": false, "target_religion_hindu": true, "target_religion_jewish": false,
"target_religion_mormon": false, "target_religion_muslim": true, "target_religion_other": true, "target_origin_immigrant": true, "target_origin_migrant_worker": true, "target_origin_specific_country": true,
"target_origin_undocumented": true, "target_origin_other": true, "target_gender_men": true, "target_gender_non_binary": false, "target_gender_transgender_men": true, "target_gender_transgender_unspecified": true,
"target_gender_transgender_women": true, "target_gender_women": true, "target_gender_other": false, "target_sexuality_bisexual": true, "target_sexuality_gay": true, "target_sexuality_lesbian": false,
"target_sexuality_straight": true, "target_sexuality_other": false, "target_age_children": false, "target_age_teenagers": true, "target_age_young_adults": true, "target_age_middle_aged": true, "target_age_seniors": true,
"target_age_other": false, "target_disability_physical": true, "target_disability_cognitive": true, "target_disability_neurological": true, "target_disability_visually_impaired": true, "target_disability_hearing_impaired": true,
"target_disability_unspecific": true, "target_disability_other": false}}


EXAMPLE 4:
INPUT:"I like that he can't even be honest with complete strangers. All hours of the day, this pathetic faggot is propagating misinformation to make himself look better. He always talks about mental illness when he's clearly suffering from narcissistic personality disorder"
OUTPUT:
{"overall": {"label": "hateful", "hate_speech_score": 0.92},
"facets": {"sentiment": 4.0, "respect": 3.0, "insult": 4.0, "humiliate": 3.0, "status": 3.0, "dehumanize": 2.0, "violence": 0.0, "genocide": 0.0, "attack_defend": 3.0, "hatespeech": 2.0},
"targets": {"target_race_asian": false, "target_race_black": false, "target_race_latinx": false, "target_race_middle_eastern": false, "target_race_native_american": false, "target_race_pacific_islander": false,
"target_race_white": false, "target_race_other": false, "target_religion_atheist": false, "target_religion_buddhist": false, "target_religion_christian": false, "target_religion_hindu": false,
"target_religion_jewish": false, "target_religion_mormon": false, "target_religion_muslim": false, "target_religion_other": false, "target_origin_immigrant": false, "target_origin_migrant_worker": false,
"target_origin_specific_country": false, "target_origin_undocumented": false, "target_origin_other": false, "target_gender_men": true, "target_gender_non_binary": false, "target_gender_transgender_men": false,
"target_gender_transgender_unspecified": false, "target_gender_transgender_women": false, "target_gender_women": false, "target_gender_other": false, "target_sexuality_bisexual": false, "target_sexuality_gay": true,
"target_sexuality_lesbian": false, "target_sexuality_straight": false, "target_sexuality_other": false, "target_age_children": false, "target_age_teenagers": false, "target_age_young_adults": false,
"target_age_middle_aged": false, "target_age_seniors": false, "target_age_other": false, "target_disability_physical": false, "target_disability_cognitive": false, "target_disability_neurological": true,
"target_disability_visually_impaired": false, "target_disability_hearing_impaired": false, "target_disability_unspecific": false, "target_disability_other": true}}



EXAMPLE 5:
INPUT:"I don't care if your an atheist, every religion has flaws but if your going around saying how smart and noble you are, your a retard"
OUTPUT:
{"overall": {"label": "neutral", "hate_speech_score": -0.5},
"facets": {"sentiment": 3.0, "respect": 4.0, "insult": 2.0, "humiliate": 2.0, "status": 2.0, "dehumanize": 2.0, "violence": 1.0, "genocide": 1.0, "attack_defend": 3.0, "hatespeech": 1.0},
"targets": {"target_race_asian": false, "target_race_black": false, "target_race_latinx": false, "target_race_middle_eastern": false, "target_race_native_american": false, "target_race_pacific_islander": false,
"target_race_white": false, "target_race_other": false, "target_religion_atheist": true, "target_religion_buddhist": false, "target_religion_christian": false, "target_religion_hindu": false,
"target_religion_jewish": false, "target_religion_mormon": false, "target_religion_muslim": false, "target_religion_other": false, "target_origin_immigrant": false, "target_origin_migrant_worker": false,
"target_origin_specific_country": false, "target_origin_undocumented": false, "target_origin_other": false, "target_gender_men": false, "target_gender_non_binary": false, "target_gender_transgender_men": false,
"target_gender_transgender_unspecified": false, "target_gender_transgender_women": false, "target_gender_women": false, "target_gender_other": false, "target_sexuality_bisexual": false,
"target_sexuality_gay": false, "target_sexuality_lesbian": false, "target_sexuality_straight": false, "target_sexuality_other": false, "target_age_children": false, "target_age_teenagers": false,
"target_age_young_adults": false, "target_age_middle_aged": false, "target_age_seniors": false, "target_age_other": false, "target_disability_physical": true, "target_disability_cognitive": false,
"target_disability_neurological": false, "target_disability_visually_impaired": false, "target_disability_hearing_impaired": false, "target_disability_unspecific": false, "target_disability_other": false}}
"""

# ============================================================================
# LORA MODEL MERGING
# ============================================================================

def merge_lora_model():
    """
    Merges your LoRA adapter with the base Llama model.
    Run this ONCE before using vLLM.
    """
    print("="*60)
    print("STEP 1: Merging LoRA adapter with base Llama model")
    print("="*60)

    print("\n Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.2-1B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    print("\n Loading LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model,
        "/content/llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL"  # Your Llama LoRA checkpoint
    )

    print("\n Merging adapter with base model...")
    merged_model = model.merge_and_unload()

    print("\n Saving merged model...")
    output_dir = "llama-1B-merged-r32"
    merged_model.save_pretrained(output_dir)

    # Also save the tokenizer
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
    tokenizer.save_pretrained(output_dir)

    print(f"\n Merged model saved to: {output_dir}")
    print("   Now you can use this with vLLM!")

    return output_dir

# !rm -rf llama-1B-merged-r32

# ============================================================================
# VLLM INFERENCE
# ============================================================================

def run_inference_vllm_merged(merged_model_path="llama-1B-merged-r32"):
    """
    Fast inference using vLLM with your merged Llama model.
    MUST run merge_lora_model() first!
    """
    print("\n" + "="*60)
    print("VLLM INFERENCE (MERGED LLAMA MODEL)")
    print("="*60)

    print("\n Loading merged model with vLLM...")

    # Load tokenizer
    # tokenizer = AutoTokenizer.from_pretrained(merged_model_path) # Some new bug apparently
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct",  trust_remote_code=True)

    # Initialize vLLM with merged model
    llm = LLM(
        model=merged_model_path,
        tokenizer = "meta-llama/Llama-3.2-1B-Instruct" , # Recently added this
        dtype="bfloat16",
        gpu_memory_utilization=0.9,
        max_model_len=8192,
        trust_remote_code=True,
    )

    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=4096,
        stop=["<|eot_id|>", "</s>", "\n}\n", "}\n\n"],  # Llama stop tokens
        stop_token_ids=[tokenizer.eos_token_id],
    )

    print("\n Loading test data...")
    test_data = load_dataset("json", data_files="test_aggregated.jsonl", split="train")
    print(f"Test dataset size: {len(test_data)}")

    # Prepare prompts with Llama chat template
    print("\n Preparing prompts...")
    prompts = []
    for sample in test_data:
        # Create messages in Llama format
        messages = [
            {"role": "system", "content": INSTRUCTION.strip()},
            {"role": "user", "content": sample['text']}
        ]

        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        prompts.append(formatted_prompt)

    # Run inference
    print(f"\n Running inference on {len(prompts)} samples...")

    outputs = llm.generate(prompts, sampling_params)

    # Process results
    print("\n Processing results...")
    predictions = []
    failed_samples = []

    for i, output in enumerate(tqdm(outputs, desc="Processing outputs")):
        sample = test_data[i]
        generated_text = output.outputs[0].text.strip()

        try:
            predicted_json_str = extract_outer_json(generated_text)
            normalized_prediction = normalize_schema(predicted_json_str)
            normalized_expected = normalize_schema(sample)

            predictions.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": normalized_expected,
                "predicted": normalized_prediction,
                "raw_output": generated_text,
                "success": True
            })
        except Exception as e:
            failed_samples.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": normalize_schema(sample),
                "raw_output": generated_text,
                "error": str(e)
            })
            predictions.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "success": False
            })

    print("\n Saving results...")
    with open("llama_test_predictions_vllm.jsonl", "w") as f:
        for pred in predictions:
            f.write(json.dumps(pred) + "\n")

    if failed_samples:
        with open("llama_failed_predictions_vllm.jsonl", "w") as f:
            for failed in failed_samples:
                f.write(json.dumps(failed) + "\n")

    success_rate = (len(predictions) - len(failed_samples)) / len(predictions)
    print(f"\n Inference complete!")
    print(f"   Total: {len(predictions)}")
    print(f"   Success: {len(predictions) - len(failed_samples)} ({success_rate:.1%})")
    print(f"   Failed: {len(failed_samples)}")

    return predictions

print(" Starting Llama LoRA Testing Pipeline")
merged_path = merge_lora_model()

# Step 2: Run inference with vLLM
predictions = run_inference_vllm_merged(merged_path)

print("\n" + "="*60)
print(" PIPELINE COMPLETE!")
print("="*60)
print(f"\n Output files created:")
print(f"   - llama_test_predictions_vllm.jsonl")
print(f"   - llama_failed_predictions_vllm.jsonl (if any failures)")
print(f"\n Next step: Run evaluation script on llama_test_predictions_vllm.jsonl")

# from vllm import LLM, SamplingParams

# llm = LLM(
#     model="meta-llama/Llama-3.2-1B-Instruct",
#     dtype="bfloat16",
#     max_model_len=4096,
#     gpu_memory_utilization=0.7,   # smaller to be safe
# )
# outputs = llm.generate(
#     ["Hello"],
#     SamplingParams(max_tokens=16, temperature=0.0)
# )
# print(outputs[0].outputs[0].text)

"""# Failed"""

def retest_failed_samples():
    """Retest failed predictions and append successful ones"""

    print("="*60)
    print("RETESTING FAILED PREDICTIONS")
    print("="*60)

    # Load failed samples
    print("\n Loading failed predictions...")
    failed_samples = []
    try:
        with open("llama_failed_predictions_vllm.jsonl", "r") as f:
            for line in f:
                failed_samples.append(json.loads(line))
        print(f"   Found {len(failed_samples)} failed sample(s)")
    except FileNotFoundError:
        print("   No failed predictions file found!")
        return

    if not failed_samples:
        print("   No failed samples to retest!")
        return

    # Load the model
    print("\n Loading merged model with vLLM...")
    merged_model_path = "llama-1B-merged"
    tokenizer = AutoTokenizer.from_pretrained(merged_model_path)

    llm = LLM(
        model=merged_model_path,
        dtype="bfloat16",
        gpu_memory_utilization=0.9,
        max_model_len=4096,
        trust_remote_code=True,
    )

    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=2048,
        stop=["<|eot_id|>", "</s>", "\n}\n", "}\n\n"],
        stop_token_ids=[tokenizer.eos_token_id],
    )

    # Prepare prompts
    print("\n Preparing prompts...")
    prompts = []
    for sample in failed_samples:
        messages = [
            {"role": "system", "content": INSTRUCTION.strip()},
            {"role": "user", "content": sample['text']}
        ]

        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        prompts.append(formatted_prompt)

    # Run inference
    print(f"\n Running inference on {len(prompts)} failed sample(s)...")
    outputs = llm.generate(prompts, sampling_params)

    # Process results
    print("\n Processing results...")
    successful_retests = []
    still_failed = []

    for i, output in enumerate(outputs):
        sample = failed_samples[i]
        generated_text = output.outputs[0].text.strip()

        try:
            predicted_json_str = extract_outer_json(generated_text)
            normalized_prediction = normalize_schema(predicted_json_str)
            normalized_expected = normalize_schema(sample['expected'])

            success_entry = {
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": normalized_expected,
                "predicted": normalized_prediction,
                "raw_output": generated_text,
                "success": True,
                "retested": True  # Mark as retested
            }
            successful_retests.append(success_entry)
            print(f"    Successfully processed: {sample.get('comment_id')}")

        except Exception as e:
            still_failed.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": sample.get('expected'),
                "raw_output": generated_text,
                "error": str(e),
                "retry_count": sample.get("retry_count", 0) + 1
            })
            print(f"    Still failed: {sample.get('comment_id')} - {str(e)}")

    # Append successful retests to main predictions file
    if successful_retests:
        print(f"\n Appending {len(successful_retests)} successful prediction(s) to main file...")
        with open("llama_test_predictions_vllm.jsonl", "a") as f:
            for pred in successful_retests:
                f.write(json.dumps(pred) + "\n")
        print("    Successfully appended!")

    # Update failed predictions file with still-failed samples
    if still_failed:
        print(f"\n  Updating failed predictions file with {len(still_failed)} sample(s)...")
        with open("llama_failed_predictions_vllm.jsonl", "w") as f:
            for failed in still_failed:
                f.write(json.dumps(failed) + "\n")
    else:
        # No more failed samples - optionally delete the file
        print("\n All samples now successful! Clearing failed predictions file...")
        with open("llama_failed_predictions_vllm.jsonl", "w") as f:
            pass  # Empty the file

    # Summary
    print("\n" + "="*60)
    print("RETEST SUMMARY")
    print("="*60)
    print(f"   Retested: {len(failed_samples)}")
    print(f"   Now successful: {len(successful_retests)}")
    print(f"   Still failed: {len(still_failed)}")

    if successful_retests:
        print(f"\n Added to: llama_test_predictions_vllm.jsonl")
    if still_failed:
        print(f"  Still failed: llama_failed_predictions_vllm.jsonl")

retest_failed_samples()
print("\n✅ Retest complete!")

"""# Bullshit"""

#!/usr/bin/env python3
"""
Llama LoRA Fine-tuned Model - Testing and Inference
Adapted from Gemma pipeline for Llama-3.2-1B-Instruct
"""

import json
import torch
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from vllm import LLM, SamplingParams
from collections import Counter


# ============================================================================
# HELPER FUNCTIONS FOR JSON PARSING AND NORMALIZATION
# ============================================================================

def extract_outer_json(text: str) -> str:
    """Extract the outermost JSON object from generated text"""
    s = text.find("{")
    e = text.rfind("}")
    if s == -1 or e == -1 or e <= s:
        raise ValueError("No JSON object found in output")
    return text[s:e+1]

def to_bool(x):
    """Convert various types to boolean"""
    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)):
        return bool(int(x))
    if isinstance(x, str):
        return x.strip().lower() in {"1", "true", "yes", "y", "t"}
    return False

def derive_label_from_score(score: float) -> str:
    """Derive label from hate_speech_score"""
    if score > 0.5:
        return "hateful"
    elif score < -1.0:
        return "supportive"
    else:
        return "neutral"

def normalize_schema(data):
    """Normalize the JSON schema to ensure type consistency"""
    if isinstance(data, str):
        data = json.loads(data)

    overall = data.get("overall", {})

    if "hate_speech_score" not in overall and "score" in overall:
        overall["hate_speech_score"] = float(overall.pop("score"))
    if "hate_speech_score" in overall:
        overall["hate_speech_score"] = float(overall["hate_speech_score"])

    if "label" not in overall and "hate_speech_score" in overall:
        overall["label"] = derive_label_from_score(overall["hate_speech_score"])

    data["overall"] = overall

    facets = data.get("facets", {})
    fixed_facets = {}
    for key, value in facets.items():
        try:
            fixed_facets[key] = max(0, min(4, int(float(value))))
        except Exception:
            fixed_facets[key] = 0
    data["facets"] = fixed_facets

    targets = data.get("targets", {})
    data["targets"] = {key: to_bool(value) for key, value in targets.items()}

    return data


# ============================================================================
# BASE INSTRUCTION (same as training - WITHOUT examples)
# ============================================================================

INSTRUCTION = """
You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.
Do not include any explanations, markdown formatting, or text outside of the JSON object.

=========================
IMPORTANT INSTRUCTIONS
=========================
1. Output must be **valid JSON only** - no markdown, no commentary.
2. Use exact field names and types from the schema.
3. Type requirements:
   - `overall.label` → string ("hateful", "supportive", or "neutral")
   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)
   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)
   - All `targets` values → booleans (`true` or `false`)

=========================
OVERALL ASSESSMENT
=========================
Provide two values in the `"overall"` object:

1. **`"hate_speech_score"`** - A float representing hate speech intensity:
   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)
   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)
   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)

2. **`"label"`** - A categorical string derived from the hate_speech_score:
   - `"hateful"` if hate_speech_score > 0.5
   - `"neutral"` if hate_speech_score is between -1.0 and 0.5 (inclusive)
   - `"supportive"` if hate_speech_score < -1.0

=========================
FACETS (0-4 SCALE)
=========================
Each facet is an **integer** from 0 to 4:
- 0 = Absent
- 1 = Mild
- 2 = Clear
- 3 = Severe
- 4 = Extreme

Example: `"insult": 2` (not `2.0` or `"2"`)

=========================
TARGETS (BOOLEAN FLAGS)
=========================
Set to `true` only if that group is explicitly targeted in the text.

=========================
JSON SCHEMA (MUST MATCH EXACTLY)
=========================
{{
  "overall": {{
    "label": "neutral",
    "hate_speech_score": 0.00
  }},
  "facets": {{
    "sentiment": 0,
    "respect": 0,
    "insult": 0,
    "humiliate": 0,
    "status": 0,
    "dehumanize": 0,
    "violence": 0,
    "genocide": 0,
    "attack_defend": 0,
    "hatespeech": 0
  }},
  "targets": {{
    "target_race_asian": false,
    "target_race_black": false,
    "target_race_latinx": false,
    "target_race_middle_eastern": false,
    "target_race_native_american": false,
    "target_race_pacific_islander": false,
    "target_race_white": false,
    "target_race_other": false,
    "target_religion_atheist": false,
    "target_religion_buddhist": false,
    "target_religion_christian": false,
    "target_religion_hindu": false,
    "target_religion_jewish": false,
    "target_religion_mormon": false,
    "target_religion_muslim": false,
    "target_religion_other": false,
    "target_origin_immigrant": false,
    "target_origin_migrant_worker": false,
    "target_origin_specific_country": false,
    "target_origin_undocumented": false,
    "target_origin_other": false,
    "target_gender_men": false,
    "target_gender_non_binary": false,
    "target_gender_transgender_men": false,
    "target_gender_transgender_unspecified": false,
    "target_gender_transgender_women": false,
    "target_gender_women": false,
    "target_gender_other": false,
    "target_sexuality_bisexual": false,
    "target_sexuality_gay": false,
    "target_sexuality_lesbian": false,
    "target_sexuality_straight": false,
    "target_sexuality_other": false,
    "target_age_children": false,
    "target_age_teenagers": false,
    "target_age_young_adults": false,
    "target_age_middle_aged": false,
    "target_age_seniors": false,
    "target_age_other": false,
    "target_disability_physical": false,
    "target_disability_cognitive": false,
    "target_disability_neurological": false,
    "target_disability_visually_impaired": false,
    "target_disability_hearing_impaired": false,
    "target_disability_unspecific": false,
    "target_disability_other": false
  }}
}}
"""


# ============================================================================
# ICL FUNCTIONS (SAME AS TRAINING - THIS IS THE KEY FIX)
# ============================================================================

def setup_icl_sampling(train_dataset):
    """Setup inverse frequency weights for ICL sampling (same as training)"""
    target_keys = list(train_dataset[0]['targets'].keys())
    target_counts = Counter()

    for row in train_dataset:
        for k, v in row['targets'].items():
            if v:
                target_counts[k] += 1

    total_samples = len(train_dataset)
    inverse_freq = {k: total_samples / (target_counts[k] + 1) for k in target_keys}

    row_weights = []
    for row in train_dataset:
        active_targets = [k for k, v in row['targets'].items() if v]
        if active_targets:
            weight = max(inverse_freq[t] for t in active_targets)
        else:
            weight = 1.0
        row_weights.append(weight)

    row_weights = np.array(row_weights)
    row_probs = row_weights / row_weights.sum()

    return row_probs


def format_example(sample):
    """Format a single sample as INPUT/OUTPUT for few-shot (same as training)"""
    output = json.dumps({
        "overall": sample["overall"],
        "facets": sample["facets"],
        "targets": sample["targets"]
    }, ensure_ascii=False)
    return f'INPUT: "{sample["text"]}"\nOUTPUT:\n{output}'


def sample_few_shot_examples(train_dataset, row_probs, k=5):
    """Sample k examples with inverse frequency weighting (same as training)"""
    indices = np.random.choice(len(train_dataset), size=k, replace=False, p=row_probs)
    return [train_dataset[int(i)] for i in indices]


def build_system_prompt(few_shot_examples):
    """Build system prompt with few-shot examples (same as training)"""
    examples_text = "\n\n".join([
        f"EXAMPLE {i+1}:\n{format_example(ex)}"
        for i, ex in enumerate(few_shot_examples)
    ])

    return INSTRUCTION.strip() + f"\n\n=========================\nEXAMPLES\n=========================\n{examples_text}"


# ============================================================================
# LORA MODEL MERGING
# ============================================================================

def merge_lora_model():
    """
    Merges your LoRA adapter with the base Llama model.
    Run this ONCE before using vLLM.
    """
    print("="*60)
    print("STEP 1: Merging LoRA adapter with base Llama model")
    print("="*60)

    print("\n Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.2-1B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    print("\n Loading LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model,
        "/content/llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL"  # Your Llama LoRA checkpoint
    )

    print("\n Merging adapter with base model...")
    merged_model = model.merge_and_unload()

    print("\n Saving merged model...")
    output_dir = "llama-1B-merged-r32"
    merged_model.save_pretrained(output_dir)

    # Also save the tokenizer
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
    tokenizer.save_pretrained(output_dir)

    print(f"\n Merged model saved to: {output_dir}")
    print("   Now you can use this with vLLM!")

    return output_dir


# ============================================================================
# VLLM INFERENCE (FIXED - NOW USES SAME ICL FORMAT AS TRAINING)
# ============================================================================

def run_inference_vllm_merged(merged_model_path="llama-1B-merged-r32"):
    """
    Fast inference using vLLM with your merged Llama model.
    MUST run merge_lora_model() first!
    """
    print("\n" + "="*60)
    print("VLLM INFERENCE (MERGED LLAMA MODEL)")
    print("="*60)

    # =========================================================================
    # KEY FIX: Load training data and setup ICL sampling (same as training)
    # =========================================================================
    print("\n Loading training data for ICL sampling...")
    train_dataset = load_dataset("json", data_files="train_aggregated.jsonl", split="train")
    row_probs = setup_icl_sampling(train_dataset)
    print(f"   Training dataset size: {len(train_dataset)}")

    # Set seed for reproducibility (optional - remove if you want different examples each run)
    np.random.seed(42)

    print("\n Loading merged model with vLLM...")

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct", trust_remote_code=True)

    llm = LLM(
        model=merged_model_path,
        tokenizer="meta-llama/Llama-3.2-1B-Instruct",
        dtype="bfloat16",
        gpu_memory_utilization=0.9,
        max_model_len=8192,
        trust_remote_code=True,
    )

    sampling_params = SamplingParams(
        temperature=0.0,
        max_tokens=4096,
        stop=["<|eot_id|>", "</s>", "\n}\n", "}\n\n"],
        stop_token_ids=[tokenizer.eos_token_id],
    )

    print("\n Loading test data...")
    test_data = load_dataset("json", data_files="test_aggregated.jsonl", split="train")
    print(f"   Test dataset size: {len(test_data)}")

    # =========================================================================
    # KEY FIX: Build prompts with dynamic ICL examples (same format as training)
    # =========================================================================
    print("\n Preparing prompts with dynamic ICL examples...")
    prompts = []
    for sample in tqdm(test_data, desc="Building prompts"):
        # Sample 5 examples from training data (same as training)
        few_shot_examples = sample_few_shot_examples(train_dataset, row_probs, k=5)

        # Build system prompt with examples (same as training)
        system_prompt = build_system_prompt(few_shot_examples)

        # Create messages in Llama format (same structure as training)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": sample['text']}
        ]

        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        prompts.append(formatted_prompt)

    # Run inference
    print(f"\n Running inference on {len(prompts)} samples...")
    outputs = llm.generate(prompts, sampling_params)

    # Process results
    print("\n Processing results...")
    predictions = []
    failed_samples = []

    for i, output in enumerate(tqdm(outputs, desc="Processing outputs")):
        sample = test_data[i]
        generated_text = output.outputs[0].text.strip()

        try:
            predicted_json_str = extract_outer_json(generated_text)
            normalized_prediction = normalize_schema(predicted_json_str)
            normalized_expected = normalize_schema(sample)

            predictions.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": normalized_expected,
                "predicted": normalized_prediction,
                "raw_output": generated_text,
                "success": True
            })
        except Exception as e:
            failed_samples.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "expected": normalize_schema(sample),
                "raw_output": generated_text,
                "error": str(e)
            })
            predictions.append({
                "comment_id": sample.get("comment_id"),
                "text": sample.get("text"),
                "success": False
            })

    print("\n Saving results...")
    with open("llama_test_predictions_vllm.jsonl", "w") as f:
        for pred in predictions:
            f.write(json.dumps(pred) + "\n")

    if failed_samples:
        with open("llama_failed_predictions_vllm.jsonl", "w") as f:
            for failed in failed_samples:
                f.write(json.dumps(failed) + "\n")

    success_rate = (len(predictions) - len(failed_samples)) / len(predictions)
    print(f"\n Inference complete!")
    print(f"   Total: {len(predictions)}")
    print(f"   Success: {len(predictions) - len(failed_samples)} ({success_rate:.1%})")
    print(f"   Failed: {len(failed_samples)}")

    return predictions


# ============================================================================
# RUN PIPELINE
# ============================================================================

print(" Starting Llama LoRA Testing Pipeline")
merged_path = merge_lora_model()

# Step 2: Run inference with vLLM
predictions = run_inference_vllm_merged(merged_path)

print("\n" + "="*60)
print(" PIPELINE COMPLETE!")
print("="*60)
print(f"\n Output files created:")
print(f"   - llama_test_predictions_vllm.jsonl")
print(f"   - llama_failed_predictions_vllm.jsonl (if any failures)")
print(f"\n Next step: Run evaluation script on llama_test_predictions_vllm.jsonl")

"""# Evaluation"""

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    classification_report,
    mean_absolute_error,
    mean_squared_error,
    hamming_loss
)
from scipy.stats import spearmanr
import sys
def to_bool(x):
    """Convert various types to boolean"""
    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)):
        return bool(int(x))
    if isinstance(x, str):
        return x.strip().lower() in {"1", "true", "yes", "y", "t"}
    return False

def derive_label_from_score(score: float) -> str:
    """Derive label from hate_speech_score"""
    if score > 0.5:
        return "hateful"
    elif score < -1.0:
        return "supportive"
    else:
        return "neutral"

def normalize_schema(data):
    """Normalize the JSON schema to ensure type consistency"""
    if isinstance(data, str):
        data = json.loads(data)

    overall = data.get("overall", {})

    # Handle score field naming
    if "hate_speech_score" not in overall and "score" in overall:
        overall["hate_speech_score"] = float(overall.pop("score"))
    if "hate_speech_score" in overall:
        overall["hate_speech_score"] = float(overall["hate_speech_score"])

    # Derive label if missing
    if "label" not in overall and "hate_speech_score" in overall:
        overall["label"] = derive_label_from_score(overall["hate_speech_score"])

    data["overall"] = overall

    # Normalize facets to integers 0-4
    facets = data.get("facets", {})
    fixed_facets = {}
    for key, value in facets.items():
        try:
            fixed_facets[key] = max(0, min(4, int(float(value))))
        except Exception:
            fixed_facets[key] = 0
    data["facets"] = fixed_facets

    # Normalize targets to booleans
    targets = data.get("targets", {})
    data["targets"] = {key: to_bool(value) for key, value in targets.items()}

    return data

def safe_get_score(sample):
    """Safely extract hate_speech_score from sample"""
    overall = sample.get("overall", {})
    if "hate_speech_score" in overall:
        return float(overall["hate_speech_score"])
    if "score" in overall:
        return float(overall["score"])
    raise KeyError("Neither 'hate_speech_score' nor 'score' found in overall")


def evaluate_predictions(predictions_file="llama_test_predictions_vllm.jsonl"):
    """
    Main evaluation function for Llama model predictions
    """
    print("="*60)
    print("LLAMA HATE SPEECH MODEL EVALUATION")
    print("="*60)
    print(f"\n Using predictions from: {predictions_file}")

    # Load predictions
    predictions = []
    with open(predictions_file, "r") as f:
        for line in f:
            predictions.append(json.loads(line))

    # Separate successful and failed predictions
    valid_preds = [p for p in predictions if p.get("success")]
    failed_samples = [p for p in predictions if not p.get("success")]

    # Re-normalize both expected and predicted data to ensure type consistency
    for p in valid_preds:
        p["expected"] = normalize_schema(p["expected"])
        p["predicted"] = normalize_schema(p["predicted"])

    print(f"\n Data Summary:")
    print(f"   Total samples: {len(predictions)}")
    print(f"    Valid predictions: {len(valid_preds)} ({100*len(valid_preds)/len(predictions):.1f}%)")
    print(f"    Failed predictions: {len(failed_samples)} ({100*len(failed_samples)/len(predictions):.1f}%)")

    # Check if there are any valid predictions to evaluate
    if not valid_preds:
        print("\n  No valid predictions found. Cannot evaluate.")
        print("   Check your llama_failed_predictions_vllm.jsonl file for errors.")
        sys.exit(1)

    print("\n" + "="*60)
    print("EVALUATION RESULTS")
    print("="*60)

    # ========================================================================
    # OVERALL: Label Classification (hateful/neutral/supportive)
    # ========================================================================

    print("\n" + "="*60)
    print("1. OVERALL: Label Classification")
    print("="*60)

    y_true_labels = [p["expected"]["overall"]["label"] for p in valid_preds]
    y_pred_labels = [p["predicted"]["overall"]["label"] for p in valid_preds]

    overall_accuracy = accuracy_score(y_true_labels, y_pred_labels)
    overall_micro_f1 = f1_score(y_true_labels, y_pred_labels, average="micro", zero_division=0)
    overall_macro_f1 = f1_score(y_true_labels, y_pred_labels, average="macro", zero_division=0)
    overall_precision = precision_score(y_true_labels, y_pred_labels, average="macro", zero_division=0)
    overall_recall = recall_score(y_true_labels, y_pred_labels, average="macro", zero_division=0)

    print(f"Accuracy:        {overall_accuracy:.4f}")
    print(f"Macro F1:        {overall_macro_f1:.4f}")
    print(f"Micro F1:        {overall_micro_f1:.4f}")
    print(f"Macro Precision: {overall_precision:.4f}")
    print(f"Macro Recall:    {overall_recall:.4f}")
    print("\nPer-class breakdown:")
    print(classification_report(y_true_labels, y_pred_labels, zero_division=0))

    # Score correlation
    y_true_scores = [safe_get_score(p["expected"]) for p in valid_preds]
    y_pred_scores = [safe_get_score(p["predicted"]) for p in valid_preds]
    score_corr = spearmanr(y_true_scores, y_pred_scores).correlation
    print(f"Score Spearman correlation: {score_corr:.4f}")

    # ========================================================================
    # FACETS: Ordinal Ratings (0-4 scale)
    # ========================================================================

    print("\n" + "="*60)
    print("2. FACETS: Ordinal Ratings (0-4 scale)")
    print("="*60)

    facet_names = list(valid_preds[0]["expected"]["facets"].keys())
    facet_results = {}

    for facet in facet_names:
        y_true = np.array([p["expected"]["facets"].get(facet, 0) for p in valid_preds])
        y_pred = np.array([p["predicted"]["facets"].get(facet, 0) for p in valid_preds])

        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        spearman = spearmanr(y_true, y_pred).correlation
        exact_match = accuracy_score(y_true, y_pred)
        within_1 = np.mean(np.abs(y_true - y_pred) <= 1)

        facet_results[facet] = {
            "mae": mae, "mse": mse, "spearman": spearman,
            "exact_match": exact_match, "within_1_accuracy": within_1
        }

    mean_mae = np.mean([r["mae"] for r in facet_results.values()])
    mean_mse = np.mean([r["mse"] for r in facet_results.values()])
    mean_spearman = np.mean([r["spearman"] for r in facet_results.values()])
    mean_exact = np.mean([r["exact_match"] for r in facet_results.values()])
    mean_within_1 = np.mean([r["within_1_accuracy"] for r in facet_results.values()])

    print(f"Mean MAE:               {mean_mae:.4f}")
    print(f"Mean MSE:               {mean_mse:.4f}")
    print(f"Mean Spearman:          {mean_spearman:.4f}")
    print(f"Mean Exact Match:       {mean_exact:.4f}")
    print(f"Mean Within-1 Accuracy: {mean_within_1:.4f}")

    print("\nPer-facet breakdown:")
    print(f"{'Facet':<20} {'MAE':<8} {'Exact':<8} {'Within-1':<10} {'Spearman':<10}")
    print("-" * 60)
    for facet in facet_names:
        r = facet_results[facet]
        print(f"{facet:<20} {r['mae']:<8.3f} {r['exact_match']:<8.3f} {r['within_1_accuracy']:<10.3f} {r['spearman']:<10.3f}")

    # ========================================================================
    # TARGETS: Multi-label Classification
    # ========================================================================

    print("\n" + "="*60)
    print("3. TARGETS: Multi-label Classification")
    print("="*60)

    target_names = list(valid_preds[0]["expected"]["targets"].keys())

    y_true_targets = np.array([[int(p["expected"]["targets"].get(t, False)) for t in target_names] for p in valid_preds])
    y_pred_targets = np.array([[int(p["predicted"]["targets"].get(t, False)) for t in target_names] for p in valid_preds])

    targets_micro_f1 = f1_score(y_true_targets, y_pred_targets, average="micro", zero_division=0)
    targets_macro_f1 = f1_score(y_true_targets, y_pred_targets, average="macro", zero_division=0)
    targets_micro_precision = precision_score(y_true_targets, y_pred_targets, average="micro", zero_division=0)
    targets_micro_recall = recall_score(y_true_targets, y_pred_targets, average="micro", zero_division=0)
    targets_hamming = hamming_loss(y_true_targets, y_pred_targets)
    exact_match_ratio = np.mean(np.all(y_true_targets == y_pred_targets, axis=1))

    print(f"Micro F1:          {targets_micro_f1:.4f}")
    print(f"Macro F1:          {targets_macro_f1:.4f}")
    print(f"Micro Precision:   {targets_micro_precision:.4f}")
    print(f"Micro Recall:      {targets_micro_recall:.4f}")
    print(f"Hamming Loss:      {targets_hamming:.4f}")
    print(f"Exact Match Ratio: {exact_match_ratio:.4f} ({int(exact_match_ratio*len(valid_preds))}/{len(valid_preds)})")

    print("\nPer-target F1 scores (bottom 10):")
    per_target_f1 = {target: f1_score(y_true_targets[:, i], y_pred_targets[:, i], zero_division=0) for i, target in enumerate(target_names)}
    sorted_targets = sorted(per_target_f1.items(), key=lambda x: x[1])
    for target, f1 in sorted_targets[:10]:
        print(f"  {target:<40} {f1:.3f}")

    # ========================================================================
    # SAVE EVALUATION SUMMARY
    # ========================================================================

    print("\n" + "="*60)
    print("SAVING EVALUATION SUMMARY")
    print("="*60)

    eval_summary = {
        "metadata": {
            "prediction_file": predictions_file,
            "model_type": "Llama-3.2-1B-Instruct-LoRA",
            "total_samples": len(predictions),
            "valid_predictions": len(valid_preds),
            "failed_predictions": len(failed_samples),
            "success_rate": len(valid_preds) / len(predictions) if predictions else 0
        },
        "overall": {
            "accuracy": overall_accuracy,
            "macro_f1": overall_macro_f1,
            "micro_f1": overall_micro_f1,
            "precision": overall_precision,
            "recall": overall_recall,
            "score_spearman": score_corr
        },
        "facets": {
            "mean_mae": mean_mae,
            "mean_mse": mean_mse,
            "mean_spearman": mean_spearman,
            "mean_exact_match": mean_exact,
            "mean_within_1_accuracy": mean_within_1,
            "per_facet": facet_results
        },
        "targets": {
            "micro_f1": targets_micro_f1,
            "macro_f1": targets_macro_f1,
            "precision": targets_micro_precision,
            "recall": targets_micro_recall,
            "hamming_loss": targets_hamming,
            "exact_match_ratio": exact_match_ratio,
            "per_target_f1": per_target_f1
        }
    }

    output_file = "llama_evaluation_summary.json"
    with open(output_file, "w") as f:
        json.dump(eval_summary, f, indent=2)

    print(f" Summary saved to: {output_file}")
    if failed_samples:
        print(f"  {len(failed_samples)} samples failed - check llama_failed_predictions_vllm.jsonl")

    print("\n" + "="*60)
    print("EVALUATION COMPLETE!")
    print("="*60)

    # Print key metrics summary
    print("\n KEY METRICS SUMMARY:")
    print(f"   Overall Accuracy: {overall_accuracy:.2%}")
    print(f"   Overall Macro F1: {overall_macro_f1:.4f}")
    print(f"   Facets Mean MAE:  {mean_mae:.4f}")
    print(f"   Targets Micro F1: {targets_micro_f1:.4f}")

    return eval_summary

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    evaluate_predictions()

from google.colab import drive
drive.mount('/content/drive')

# Zip directly into Drive
!mkdir -p "/content/drive/MyDrive/colab_exports"
!zip -r "/content/drive/MyDrive/colab_exports/llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL.zip" "llama-1B-ctrl-alt-del-h8-r32-epochs5-alpha16-2EPOCH-ICL"

"""# Testing"""

# # Free memory from training
# del model
# del trainer
# torch.cuda.empty_cache()

# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from peft import PeftModel
# from datasets import load_dataset
# import json
# from tqdm import tqdm
# import numpy as np
# from sklearn.metrics import (
#     f1_score, precision_score, recall_score,
#     mean_absolute_error, mean_squared_error,
#     hamming_loss, accuracy_score, classification_report
# )
# from scipy.stats import spearmanr

"""### Utility functions"""

# def extract_outer_json(text: str) -> str:
#     """Extract JSON object from text that may contain extra tokens."""
#     s = text.find("{")
#     e = text.rfind("}")
#     if s == -1 or e == -1 or e <= s:
#         raise ValueError("No JSON object found in output")
#     return text[s:e+1]

# def to_bool(x):
#     """Robust boolean coercion that handles strings like 'false'."""
#     if isinstance(x, bool):
#         return x
#     if isinstance(x, (int, float)):
#         return bool(int(x))
#     if isinstance(x, str):
#         return x.strip().lower() in {"1", "true", "yes", "y", "t"}
#     return False

# def derive_label_from_score(score: float) -> str:
#     """Derive label using the same logic as training data."""
#     if score > 0.5:
#         return "hateful"
#     elif score < -1.0:
#         return "supportive"
#     else:
#         return "neutral"

# def normalize_schema(data):
#     """
#     Normalize prediction/expected schema to handle field name variations.
#     - Converts 'score' to 'hate_speech_score' if needed (for consistency)
#     - Ensures facets are integers 0-4
#     - Ensures targets are booleans
#     - Derives missing labels from scores
#     """
#     if isinstance(data, str):
#         data = json.loads(data)

#     # Normalize overall score and label
#     overall = data.get("overall", {})

#     # Unify score field name (prefer hate_speech_score for consistency with training)
#     if "hate_speech_score" not in overall and "score" in overall:
#         overall["hate_speech_score"] = float(overall.pop("score"))
#     if "hate_speech_score" in overall:
#         overall["hate_speech_score"] = float(overall["hate_speech_score"])

#     # Derive label if missing
#     if "label" not in overall and "hate_speech_score" in overall:
#         overall["label"] = derive_label_from_score(overall["hate_speech_score"])

#     data["overall"] = overall

#     # Normalize facets to integers 0-4 (handles "2.0" strings)
#     facets = data.get("facets", {})
#     fixed_facets = {}
#     for key, value in facets.items():
#         try:
#             fixed_facets[key] = max(0, min(4, int(float(value))))
#         except Exception:
#             fixed_facets[key] = 0  # Default to 0 on error
#     data["facets"] = fixed_facets

#     # Normalize targets to booleans (handles "false" strings)
#     targets = data.get("targets", {})
#     data["targets"] = {key: to_bool(value) for key, value in targets.items()}

#     return data

# def safe_get_score(obj):
#     """Safely extract score from overall, handling both field names."""
#     overall = obj.get("overall", {})
#     if "hate_speech_score" in overall:
#         return float(overall["hate_speech_score"])
#     elif "score" in overall:
#         return float(overall["score"])
#     else:
#         raise KeyError("Neither 'hate_speech_score' nor 'score' found in overall")

# print("Loading model...")
# base_model = AutoModelForCausalLM.from_pretrained(
#     "meta-llama/Llama-3.2-1B-Instruct",
#     device_map="auto",
#     torch_dtype=torch.bfloat16,
#     attn_implementation="eager",  # if this errors on your GPU/torch, change to "sdpa" or "flash_attention_2"
# )

# model = PeftModel.from_pretrained(
#     base_model, "/kaggle/input/llama-1b-ctrl-alt-del-h8/llama-1B-ctrl-alt-del-h8"
# )

# # Use the -it tokenizer (chat template). Set pad_token if needed.
# tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")
# if tokenizer.pad_token_id is None:
#     tokenizer.pad_token = tokenizer.eos_token

# pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     device_map="auto"
# )

"""## Single test"""

# sample_text = "Worthless piece of white trash. Keep him locked up forever. It's clear he has no use in society aside from being a chew toy."
# prompt_text = INSTRUCTION.format(text=sample_text)

# # Build a chat-formatted prompt using the tokenizer template
# prompt = tokenizer.apply_chat_template(
#     [{"role": "user", "content": prompt_text}],
#     tokenize=False,
#     add_generation_prompt=True
# )

# # Stop tokens (OK if some are missing)
# eos_id = tokenizer.eos_token_id
# eot_id = tokenizer.convert_tokens_to_ids("<end_of_turn>")
# stop_token_ids = [tid for tid in [eos_id, eot_id] if tid is not None]

# # ---- Generate ONCE ----
# with torch.inference_mode():
#     outputs = pipe(
#         prompt,
#         max_new_tokens=2048,    # DONT CHANGE THIS
#         do_sample=False,
#         temperature=0.0,
#         top_p=1.0,
#         eos_token_id=stop_token_ids,
#         pad_token_id=tokenizer.eos_token_id,
#         return_full_text=False
#     )

# generated = outputs[0]["generated_text"].strip()
# print("\n--- RAW MODEL OUTPUT ---")
# print(generated)

# # ---- Parse & normalize JSON ----
# parsed = normalize_schema(extract_outer_json(generated))
# print("\n--- PARSED JSON ---")
# print(json.dumps(parsed, indent=2))

"""# Evaluation on test set"""

# import json
# from tqdm import tqdm

# print("\nGenerating predictions for the test set...")

# # Initialize lists to store results and failures
# predictions = []
# failed_samples = []

# # -----------------------------------------------------------------
# # Slice the dataset to select only the first 50 samples for prediction.
# test_data_subset = test_data.select(range(50))
# print(f"INFO: Limiting prediction run to the first {len(test_data_subset)} samples.")
# # -----------------------------------------------------------------


# # Loop through each sample in the SUBSET with a progress bar
# for sample in tqdm(test_data_subset):
#     # Format the instructional prompt with the text from the current sample
#     prompt_text = INSTRUCTION.format(text=sample['text'])

#     # Apply the chat template to format the prompt correctly for the model
#     prompt = tokenizer.apply_chat_template(
#         [{"role": "user", "content": prompt_text}],
#         tokenize=False,
#         add_generation_prompt=True
#     )

#     # Use a try-except block to gracefully handle potential errors
#     try:
#         # Generate the model's response using the pipeline
#         outputs = pipe(
#             prompt,
#             max_new_tokens=2048,
#             do_sample=False,          # Use deterministic generation
#             temperature=0.0,
#             top_p=1.0,
#             eos_token_id=stop_token_ids,
#             pad_token_id=tokenizer.eos_token_id,
#             return_full_text=False    # We only need the generated part
#         )

#         # Get the raw text output from the model
#         generated_text = outputs[0]['generated_text'].strip()

#         # 1. Extract the JSON object from the potentially messy raw output
#         predicted_json_str = extract_outer_json(generated_text)

#         # 2. Normalize the predicted JSON to ensure it matches the required schema
#         normalized_prediction = normalize_schema(predicted_json_str)

#         # 3. Normalize the ground truth data from the sample for a fair comparison
#         normalized_expected = normalize_schema(sample)

#         # Store everything in a single record
#         predictions.append({
#             "comment_id": sample.get("comment_id"),
#             "text": sample.get("text"),
#             "expected": normalized_expected,   # The ground truth
#             "predicted": normalized_prediction, # The processed prediction
#             "raw_output": generated_text,       # The original, unfiltered model output
#             "success": True
#         })

#     except (json.JSONDecodeError, ValueError) as e:
#         # Handle cases where the output is not valid JSON or JSON can't be found
#         error_type = "Invalid JSON" if isinstance(e, json.JSONDecodeError) else "JSON extraction failed"

#         # Use a variable to safely access generated_text
#         raw_out = locals().get("generated_text", "Model produced no output")

#         failed_samples.append({
#             "comment_id": sample.get("comment_id"),
#             "text": sample.get("text"),
#             "expected": normalize_schema(sample), # Still save the expected value
#             "raw_output": raw_out,
#             "error": error_type,
#             "error_details": str(e)
#         })

#         # Add a failure record to the main predictions list to keep counts accurate
#         predictions.append({
#             "comment_id": sample.get("comment_id"),
#             "text": sample.get("text"),
#             "success": False
#         })

#     except Exception as e:
#         # Catch any other unexpected errors during generation
#         failed_samples.append({
#             "comment_id": sample.get("comment_id"),
#             "text": sample.get("text"),
#             "error": type(e).__name__,
#             "error_details": str(e)
#         })
#         predictions.append({
#             "comment_id": sample.get("comment_id"),
#             "text": sample.get("text"),
#             "success": False
#         })

# # --- Post-prediction summary and saving failures ---

# # Save the samples that failed for later inspection
# if failed_samples:
#     with open("failed_predictions_50_samples.jsonl", "w") as f:
#         for failed in failed_samples:
#             f.write(json.dumps(failed) + "\n")

# # Print a summary of how many predictions succeeded or failed
# total_predictions = len(predictions)
# num_failed = len(failed_samples)
# success_rate = (total_predictions - num_failed) / total_predictions if total_predictions > 0 else 0

# print(f"\n✅ Prediction generation complete.")
# print(f"Total samples processed: {total_predictions}")
# print(f"✔️ Successful predictions: {total_predictions - num_failed} ({success_rate:.1%})")
# if num_failed > 0:
#     print(f"❌ Failed predictions: {num_failed} ({1 - success_rate:.1%})")
#     print(f"   Details saved to: failed_predictions_50_samples.jsonl")







