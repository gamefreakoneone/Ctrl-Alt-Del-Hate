{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629aefd4",
   "metadata": {},
   "source": [
    "Midterm Report:\n",
    "Week 1\n",
    "1. Benchmarks: Gemma 3 (4 Billion) and Llama 3 (Llama 3.2 3B)\n",
    "2. Dataset: UCBerkely\n",
    "\n",
    "\n",
    "Week 2:\n",
    "1. Finetuning of Gemma 3 and Llama 3.2 3B with LoRA , and if necesasary QLoRA (maybe for endterm report). [Finetuning Benchmarks]\n",
    "2. Evaluation of other opensource models with more parameters (low priority)\n",
    "\n",
    "\n",
    "Week 3:\n",
    "TBD\n",
    "1. Start working on report and final exam.\n",
    "2. Prompting strategies for Gemma 3 and Llama 3.2 3B which can improve performance.\n",
    "3. Again perform evaluation on unfinetuned performed models with improved prompting strategies. (low priority)\n",
    "\n",
    "\n",
    "After Midterm Exam:\n",
    "1. We will focus on improving parameters of LoRA and QLoRA.\n",
    "2. Also work on improvements suggested in midterm report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d43d12",
   "metadata": {},
   "source": [
    "<!-- # Proposed Schema\n",
    "\n",
    "Input schema for finetuningb and ground truth:\n",
    "```\n",
    "{\n",
    "  \"overall\": {\"label\": \"none|abusive|hateful\", \"score\": 0.0},\n",
    "  \"facets\": {\n",
    "    \"insult\": 0, \"humiliate\": 0, \"dehumanize\": 0,\n",
    "    \"violence\": 0, \"genocide\": 0,\n",
    "    \"respect\": 0, \"status\": 0, \"attack_defend\": 0, \"sentiment\": 0\n",
    "  },\n",
    "  \"targets\": {\n",
    "    \"race_asian\": false, \"race_black\": false, \"race_white\": false, \"race_latinx\": false,\n",
    "    \"religion_muslim\": false, \"religion_jewish\": false, \"religion_hindu\": false, \"religion_buddhist\": false,\n",
    "    \"gender_men\": false, \"gender_women\": false, \"gender_transgender\": false, \"gender_non_binary\": false,\n",
    "    \"sexuality_gay\": false, \"sexuality_lesbian\": false, \"sexuality_bisexual\": false, \"sexuality_straight\": false,\n",
    "    \"disability_physical\": false, \"disability_cognitive\": false, \"origin_immigrant\": false, \"other\": false\n",
    "  },\n",
    "  \"explanation\" : \"text\",\n",
    "}``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb711d",
   "metadata": {},
   "source": [
    "# Gold Standard Dataset schema\n",
    "\n",
    "```\n",
    "{\n",
    "  // --- Core Identifiers ---\n",
    "  \"comment_id\": \"131169.0\", \n",
    "  \"text\": \"The original text content of the social media comment.\",\n",
    "\n",
    "  // --- Holistic Overall Assessment ---\n",
    "  \"overall\": {\n",
    "    \"label\": \"hateful\",             // A categorical label derived from the paper's official thresholds.\n",
    "                                    // - \"hateful\": If hate_speech_score > 0.5\n",
    "                                    // - \"supportive\": If hate_speech_score < -1.0\n",
    "                                    // - \"neutral\": If between -1.0 and 0.5 (inclusive)\n",
    "    \"hate_speech_score\": 1.52       // Ground truth \n",
    "  },\n",
    "\n",
    "  // --- Multi-Label Facet Analysis (The Core of the Task) ---\n",
    "  \"facets\": {\n",
    "    \"sentiment\": 3.0,               \n",
    "    \"respect\": 3.0,                 \n",
    "    \"insult\": 3.0,                  \n",
    "    \"humiliate\": 3.0,\n",
    "    \"status\": 0.0,\n",
    "    \"dehumanize\": 3.0,\n",
    "    \"violence\": 0.0,\n",
    "    \"genocide\": 0.0,\n",
    "    \"attack_defend\": 3.0,\n",
    "    \"hatespeech\": 1.0               \n",
    "  },\n",
    "\n",
    "  // --- Multi-Label Target Group Identification ---\n",
    "  \"targets\": {\n",
    "    \"race_asian\": true,             // A dictionary of booleans. `true` means the comment targets this group.\n",
    "    \"race_black\": false,            // This allows a single comment to target multiple groups simultaneously.\n",
    "    \"religion_muslim\": true,\n",
    "    \"gender_women\": false\n",
    "    // ... and so on for all other possible target groups.\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17a5fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset into a pandas DataFrame...\n",
      "Dataset loaded successfully with 135556 rows and 131 columns.\n",
      "Applying the transformation to each row of the DataFrame...\n",
      "\n",
      "Example of a processed record:\n",
      "{\n",
      "  \"comment_id\": 47777,\n",
      "  \"text\": \"Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \\\"Titanic\\\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\",\n",
      "  \"overall\": {\n",
      "    \"label\": \"supportive\",\n",
      "    \"hate_speech_score\": -3.9\n",
      "  },\n",
      "  \"facets\": {\n",
      "    \"sentiment\": 0.0,\n",
      "    \"respect\": 0.0,\n",
      "    \"insult\": 0.0,\n",
      "    \"humiliate\": 0.0,\n",
      "    \"status\": 2.0,\n",
      "    \"dehumanize\": 0.0,\n",
      "    \"violence\": 0.0,\n",
      "    \"genocide\": 0.0,\n",
      "    \"attack_defend\": 0.0,\n",
      "    \"hatespeech\": 0.0\n",
      "  },\n",
      "  \"targets\": {\n",
      "    \"target_race_asian\": true,\n",
      "    \"target_race_black\": true,\n",
      "    \"target_race_latinx\": true,\n",
      "    \"target_race_middle_eastern\": true,\n",
      "    \"target_race_native_american\": true,\n",
      "    \"target_race_pacific_islander\": true,\n",
      "    \"target_race_white\": true,\n",
      "    \"target_race_other\": false,\n",
      "    \"target_religion_atheist\": false,\n",
      "    \"target_religion_buddhist\": false,\n",
      "    \"target_religion_christian\": false,\n",
      "    \"target_religion_hindu\": false,\n",
      "    \"target_religion_jewish\": false,\n",
      "    \"target_religion_mormon\": false,\n",
      "    \"target_religion_muslim\": false,\n",
      "    \"target_religion_other\": false,\n",
      "    \"target_origin_immigrant\": false,\n",
      "    \"target_origin_migrant_worker\": false,\n",
      "    \"target_origin_specific_country\": false,\n",
      "    \"target_origin_undocumented\": false,\n",
      "    \"target_origin_other\": false,\n",
      "    \"target_gender_men\": false,\n",
      "    \"target_gender_non_binary\": false,\n",
      "    \"target_gender_transgender_men\": false,\n",
      "    \"target_gender_transgender_unspecified\": false,\n",
      "    \"target_gender_transgender_women\": false,\n",
      "    \"target_gender_women\": false,\n",
      "    \"target_gender_other\": false,\n",
      "    \"target_sexuality_bisexual\": false,\n",
      "    \"target_sexuality_gay\": false,\n",
      "    \"target_sexuality_lesbian\": false,\n",
      "    \"target_sexuality_straight\": false,\n",
      "    \"target_sexuality_other\": false,\n",
      "    \"target_disability_physical\": false,\n",
      "    \"target_disability_cognitive\": false,\n",
      "    \"target_disability_neurological\": false,\n",
      "    \"target_disability_visually_impaired\": false,\n",
      "    \"target_disability_hearing_impaired\": false,\n",
      "    \"target_disability_unspecific\": false,\n",
      "    \"target_disability_other\": false\n",
      "  }\n",
      "}\n",
      "\n",
      "Saving the 135556 records to 'gold_benchmark_dataset.jsonl'...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Note: This script requires the following libraries to be installed:\n",
    "# pip install pandas pyarrow fsspec huggingface_hub\n",
    "\n",
    "# 1. Load the dataset into a pandas DataFrame (your preferred method)\n",
    "print(\"Loading the dataset into a pandas DataFrame...\")\n",
    "try:\n",
    "    df = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "    print(\"Please ensure you have run 'pip install pandas pyarrow fsspec huggingface_hub'\")\n",
    "    exit()\n",
    "\n",
    "# 2. Define the columns we want to extract\n",
    "# These are the actual facet and target columns present in the dataset\n",
    "FACET_COLUMNS = [\n",
    "    'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', \n",
    "    'violence', 'genocide', 'attack_defend', 'hatespeech'\n",
    "]\n",
    "\n",
    "TARGET_COLUMNS = [\n",
    "    'target_race_asian', 'target_race_black', 'target_race_latinx', \n",
    "    'target_race_middle_eastern', 'target_race_native_american', \n",
    "    'target_race_pacific_islander', 'target_race_white', 'target_race_other',\n",
    "    'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian',\n",
    "    'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon',\n",
    "    'target_religion_muslim', 'target_religion_other', 'target_origin_immigrant',\n",
    "    'target_origin_migrant_worker', 'target_origin_specific_country',\n",
    "    'target_origin_undocumented', 'target_origin_other', 'target_gender_men',\n",
    "    'target_gender_non_binary', 'target_gender_transgender_men',\n",
    "    'target_gender_transgender_unspecified', 'target_gender_transgender_women',\n",
    "    'target_gender_women', 'target_gender_other', 'target_sexuality_bisexual',\n",
    "    'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight',\n",
    "    'target_sexuality_other', 'target_disability_physical', 'target_disability_cognitive',\n",
    "    'target_disability_neurological', 'target_disability_visually_impaired',\n",
    "    'target_disability_hearing_impaired', 'target_disability_unspecific',\n",
    "    'target_disability_other'\n",
    "]\n",
    "\n",
    "# Helper function to classify the score based on the paper's logic\n",
    "def classify_score(score):\n",
    "    if score > 0.5:\n",
    "        return \"hateful\"\n",
    "    if score < -1.0:\n",
    "        return \"supportive\"\n",
    "    return \"neutral\"\n",
    "\n",
    "# 3. Create the function to transform each row of the DataFrame\n",
    "def create_gold_standard_record(row):\n",
    "    # Create the 'overall' object\n",
    "    overall = {\n",
    "        \"label\": classify_score(row['hate_speech_score']),\n",
    "        \"hate_speech_score\": row['hate_speech_score']\n",
    "    }\n",
    "    \n",
    "    # Create the 'facets' object\n",
    "    facets = {col: row[col] for col in FACET_COLUMNS}\n",
    "    \n",
    "    # Create the 'targets' object (ensuring columns exist)\n",
    "    targets = {col: bool(row[col]) for col in TARGET_COLUMNS if col in row}\n",
    "    \n",
    "    # Assemble the final record as a dictionary\n",
    "    return {\n",
    "        \"comment_id\": row['comment_id'],\n",
    "        \"text\": row['text'],\n",
    "        \"overall\": overall,\n",
    "        \"facets\": facets,\n",
    "        \"targets\": targets\n",
    "    }\n",
    "\n",
    "# 4. Apply the function to each row of the DataFrame\n",
    "print(\"Applying the transformation to each row of the DataFrame...\")\n",
    "gold_records = df.apply(create_gold_standard_record, axis=1).tolist()\n",
    "\n",
    "print(\"\\nExample of a processed record:\")\n",
    "print(json.dumps(gold_records[0], indent=2))\n",
    "\n",
    "output_file = \"gold_benchmark_dataset.jsonl\"\n",
    "print(f\"\\nSaving the {len(gold_records)} records to '{output_file}'...\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for record in gold_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86013981",
   "metadata": {},
   "source": [
    "# Next Steps: Benchmarking with unfinetuned models and calculate metrics against gold standard\n",
    "\n",
    "\n",
    "Essentially pass in a prompt similar to this? I am not sure but I think you can see where I am coming from.\n",
    "```You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly adheres to the schema below. Do not include explanations, markdown, or any other text outside of the JSON object.\n",
    "\n",
    "--- SCORING RUBRIC ---\n",
    "Use integers from 0 to 4 for all facets based on this scale:\n",
    "0 = Absent: The facet is not present at all.\n",
    "1 = Mild: The facet is subtly present or implied.\n",
    "2 = Clear: The facet is unambiguously present.\n",
    "3 = Severe: The facet is present in an explicit and severe manner.\n",
    "4 = Extreme: The facet is present in an egregious or extreme manner.\n",
    "\n",
    "--- SCHEMA DEFINITIONS ---\n",
    "1.  **overall.label**: Predict one of three string categories: \"hateful\", \"supportive\", or \"neutral\".\n",
    "2.  **overall.score**: Predict a confidence score as a float from 0.0 to 1.0, where 1.0 is maximum confidence in the \"hateful\" label.\n",
    "3.  **facets**: A dictionary of 10 facets, each scored with an integer from 0-4.\n",
    "4.  **targets**: A dictionary of all possible target groups. Set the value to `true` only if the group is explicitly targeted, otherwise `false`.\n",
    "\n",
    "--- JSON FORMAT TO RETURN ---\n",
    "{\n",
    "  \"overall\": {\n",
    "    \"label\": \"hateful\",\n",
    "    \"score\": 0.0\n",
    "  },\n",
    "  \"facets\": {\n",
    "    \"sentiment\": 0,\n",
    "    \"respect\": 0,\n",
    "    \"insult\": 0,\n",
    "    \"humiliate\": 0,\n",
    "    \"status\": 0,\n",
    "    \"dehumanize\": 0,\n",
    "    \"violence\": 0,\n",
    "    \"genocide\": 0,\n",
    "    \"attack_defend\": 0,\n",
    "    \"hatespeech\": 0\n",
    "  },\n",
    "  \"targets\": {\n",
    "    \"target_race_asian\": false,\n",
    "    \"target_race_black\": false,\n",
    "    \"target_race_latinx\": false,\n",
    "    \"target_race_middle_eastern\": false,\n",
    "    \"target_race_native_american\": false,\n",
    "    \"target_race_pacific_islander\": false,\n",
    "    \"target_race_white\": false,\n",
    "    \"target_race_other\": false,\n",
    "    \"target_religion_ athe ist\": false,\n",
    "    \"target_religion_buddhist\": false,\n",
    "    \"target_religion_christian\": false,\n",
    "    \"target_religion_hindu\": false,\n",
    "    \"target_religion_jewish\": false,\n",
    "    \"target_religion_mormon\": false,\n",
    "    \"target_religion_muslim\": false,\n",
    "    \"target_religion_other\": false,\n",
    "    \"target_origin_immigrant\": false,\n",
    "    \"target_origin_migrant_worker\": false,\n",
    "    \"target_origin_specific_country\": false,\n",
    "    \"target_origin_undocumented\": false,\n",
    "    \"target_origin_other\": false,\n",
    "    \"target_gender_men\": false,\n",
    "    \"target_gender_non_binary\": false,\n",
    "    \"target_gender_transgender_men\": false,\n",
    "    \"target_gender_transgender_unspecified\": false,\n",
    "    \"target_gender_transgender_women\": false,\n",
    "    \"target_gender_women\": false,\n",
    "    \"target_gender_other\": false,\n",
    "    \"target_sexuality_bisexual\": false,\n",
    "    \"target_sexuality_gay\": false,\n",
    "    \"target_sexuality_lesbian\": false,\n",
    "    \"target_sexuality_straight\": false,\n",
    "    \"target_sexuality_other\": false,\n",
    "    \"target_age_children\": false,\n",
    "    \"target_age_teenagers\": false,\n",
    "    \"target_age_young_adults\": false,\n",
    "    \"target_age_middle_aged\": false,\n",
    "    \"target_age_seniors\": false,\n",
    "    \"target_age_other\": false,\n",
    "    \"target_disability_physical\": false,\n",
    "    \"target_disability_cognitive\": false,\n",
    "    \"target_disability_neurological\": false,\n",
    "    \"target_disability_visually_impaired\": false,\n",
    "    \"target_disability_hearing_impaired\": false,\n",
    "    \"target_disability_unspecific\": false,\n",
    "    \"target_disability_other\": false\n",
    "  }\n",
    "}\n",
    "\n",
    "--- TEXT TO ANALYZE ---\n",
    "{text}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e3a7c",
   "metadata": {},
   "source": [
    "# Benchmarks:\n",
    "\n",
    "Overall Assessment:\n",
    "Label: We'll calculate the Macro F1-Score and Accuracy for the overall.label field (\"hateful\", \"neutral\", \"supportive\")\n",
    "\n",
    "Facet Analysis (Ordinal):\n",
    "Accuracy: We'll calculate the Mean Absolute Error (MAE) for each of the 10 facets by comparing our gold integer values to the model's predicted integer values.\n",
    "\n",
    "Target Identification (Multi-Label):\n",
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439748a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
