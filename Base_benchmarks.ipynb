{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23a535eef25a41db8105536e1228228e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b566359e3c5e44fb91b283f15e97aac6",
              "IPY_MODEL_7011253ef869459389d56e7853d3fd98",
              "IPY_MODEL_ddd851356c984b6e9417e5a520540dbc"
            ],
            "layout": "IPY_MODEL_5e6a89ccae734c01bd4c8c5c4f7af5a3"
          }
        },
        "b566359e3c5e44fb91b283f15e97aac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25a8e812c8f94ce08e3d9b194dd78001",
            "placeholder": "​",
            "style": "IPY_MODEL_36e371dd7ef54e209fa114c7d5aaca3c",
            "value": "Map: 100%"
          }
        },
        "7011253ef869459389d56e7853d3fd98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b73e088e624b4f05bc390d9b7a04659c",
            "max": 3957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d787654f97bb4e398f5f6dd0186c26f7",
            "value": 3957
          }
        },
        "ddd851356c984b6e9417e5a520540dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3338b4b8ddb7476da4e69d2f54809b25",
            "placeholder": "​",
            "style": "IPY_MODEL_6c18776ec46d4c4b9d7be30b8b16a0c0",
            "value": " 3957/3957 [00:01&lt;00:00, 3518.50 examples/s]"
          }
        },
        "5e6a89ccae734c01bd4c8c5c4f7af5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a8e812c8f94ce08e3d9b194dd78001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e371dd7ef54e209fa114c7d5aaca3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b73e088e624b4f05bc390d9b7a04659c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d787654f97bb4e398f5f6dd0186c26f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3338b4b8ddb7476da4e69d2f54809b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c18776ec46d4c4b9d7be30b8b16a0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a3a49cbb73f45a7b37d7577e54d5155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad50c748ce1d40e39ae68da196e7fa7d",
              "IPY_MODEL_ce726c3e396d4f8198077a7dbe711fec",
              "IPY_MODEL_44530f50060c4a648e5e5729bc384a2b"
            ],
            "layout": "IPY_MODEL_80aa3368ae9e445296dbc27b036ec6c3"
          }
        },
        "ad50c748ce1d40e39ae68da196e7fa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bdba850e35a4a0fa4e3ad7fc60569d7",
            "placeholder": "​",
            "style": "IPY_MODEL_3bdc601eeadf4a6493edf58e426628ee",
            "value": "Adding requests: 100%"
          }
        },
        "ce726c3e396d4f8198077a7dbe711fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1e5deaabdd41e6bff6a7644fbc926b",
            "max": 3957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09c263aabad3443fa5bfd747484fbb81",
            "value": 3957
          }
        },
        "44530f50060c4a648e5e5729bc384a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0c28858a879424d885cff016ad0a742",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3d6b1c28644965bc6da44ad7c0f337",
            "value": " 3957/3957 [00:08&lt;00:00, 443.64it/s]"
          }
        },
        "80aa3368ae9e445296dbc27b036ec6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bdba850e35a4a0fa4e3ad7fc60569d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bdc601eeadf4a6493edf58e426628ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b1e5deaabdd41e6bff6a7644fbc926b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09c263aabad3443fa5bfd747484fbb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0c28858a879424d885cff016ad0a742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3d6b1c28644965bc6da44ad7c0f337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "847ac5e176d646c4a3d13afa796e256d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6dd2af541964bb9bc845da9412f56a0",
              "IPY_MODEL_973fb97169aa4ce980c759c9d944c3d5",
              "IPY_MODEL_c03ce28c8d454d5094f785a45d0651f7"
            ],
            "layout": "IPY_MODEL_eb56eaa7d51049239812693780a82048"
          }
        },
        "b6dd2af541964bb9bc845da9412f56a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6532935f49c0486fbc7b50703a0deb1d",
            "placeholder": "​",
            "style": "IPY_MODEL_4f91f71179d74cf5aa6a077886bde6de",
            "value": "Processed prompts: 100%"
          }
        },
        "973fb97169aa4ce980c759c9d944c3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4823b422e80a47a9af01e30cbafcbe7f",
            "max": 3957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fbfa6205c4b4f8794c3bb18bc27ee90",
            "value": 3957
          }
        },
        "c03ce28c8d454d5094f785a45d0651f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c3f196036d94680bd5d7c9df1e075d2",
            "placeholder": "​",
            "style": "IPY_MODEL_9921945f7b654835a38ac5b5eda84e34",
            "value": " 3957/3957 [07:43&lt;00:00,  9.74it/s, est. speed input: 11394.25 toks/s, output: 8400.24 toks/s]"
          }
        },
        "eb56eaa7d51049239812693780a82048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6532935f49c0486fbc7b50703a0deb1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f91f71179d74cf5aa6a077886bde6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4823b422e80a47a9af01e30cbafcbe7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fbfa6205c4b4f8794c3bb18bc27ee90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c3f196036d94680bd5d7c9df1e075d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9921945f7b654835a38ac5b5eda84e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7d1430541847eba80b5af5ae292962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cea4b9da1f3549fe9c5c90e9782a56bd",
              "IPY_MODEL_37a53060315d4a2cb7f7a136490ad844",
              "IPY_MODEL_0d5153125d8749fe952d415630d023f3"
            ],
            "layout": "IPY_MODEL_648fef65c7de4d4480d9f02b752561fd"
          }
        },
        "cea4b9da1f3549fe9c5c90e9782a56bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaa3269f5b3345e3a36433bc5327a970",
            "placeholder": "​",
            "style": "IPY_MODEL_9580f242f46e47ca9fd77142e7b3abfc",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "37a53060315d4a2cb7f7a136490ad844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5458adfe12e484db9c91380c2920c83",
            "max": 54528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bc47a5f4fcb4fc0b9186268194ea555",
            "value": 54528
          }
        },
        "0d5153125d8749fe952d415630d023f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d84bdf5d2ad14c40885a28653306b0b5",
            "placeholder": "​",
            "style": "IPY_MODEL_97aac808f8ea460ebab0dadd1d8245bd",
            "value": " 54.5k/54.5k [00:00&lt;00:00, 6.18MB/s]"
          }
        },
        "648fef65c7de4d4480d9f02b752561fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa3269f5b3345e3a36433bc5327a970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9580f242f46e47ca9fd77142e7b3abfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5458adfe12e484db9c91380c2920c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc47a5f4fcb4fc0b9186268194ea555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d84bdf5d2ad14c40885a28653306b0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97aac808f8ea460ebab0dadd1d8245bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cd623d34b7d4321aa94c2d12a51c1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d6778dfc99d4374af29f684d8ba6f52",
              "IPY_MODEL_748ef0f7020640ef98382ef2c60dc5cb",
              "IPY_MODEL_d73d83a09a3f4b0dabeec4c454556aec"
            ],
            "layout": "IPY_MODEL_6a6d7192145e42728b608e9493b33dcd"
          }
        },
        "8d6778dfc99d4374af29f684d8ba6f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f4e0cf67cd44af8157cc33030c0e77",
            "placeholder": "​",
            "style": "IPY_MODEL_1c283be0a674423e824009c22324f671",
            "value": "tokenizer.json: 100%"
          }
        },
        "748ef0f7020640ef98382ef2c60dc5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c739c101f594956923c539d44ec8529",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3b1e7e8636842038f0b873b5bd46abd",
            "value": 9085657
          }
        },
        "d73d83a09a3f4b0dabeec4c454556aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42aa3f09a50a43778a6aff68c0e1e35c",
            "placeholder": "​",
            "style": "IPY_MODEL_e528c733360e4aa7b75e7f0c7f16c33f",
            "value": " 9.09M/9.09M [00:01&lt;00:00, 7.07MB/s]"
          }
        },
        "6a6d7192145e42728b608e9493b33dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f4e0cf67cd44af8157cc33030c0e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c283be0a674423e824009c22324f671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c739c101f594956923c539d44ec8529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b1e7e8636842038f0b873b5bd46abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42aa3f09a50a43778a6aff68c0e1e35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e528c733360e4aa7b75e7f0c7f16c33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f5f5151c5b04fa4a125a4f6fcd5a445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ab569703d1842e3b3a23222c44705dc",
              "IPY_MODEL_68f298b63d2640499870a6762e556edd",
              "IPY_MODEL_089cef4e3aa24da38dcf5e9124d64ebb"
            ],
            "layout": "IPY_MODEL_71a1615e829c49bc8061ffdb9eb6fa0a"
          }
        },
        "1ab569703d1842e3b3a23222c44705dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_148bcdad4d364d5197d242f98aa5c7c6",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e82ac7ffb64c888caa5144b81b4ef9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "68f298b63d2640499870a6762e556edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_306194d117974ac4a16301a62cf6f7c4",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_438c324e4da4466d86b7a5c83d98ddb8",
            "value": 296
          }
        },
        "089cef4e3aa24da38dcf5e9124d64ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d29eb2b460794ab396a0e124e42c7ded",
            "placeholder": "​",
            "style": "IPY_MODEL_a110ef07c8b94c8880099d65703626f3",
            "value": " 296/296 [00:00&lt;00:00, 40.7kB/s]"
          }
        },
        "71a1615e829c49bc8061ffdb9eb6fa0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148bcdad4d364d5197d242f98aa5c7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e82ac7ffb64c888caa5144b81b4ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "306194d117974ac4a16301a62cf6f7c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438c324e4da4466d86b7a5c83d98ddb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d29eb2b460794ab396a0e124e42c7ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a110ef07c8b94c8880099d65703626f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c61ef7b5898e4cf48f7bec4c0e0fc979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7aa294b9040d41778f69fcc911458a0f",
              "IPY_MODEL_be2a58cea3b54cfd8d09fecb7ba0e05c",
              "IPY_MODEL_3f5d2306fc554e5ebe2d46c1256f7569"
            ],
            "layout": "IPY_MODEL_05992e20521a45e19e952a473a9fe263"
          }
        },
        "7aa294b9040d41778f69fcc911458a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d22e62cd3cc24ee08a5eb71d5f823966",
            "placeholder": "​",
            "style": "IPY_MODEL_7c0f76b0e51748ab906f2ca82606eae0",
            "value": "config.json: 100%"
          }
        },
        "be2a58cea3b54cfd8d09fecb7ba0e05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783fd3eb7d514fe08b17472075236d9e",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d74bb7708044412824bd0681ff549bb",
            "value": 877
          }
        },
        "3f5d2306fc554e5ebe2d46c1256f7569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92def963f6647edbcdc2413061db075",
            "placeholder": "​",
            "style": "IPY_MODEL_d92e2fa6ed674f708582bc1a952cd1fa",
            "value": " 877/877 [00:00&lt;00:00, 113kB/s]"
          }
        },
        "05992e20521a45e19e952a473a9fe263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22e62cd3cc24ee08a5eb71d5f823966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c0f76b0e51748ab906f2ca82606eae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "783fd3eb7d514fe08b17472075236d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d74bb7708044412824bd0681ff549bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c92def963f6647edbcdc2413061db075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d92e2fa6ed674f708582bc1a952cd1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8a3904783484be0a5a4bb168bdf02d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba8fd6686699459593dda35a00816e70",
              "IPY_MODEL_7dc5937141d54f46a1e20dda119b8b3a",
              "IPY_MODEL_1a87d3db69f24ac6803f600177e12fd8"
            ],
            "layout": "IPY_MODEL_11447d610767440ba31f48654c6e47ac"
          }
        },
        "ba8fd6686699459593dda35a00816e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04d76158e0e643aeacfbff847965e25e",
            "placeholder": "​",
            "style": "IPY_MODEL_a518fd5b153a457aae632da75c2d6cf4",
            "value": "generation_config.json: 100%"
          }
        },
        "7dc5937141d54f46a1e20dda119b8b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33f094fda753417d8c72f33ad11718a2",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cec14cd50402479dad5767dd331aa39f",
            "value": 189
          }
        },
        "1a87d3db69f24ac6803f600177e12fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4874d4958a247bb9ae41de2fb27bc02",
            "placeholder": "​",
            "style": "IPY_MODEL_d69f87e571ad453193cafc3e0fdb1e91",
            "value": " 189/189 [00:00&lt;00:00, 26.3kB/s]"
          }
        },
        "11447d610767440ba31f48654c6e47ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d76158e0e643aeacfbff847965e25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a518fd5b153a457aae632da75c2d6cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33f094fda753417d8c72f33ad11718a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec14cd50402479dad5767dd331aa39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4874d4958a247bb9ae41de2fb27bc02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d69f87e571ad453193cafc3e0fdb1e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "816d35dfb4784f78b39a3176345ddc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b59cd5e2ee94dd9b9f23a4cea9f2136",
              "IPY_MODEL_228271766ef44df3bb7364bd267c54cb",
              "IPY_MODEL_bb8c7266b8f741a2bcdecba045b1c31e"
            ],
            "layout": "IPY_MODEL_ee736b489a38435ea12e082452b045fc"
          }
        },
        "3b59cd5e2ee94dd9b9f23a4cea9f2136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973953d9794547fc81095d9bb914529f",
            "placeholder": "​",
            "style": "IPY_MODEL_60a8c5be8973402ba3f1fb998708c7c5",
            "value": "Adding requests: 100%"
          }
        },
        "228271766ef44df3bb7364bd267c54cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e174881d681453f80f34671f76dae14",
            "max": 3957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b30b7fc423254bd6a3ecfbdf3f6d1db7",
            "value": 3957
          }
        },
        "bb8c7266b8f741a2bcdecba045b1c31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd09e5a11dac4a2494a6189ce9399274",
            "placeholder": "​",
            "style": "IPY_MODEL_55627cfd965f4e9fb7221b90b1c64ec2",
            "value": " 3957/3957 [00:13&lt;00:00, 158.65it/s]"
          }
        },
        "ee736b489a38435ea12e082452b045fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "973953d9794547fc81095d9bb914529f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a8c5be8973402ba3f1fb998708c7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e174881d681453f80f34671f76dae14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30b7fc423254bd6a3ecfbdf3f6d1db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd09e5a11dac4a2494a6189ce9399274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55627cfd965f4e9fb7221b90b1c64ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38a922ab906a4ca3ab17368ebe601b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82e79bafa7ae4b3ea997efbddf150734",
              "IPY_MODEL_3f5cec9cda844fe08dd761729eaebc46",
              "IPY_MODEL_8da40534e08141358481fa95c2809b1d"
            ],
            "layout": "IPY_MODEL_03c1ae23eb2b4e7181bfdf69caf7bbb6"
          }
        },
        "82e79bafa7ae4b3ea997efbddf150734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62805cc9cccd42a79bb85ee78b133742",
            "placeholder": "​",
            "style": "IPY_MODEL_110a74af1edc4116baf31b2260a1e6d4",
            "value": "Processed prompts: 100%"
          }
        },
        "3f5cec9cda844fe08dd761729eaebc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eac02c4bd2f4151a20d43a1f709c976",
            "max": 3957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf0ed0a6c9c5497ba474126974a2f7c8",
            "value": 3957
          }
        },
        "8da40534e08141358481fa95c2809b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc94f2b03b444ca8a1f9086d2572812",
            "placeholder": "​",
            "style": "IPY_MODEL_e535e4cacd724975887f98b20ff31d83",
            "value": " 3957/3957 [01:47&lt;00:00,  5.02it/s, est. speed input: 43713.82 toks/s, output: 12575.17 toks/s]"
          }
        },
        "03c1ae23eb2b4e7181bfdf69caf7bbb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "62805cc9cccd42a79bb85ee78b133742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "110a74af1edc4116baf31b2260a1e6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eac02c4bd2f4151a20d43a1f709c976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0ed0a6c9c5497ba474126974a2f7c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fc94f2b03b444ca8a1f9086d2572812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e535e4cacd724975887f98b20ff31d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install \"torch>=2.4.0\" tensorboard\n",
        "\n",
        "%pip install \"transformers>=4.51.3\"\n",
        "%pip install vllm\n",
        "%pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.21.0\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece\n",
        "\n",
        "# COMMENT IN: if you are running on a GPU that supports BF16 data type and flash attn, such as NVIDIA L4 or NVIDIA A100\n",
        "%pip install flash-attn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kID_k3024m8w",
        "outputId": "63b27f7e-04e7-4620-fc6d-aae35d6b1a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (79.0.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0) (3.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Requirement already satisfied: transformers>=4.51.3 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.51.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.51.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.51.3) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.51.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.51.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.51.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.51.3) (2025.10.5)\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.55.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.57.1)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.121.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.11.10)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.11.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.11.3)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.7.30)\n",
            "Requirement already satisfied: outlines_core==0.2.11 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.11)\n",
            "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (5.6.3)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.25 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.1.25)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.12/dist-packages (from vllm) (0.2.1.1.post6)\n",
            "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.17.1)\n",
            "Requirement already satisfied: mistral_common>=1.8.2 in /usr/local/lib/python3.12/dist-packages (from mistral_common[audio,image]>=1.8.2->vllm) (1.8.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
            "Requirement already satisfied: setuptools<80,>=77.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (79.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.11.0)\n",
            "Requirement already satisfied: depyf==0.19.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.2)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.12/dist-packages (from vllm) (1.1.1)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.16.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\n",
            "Requirement already satisfied: pybase64 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.4.2)\n",
            "Requirement already satisfied: cbor2 in /usr/local/lib/python3.12/dist-packages (from vllm) (5.7.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from vllm) (1.3.7)\n",
            "Requirement already satisfied: openai-harmony>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.0.8)\n",
            "Requirement already satisfied: numba==0.61.2 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.61.2)\n",
            "Requirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.51.1)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.0+cu126)\n",
            "Requirement already satisfied: xformers==0.0.32.post1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.0.32.post1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.11.0->vllm) (2.4.6)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.19.0->vllm) (0.3.8)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.11.3->vllm) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba==0.61.2->vllm) (0.44.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->vllm) (3.4.0)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.49.3)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.3)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.14)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (4.25.1)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.1->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->vllm) (0.4.2)\n",
            "Requirement already satisfied: click!=8.3.0,>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (8.2.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.10.5)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.21.1->vllm) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.2->vllm) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
            "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\n",
            "Requirement already satisfied: rich-toolkit>=0.14.8 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.1)\n",
            "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->vllm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.28.0)\n",
            "Requirement already satisfied: pycountry>=23 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (24.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (0.13.1)\n",
            "Requirement already satisfied: soxr>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (1.0.0)\n",
            "Requirement already satisfied: rignore>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.43.0)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Collecting datasets==3.3.2\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate==1.4.0\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate==0.4.3\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting bitsandbytes==0.45.3\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting trl==0.21.0\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting peft==0.14.0\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (0.70.16)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.3.2)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (3.13.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.3.2) (6.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.4.0) (0.6.2)\n",
            "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.21.0) (4.57.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==3.3.2) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.3.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.3.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.3.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.3.2) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (79.0.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.4.0) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl==0.21.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl==0.21.0) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.3.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.3.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.3.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.3.2) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.4.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.4.0) (3.0.3)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, fsspec, datasets, bitsandbytes, accelerate, trl, peft, evaluate\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.11.0\n",
            "    Uninstalling accelerate-1.11.0:\n",
            "      Successfully uninstalled accelerate-1.11.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.4.0 bitsandbytes-0.45.3 datasets-3.3.2 evaluate-0.4.3 fsspec-2024.12.0 peft-0.14.0 protobuf-6.33.0 trl-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c8c910a56e014d088a4257327d100f3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (79.0.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "lRWOMFAG4y2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Note: This script requires the following libraries to be installed:\n",
        "# pip install pandas pyarrow fsspec huggingface_hub\n",
        "\n",
        "# 1. Load the dataset into a pandas DataFrame (your preferred method)\n",
        "print(\"Loading the dataset into a pandas DataFrame...\")\n",
        "try:\n",
        "    df = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
        "    print(f\"Dataset loaded successfully with {len(df)} rows and {len(df.columns)} columns.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")\n",
        "    print(\"Please ensure you have run 'pip install pandas pyarrow fsspec huggingface_hub'\")\n",
        "    exit()\n",
        "\n",
        "# 2. Define the columns we want to extract\n",
        "# These are the actual facet and target columns present in the dataset\n",
        "FACET_COLUMNS = [\n",
        "    'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize',\n",
        "    'violence', 'genocide', 'attack_defend', 'hatespeech'\n",
        "]\n",
        "\n",
        "TARGET_COLUMNS = [\n",
        "    \"target_race_asian\", \"target_race_black\", \"target_race_latinx\", \"target_race_middle_eastern\",\n",
        "    \"target_race_native_american\", \"target_race_pacific_islander\", \"target_race_white\", \"target_race_other\",\n",
        "    \"target_religion_atheist\", \"target_religion_buddhist\", \"target_religion_christian\", \"target_religion_hindu\",\n",
        "    \"target_religion_jewish\", \"target_religion_mormon\", \"target_religion_muslim\", \"target_religion_other\",\n",
        "    \"target_origin_immigrant\",  \"target_origin_migrant_worker\", \"target_origin_specific_country\",\n",
        "    \"target_origin_undocumented\", \"target_origin_other\", \"target_gender_men\", \"target_gender_non_binary\",\n",
        "    \"target_gender_transgender_men\", \"target_gender_transgender_unspecified\", \"target_gender_transgender_women\",\n",
        "    \"target_gender_women\", \"target_gender_other\", \"target_sexuality_bisexual\", \"target_sexuality_gay\",\n",
        "    \"target_sexuality_lesbian\", \"target_sexuality_straight\", \"target_sexuality_other\",\n",
        "    \"target_age_children\", \"target_age_teenagers\", \"target_age_young_adults\", \"target_age_middle_aged\", \"target_age_seniors\",\n",
        "    \"target_age_other\",  \"target_disability_physical\", \"target_disability_cognitive\",\n",
        "    \"target_disability_neurological\", \"target_disability_visually_impaired\",\n",
        "    \"target_disability_hearing_impaired\", \"target_disability_unspecific\", \"target_disability_other\"\n",
        "]\n",
        "\n",
        "# Helper function to classify the score based on the paper's logic\n",
        "def classify_score(score):\n",
        "    if score > 0.5:\n",
        "        return \"hateful\"\n",
        "    if score < -1.0:\n",
        "        return \"supportive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "# 3. Create the function to transform each row of the DataFrame\n",
        "def create_gold_standard_record(row):\n",
        "    # Create the 'overall' object\n",
        "    overall = {\n",
        "        \"label\": classify_score(row['hate_speech_score']),\n",
        "        \"hate_speech_score\": row['hate_speech_score']\n",
        "    }\n",
        "\n",
        "    # Create the 'facets' object\n",
        "    facets = {col: row[col] for col in FACET_COLUMNS}\n",
        "\n",
        "    # Create the 'targets' object (ensuring columns exist)\n",
        "    targets = {col: bool(row[col]) for col in TARGET_COLUMNS if col in row}\n",
        "\n",
        "    # Assemble the final record as a dictionary\n",
        "    return {\n",
        "        \"comment_id\": row['comment_id'],\n",
        "        \"text\": row['text'],\n",
        "        \"overall\": overall,\n",
        "        \"facets\": facets,\n",
        "        \"targets\": targets\n",
        "    }\n",
        "\n",
        "# 4. Apply the function to each row of the DataFrame\n",
        "print(\"Applying the transformation to each row of the DataFrame...\")\n",
        "gold_records = df.apply(create_gold_standard_record, axis=1).tolist()\n",
        "\n",
        "print(\"\\nExample of a processed record:\")\n",
        "print(json.dumps(gold_records[0], indent=2))\n",
        "\n",
        "output_file = \"gold_benchmark_dataset.jsonl\"\n",
        "print(f\"\\nSaving the {len(gold_records)} records to '{output_file}'...\")\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for record in gold_records:\n",
        "        f.write(json.dumps(record) + '\\n')\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "INPUT_FILE = \"gold_benchmark_dataset.jsonl\"\n",
        "TRAIN_FILE = \"train_aggregated.jsonl\"\n",
        "VAL_FILE = \"val_aggregated.jsonl\"\n",
        "TEST_FILE = \"test_aggregated.jsonl\"\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "def save_jsonl(data, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "def aggregate_annotations(records):\n",
        "    \"\"\"Aggregate multiple annotations for the same comment into one record.\"\"\"\n",
        "    if len(records) == 1:\n",
        "        return records[0]\n",
        "\n",
        "    aggregated = {\n",
        "        \"comment_id\": records[0][\"comment_id\"],\n",
        "        \"text\": records[0][\"text\"],\n",
        "        \"overall\": {},\n",
        "        \"facets\": {},\n",
        "        \"targets\": {}\n",
        "    }\n",
        "\n",
        "    # Aggregate hate_speech_score (mean)\n",
        "    hate_scores = [r[\"overall\"][\"hate_speech_score\"] for r in records]\n",
        "    avg_hate_score = sum(hate_scores) / len(hate_scores)\n",
        "\n",
        "    # Classify based on averaged score\n",
        "    if avg_hate_score > 0.5:\n",
        "        label = \"hateful\"\n",
        "    elif avg_hate_score < -1.0:\n",
        "        label = \"supportive\"\n",
        "    else:\n",
        "        label = \"neutral\"\n",
        "\n",
        "    aggregated[\"overall\"] = {\n",
        "        \"label\": label,\n",
        "        \"hate_speech_score\": avg_hate_score\n",
        "    }\n",
        "\n",
        "    # Aggregate facets (mean then round)\n",
        "    facet_keys = records[0][\"facets\"].keys()\n",
        "    for key in facet_keys:\n",
        "        values = [r[\"facets\"][key] for r in records]\n",
        "        aggregated[\"facets\"][key] = round(sum(values) / len(values))\n",
        "\n",
        "    # Aggregate targets (OR logic)\n",
        "    target_keys = records[0][\"targets\"].keys()\n",
        "    for key in target_keys:\n",
        "        aggregated[\"targets\"][key] = any(r[\"targets\"][key] for r in records)\n",
        "\n",
        "    return aggregated\n",
        "\n",
        "def main():\n",
        "    # Load all data\n",
        "    data = load_data(INPUT_FILE)\n",
        "    print(f\"Loaded {len(data)} total records\")\n",
        "\n",
        "    # Group by comment_id\n",
        "    comment_groups = {}\n",
        "    for record in data:\n",
        "        comment_id = record[\"comment_id\"]\n",
        "        if comment_id not in comment_groups:\n",
        "            comment_groups[comment_id] = []\n",
        "        comment_groups[comment_id].append(record)\n",
        "\n",
        "    unique_comment_ids = list(comment_groups.keys())\n",
        "    print(f\"Found {len(unique_comment_ids)} unique comments\")\n",
        "\n",
        "    # Split comment_ids: 80% train, 10% val, 10% test\n",
        "    train_ids, temp_ids = train_test_split(unique_comment_ids, test_size=0.2, random_state=42)\n",
        "    val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
        "\n",
        "    # ✅ CHANGE: Aggregate ALL splits (not just val/test)\n",
        "    train_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in train_ids]\n",
        "    val_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in val_ids]\n",
        "    test_data = [aggregate_annotations(comment_groups[comment_id]) for comment_id in test_ids]\n",
        "\n",
        "    # Save datasets\n",
        "    save_jsonl(train_data, TRAIN_FILE)\n",
        "    save_jsonl(val_data, VAL_FILE)\n",
        "    save_jsonl(test_data, TEST_FILE)\n",
        "\n",
        "    print(f\"\\n✅ Data split complete:\")\n",
        "    print(f\"Train: {len(train_data)} aggregated records ({len(train_ids)} unique comments)\")\n",
        "    print(f\"Val: {len(val_data)} aggregated records ({len(val_ids)} unique comments)\")\n",
        "    print(f\"Test: {len(test_data)} aggregated records ({len(test_ids)} unique comments)\")\n",
        "    print(f\"Total: {len(train_data) + len(val_data) + len(test_data)} records\")\n",
        "    print(f\"Reduction: {len(data)} → {len(train_data) + len(val_data) + len(test_data)} (~{100*(1 - (len(train_data) + len(val_data) + len(test_data))/len(data)):.1f}% reduction)\")\n",
        "\n",
        "    # Verify no overlap\n",
        "    assert len(set(train_ids) & set(val_ids)) == 0, \"Train/Val overlap detected!\"\n",
        "    assert len(set(train_ids) & set(test_ids)) == 0, \"Train/Test overlap detected!\"\n",
        "    assert len(set(val_ids) & set(test_ids)) == 0, \"Val/Test overlap detected!\"\n",
        "    print(\"\\n✅ Verified: No comment_id overlap between splits\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsyFTAvr5Vdl",
        "outputId": "6a06755f-d5ff-4051-c8c8-8594fee41dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the dataset into a pandas DataFrame...\n",
            "Dataset loaded successfully with 135556 rows and 131 columns.\n",
            "Applying the transformation to each row of the DataFrame...\n",
            "\n",
            "Example of a processed record:\n",
            "{\n",
            "  \"comment_id\": 47777,\n",
            "  \"text\": \"Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \\\"Titanic\\\" who was telling her story!!! And I wouldn't have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!\",\n",
            "  \"overall\": {\n",
            "    \"label\": \"supportive\",\n",
            "    \"hate_speech_score\": -3.9\n",
            "  },\n",
            "  \"facets\": {\n",
            "    \"sentiment\": 0.0,\n",
            "    \"respect\": 0.0,\n",
            "    \"insult\": 0.0,\n",
            "    \"humiliate\": 0.0,\n",
            "    \"status\": 2.0,\n",
            "    \"dehumanize\": 0.0,\n",
            "    \"violence\": 0.0,\n",
            "    \"genocide\": 0.0,\n",
            "    \"attack_defend\": 0.0,\n",
            "    \"hatespeech\": 0.0\n",
            "  },\n",
            "  \"targets\": {\n",
            "    \"target_race_asian\": true,\n",
            "    \"target_race_black\": true,\n",
            "    \"target_race_latinx\": true,\n",
            "    \"target_race_middle_eastern\": true,\n",
            "    \"target_race_native_american\": true,\n",
            "    \"target_race_pacific_islander\": true,\n",
            "    \"target_race_white\": true,\n",
            "    \"target_race_other\": false,\n",
            "    \"target_religion_atheist\": false,\n",
            "    \"target_religion_buddhist\": false,\n",
            "    \"target_religion_christian\": false,\n",
            "    \"target_religion_hindu\": false,\n",
            "    \"target_religion_jewish\": false,\n",
            "    \"target_religion_mormon\": false,\n",
            "    \"target_religion_muslim\": false,\n",
            "    \"target_religion_other\": false,\n",
            "    \"target_origin_immigrant\": false,\n",
            "    \"target_origin_migrant_worker\": false,\n",
            "    \"target_origin_specific_country\": false,\n",
            "    \"target_origin_undocumented\": false,\n",
            "    \"target_origin_other\": false,\n",
            "    \"target_gender_men\": false,\n",
            "    \"target_gender_non_binary\": false,\n",
            "    \"target_gender_transgender_men\": false,\n",
            "    \"target_gender_transgender_unspecified\": false,\n",
            "    \"target_gender_transgender_women\": false,\n",
            "    \"target_gender_women\": false,\n",
            "    \"target_gender_other\": false,\n",
            "    \"target_sexuality_bisexual\": false,\n",
            "    \"target_sexuality_gay\": false,\n",
            "    \"target_sexuality_lesbian\": false,\n",
            "    \"target_sexuality_straight\": false,\n",
            "    \"target_sexuality_other\": false,\n",
            "    \"target_age_children\": false,\n",
            "    \"target_age_teenagers\": false,\n",
            "    \"target_age_young_adults\": false,\n",
            "    \"target_age_middle_aged\": false,\n",
            "    \"target_age_seniors\": false,\n",
            "    \"target_age_other\": false,\n",
            "    \"target_disability_physical\": false,\n",
            "    \"target_disability_cognitive\": false,\n",
            "    \"target_disability_neurological\": false,\n",
            "    \"target_disability_visually_impaired\": false,\n",
            "    \"target_disability_hearing_impaired\": false,\n",
            "    \"target_disability_unspecific\": false,\n",
            "    \"target_disability_other\": false\n",
            "  }\n",
            "}\n",
            "\n",
            "Saving the 135556 records to 'gold_benchmark_dataset.jsonl'...\n",
            "Loaded 135556 total records\n",
            "Found 39565 unique comments\n",
            "\n",
            "✅ Data split complete:\n",
            "Train: 31652 aggregated records (31652 unique comments)\n",
            "Val: 3956 aggregated records (3956 unique comments)\n",
            "Test: 3957 aggregated records (3957 unique comments)\n",
            "Total: 39565 records\n",
            "Reduction: 135556 → 39565 (~70.8% reduction)\n",
            "\n",
            "✅ Verified: No comment_id overlap between splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Your instruction template\n",
        "INSTRUCTION = \"\"\"\n",
        "You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.\n",
        "Do not include any explanations, markdown formatting, or text outside of the JSON object.\n",
        "\n",
        "=========================\n",
        "IMPORTANT INSTRUCTIONS\n",
        "=========================\n",
        "1. Output must be **valid JSON only** - no markdown, no commentary.\n",
        "2. Use exact field names and types from the schema.\n",
        "3. Type requirements:\n",
        "   - `overall.label` → string (\"hateful\", \"supportive\", or \"neutral\")\n",
        "   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)\n",
        "   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)\n",
        "   - All `targets` values → booleans (`true` or `false`)\n",
        "\n",
        "=========================\n",
        "OVERALL ASSESSMENT\n",
        "=========================\n",
        "Provide two values in the `\"overall\"` object:\n",
        "\n",
        "1. **`\"hate_speech_score\"`** - A float representing hate speech intensity:\n",
        "   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)\n",
        "   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)\n",
        "   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)\n",
        "\n",
        "2. **`\"label\"`** - A categorical string derived from the hate_speech_score:\n",
        "   - `\"hateful\"` if hate_speech_score > 0.5\n",
        "   - `\"neutral\"` if hate_speech_score is between -1.0 and 0.5 (inclusive)\n",
        "   - `\"supportive\"` if hate_speech_score < -1.0\n",
        "\n",
        "=========================\n",
        "FACETS (0-4 SCALE)\n",
        "=========================\n",
        "Each facet is an **integer** from 0 to 4:\n",
        "- 0 = Absent\n",
        "- 1 = Mild\n",
        "- 2 = Clear\n",
        "- 3 = Severe\n",
        "- 4 = Extreme\n",
        "\n",
        "Example: `\"insult\": 2` (not `2.0` or `\"2\"`)\n",
        "\n",
        "=========================\n",
        "TARGETS (BOOLEAN FLAGS)\n",
        "=========================\n",
        "Set to `true` only if that group is explicitly targeted in the text.\n",
        "\n",
        "=========================\n",
        "JSON SCHEMA (MUST MATCH EXACTLY)\n",
        "=========================\n",
        "{{\n",
        "  \"overall\": {{\n",
        "    \"label\": \"neutral\",\n",
        "    \"hate_speech_score\": 0.00\n",
        "  }},\n",
        "  \"facets\": {{\n",
        "    \"sentiment\": 0,\n",
        "    \"respect\": 0,\n",
        "    \"insult\": 0,\n",
        "    \"humiliate\": 0,\n",
        "    \"status\": 0,\n",
        "    \"dehumanize\": 0,\n",
        "    \"violence\": 0,\n",
        "    \"genocide\": 0,\n",
        "    \"attack_defend\": 0,\n",
        "    \"hatespeech\": 0\n",
        "  }},\n",
        "  \"targets\": {{\n",
        "    \"target_race_asian\": false,\n",
        "    \"target_race_black\": false,\n",
        "    \"target_race_latinx\": false,\n",
        "    \"target_race_middle_eastern\": false,\n",
        "    \"target_race_native_american\": false,\n",
        "    \"target_race_pacific_islander\": false,\n",
        "    \"target_race_white\": false,\n",
        "    \"target_race_other\": false,\n",
        "    \"target_religion_atheist\": false,\n",
        "    \"target_religion_buddhist\": false,\n",
        "    \"target_religion_christian\": false,\n",
        "    \"target_religion_hindu\": false,\n",
        "    \"target_religion_jewish\": false,\n",
        "    \"target_religion_mormon\": false,\n",
        "    \"target_religion_muslim\": false,\n",
        "    \"target_religion_other\": false,\n",
        "    \"target_origin_immigrant\": false,\n",
        "    \"target_origin_migrant_worker\": false,\n",
        "    \"target_origin_specific_country\": false,\n",
        "    \"target_origin_undocumented\": false,\n",
        "    \"target_origin_other\": false,\n",
        "    \"target_gender_men\": false,\n",
        "    \"target_gender_non_binary\": false,\n",
        "    \"target_gender_transgender_men\": false,\n",
        "    \"target_gender_transgender_unspecified\": false,\n",
        "    \"target_gender_transgender_women\": false,\n",
        "    \"target_gender_women\": false,\n",
        "    \"target_gender_other\": false,\n",
        "    \"target_sexuality_bisexual\": false,\n",
        "    \"target_sexuality_gay\": false,\n",
        "    \"target_sexuality_lesbian\": false,\n",
        "    \"target_sexuality_straight\": false,\n",
        "    \"target_sexuality_other\": false,\n",
        "    \"target_age_children\": false,\n",
        "    \"target_age_teenagers\": false,\n",
        "    \"target_age_young_adults\": false,\n",
        "    \"target_age_middle_aged\": false,\n",
        "    \"target_age_seniors\": false,\n",
        "    \"target_age_other\": false,\n",
        "    \"target_disability_physical\": false,\n",
        "    \"target_disability_cognitive\": false,\n",
        "    \"target_disability_neurological\": false,\n",
        "    \"target_disability_visually_impaired\": false,\n",
        "    \"target_disability_hearing_impaired\": false,\n",
        "    \"target_disability_unspecific\": false,\n",
        "    \"target_disability_other\": false\n",
        "  }}\n",
        "}}\n",
        "\n",
        "=========================\n",
        "TEXT TO ANALYZE\n",
        "=========================\n",
        "{{text}}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "oZM-Gv-u6Ksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "test_data = load_dataset(\"json\", data_files=\"test_aggregated.jsonl\", split=\"train\")\n",
        "\n",
        "def create_conversation(sample):\n",
        "    model_response = json.dumps({\n",
        "        \"overall\": sample[\"overall\"],\n",
        "        \"facets\": sample[\"facets\"],\n",
        "        \"targets\": sample[\"targets\"]\n",
        "    }, ensure_ascii=False)\n",
        "\n",
        "    user_content = f\"{INSTRUCTION}\\n\\n<text>\\n{sample['text']}\\n</text>\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"model\", \"content\": model_response}\n",
        "        ],\n",
        "        \"comment_id\": sample[\"comment_id\"],  # Keep this for evaluation!\n",
        "        \"text\": sample[\"text\"]\n",
        "    }\n",
        "\n",
        "test_data = test_data.map(create_conversation, remove_columns=test_data.column_names)\n",
        "print(f\"Test dataset size: {len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "23a535eef25a41db8105536e1228228e",
            "b566359e3c5e44fb91b283f15e97aac6",
            "7011253ef869459389d56e7853d3fd98",
            "ddd851356c984b6e9417e5a520540dbc",
            "5e6a89ccae734c01bd4c8c5c4f7af5a3",
            "25a8e812c8f94ce08e3d9b194dd78001",
            "36e371dd7ef54e209fa114c7d5aaca3c",
            "b73e088e624b4f05bc390d9b7a04659c",
            "d787654f97bb4e398f5f6dd0186c26f7",
            "3338b4b8ddb7476da4e69d2f54809b25",
            "6c18776ec46d4c4b9d7be30b8b16a0c0"
          ]
        },
        "id": "t9IJ6hrm5UpF",
        "outputId": "14379a4e-2bcd-4cea-baef-87aa5002d044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3957 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23a535eef25a41db8105536e1228228e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset size: 3957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "ObuLAMDy5ycw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612,
          "referenced_widgets": [
            "4a3a49cbb73f45a7b37d7577e54d5155",
            "ad50c748ce1d40e39ae68da196e7fa7d",
            "ce726c3e396d4f8198077a7dbe711fec",
            "44530f50060c4a648e5e5729bc384a2b",
            "80aa3368ae9e445296dbc27b036ec6c3",
            "5bdba850e35a4a0fa4e3ad7fc60569d7",
            "3bdc601eeadf4a6493edf58e426628ee",
            "9b1e5deaabdd41e6bff6a7644fbc926b",
            "09c263aabad3443fa5bfd747484fbb81",
            "b0c28858a879424d885cff016ad0a742",
            "4c3d6b1c28644965bc6da44ad7c0f337",
            "847ac5e176d646c4a3d13afa796e256d",
            "b6dd2af541964bb9bc845da9412f56a0",
            "973fb97169aa4ce980c759c9d944c3d5",
            "c03ce28c8d454d5094f785a45d0651f7",
            "eb56eaa7d51049239812693780a82048",
            "6532935f49c0486fbc7b50703a0deb1d",
            "4f91f71179d74cf5aa6a077886bde6de",
            "4823b422e80a47a9af01e30cbafcbe7f",
            "0fbfa6205c4b4f8794c3bb18bc27ee90",
            "2c3f196036d94680bd5d7c9df1e075d2",
            "9921945f7b654835a38ac5b5eda84e34"
          ]
        },
        "id": "CTexd3MR4Zio",
        "outputId": "c4554661-16a4-4753-c6a7-5ce4c17ce213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BENCHMARKING BASE GEMMA-3-1B-IT MODEL\n",
            "============================================================\n",
            "\n",
            "📥 Loading tokenizer...\n",
            "🚀 Loading base model with vLLM...\n",
            "INFO 11-08 05:38:14 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'google/gemma-3-1b-it'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-08 05:38:15 [model.py:547] Resolved architecture: Gemma3ForCausalLM\n",
            "INFO 11-08 05:38:15 [model.py:1510] Using max model len 4096\n",
            "INFO 11-08 05:38:15 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 11-08 05:39:05 [llm.py:306] Supported_tasks: ['generate']\n",
            "📂 Loading test data...\n",
            "   Test dataset size: 3957\n",
            "📝 Preparing prompts...\n",
            "⚡ Running inference on 3957 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/3957 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a3a49cbb73f45a7b37d7577e54d5155"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/3957 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "847ac5e176d646c4a3d13afa796e256d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Processing outputs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 3957/3957 [00:02<00:00, 1703.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving results...\n",
            "   Failed samples saved to: base_gemma_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "✅ BENCHMARK COMPLETE!\n",
            "============================================================\n",
            "   Total samples:    3957\n",
            "   Successful:       25 (0.6%)\n",
            "   Failed:           3932\n",
            "   Results saved to: base_gemma_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Your instruction template\n",
        "INSTRUCTION = \"\"\"\n",
        "You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.\n",
        "Do not include any explanations, markdown formatting, or text outside of the JSON object.\n",
        "\n",
        "=========================\n",
        "IMPORTANT INSTRUCTIONS\n",
        "=========================\n",
        "1. Output must be **valid JSON only** - no markdown, no commentary.\n",
        "2. Use exact field names and types from the schema.\n",
        "3. Type requirements:\n",
        "   - `overall.label` → string (\"hateful\", \"supportive\", or \"neutral\")\n",
        "   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)\n",
        "   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)\n",
        "   - All `targets` values → booleans (`true` or `false`)\n",
        "\n",
        "=========================\n",
        "OVERALL ASSESSMENT\n",
        "=========================\n",
        "Provide two values in the `\"overall\"` object:\n",
        "\n",
        "1. **`\"hate_speech_score\"`** - A float representing hate speech intensity:\n",
        "   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)\n",
        "   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)\n",
        "   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)\n",
        "\n",
        "2. **`\"label\"`** - A categorical string derived from the hate_speech_score:\n",
        "   - `\"hateful\"` if hate_speech_score > 0.5\n",
        "   - `\"neutral\"` if hate_speech_score is between -1.0 and 0.5 (inclusive)\n",
        "   - `\"supportive\"` if hate_speech_score < -1.0\n",
        "\n",
        "=========================\n",
        "FACETS (0-4 SCALE)\n",
        "=========================\n",
        "Each facet is an **integer** from 0 to 4:\n",
        "- 0 = Absent\n",
        "- 1 = Mild\n",
        "- 2 = Clear\n",
        "- 3 = Severe\n",
        "- 4 = Extreme\n",
        "\n",
        "Example: `\"insult\": 2` (not `2.0` or `\"2\"`)\n",
        "\n",
        "=========================\n",
        "TARGETS (BOOLEAN FLAGS)\n",
        "=========================\n",
        "Set to `true` only if that group is explicitly targeted in the text.\n",
        "\n",
        "=========================\n",
        "JSON SCHEMA (MUST MATCH EXACTLY)\n",
        "=========================\n",
        "{{\n",
        "  \"overall\": {{\n",
        "    \"label\": \"neutral\",\n",
        "    \"hate_speech_score\": 0.00\n",
        "  }},\n",
        "  \"facets\": {{\n",
        "    \"sentiment\": 0,\n",
        "    \"respect\": 0,\n",
        "    \"insult\": 0,\n",
        "    \"humiliate\": 0,\n",
        "    \"status\": 0,\n",
        "    \"dehumanize\": 0,\n",
        "    \"violence\": 0,\n",
        "    \"genocide\": 0,\n",
        "    \"attack_defend\": 0,\n",
        "    \"hatespeech\": 0\n",
        "  }},\n",
        "  \"targets\": {{\n",
        "    \"target_race_asian\": false,\n",
        "    \"target_race_black\": false,\n",
        "    \"target_race_latinx\": false,\n",
        "    \"target_race_middle_eastern\": false,\n",
        "    \"target_race_native_american\": false,\n",
        "    \"target_race_pacific_islander\": false,\n",
        "    \"target_race_white\": false,\n",
        "    \"target_race_other\": false,\n",
        "    \"target_religion_atheist\": false,\n",
        "    \"target_religion_buddhist\": false,\n",
        "    \"target_religion_christian\": false,\n",
        "    \"target_religion_hindu\": false,\n",
        "    \"target_religion_jewish\": false,\n",
        "    \"target_religion_mormon\": false,\n",
        "    \"target_religion_muslim\": false,\n",
        "    \"target_religion_other\": false,\n",
        "    \"target_origin_immigrant\": false,\n",
        "    \"target_origin_migrant_worker\": false,\n",
        "    \"target_origin_specific_country\": false,\n",
        "    \"target_origin_undocumented\": false,\n",
        "    \"target_origin_other\": false,\n",
        "    \"target_gender_men\": false,\n",
        "    \"target_gender_non_binary\": false,\n",
        "    \"target_gender_transgender_men\": false,\n",
        "    \"target_gender_transgender_unspecified\": false,\n",
        "    \"target_gender_transgender_women\": false,\n",
        "    \"target_gender_women\": false,\n",
        "    \"target_gender_other\": false,\n",
        "    \"target_sexuality_bisexual\": false,\n",
        "    \"target_sexuality_gay\": false,\n",
        "    \"target_sexuality_lesbian\": false,\n",
        "    \"target_sexuality_straight\": false,\n",
        "    \"target_sexuality_other\": false,\n",
        "    \"target_age_children\": false,\n",
        "    \"target_age_teenagers\": false,\n",
        "    \"target_age_young_adults\": false,\n",
        "    \"target_age_middle_aged\": false,\n",
        "    \"target_age_seniors\": false,\n",
        "    \"target_age_other\": false,\n",
        "    \"target_disability_physical\": false,\n",
        "    \"target_disability_cognitive\": false,\n",
        "    \"target_disability_neurological\": false,\n",
        "    \"target_disability_visually_impaired\": false,\n",
        "    \"target_disability_hearing_impaired\": false,\n",
        "    \"target_disability_unspecific\": false,\n",
        "    \"target_disability_other\": false\n",
        "  }}\n",
        "}}\n",
        "\n",
        "=========================\n",
        "TEXT TO ANALYZE\n",
        "=========================\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "def extract_outer_json(text: str) -> str:\n",
        "    \"\"\"Extract JSON object from model output, stripping markdown and handling malformed JSON.\"\"\"\n",
        "    # Remove markdown code fences\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    s = text.find(\"{\")\n",
        "    if s == -1:\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "\n",
        "    # Find matching closing brace\n",
        "    brace_count = 0\n",
        "    for i in range(s, len(text)):\n",
        "        if text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                # Found the matching closing brace\n",
        "                return text[s:i+1]\n",
        "\n",
        "    # Fallback to rfind if no matching brace found\n",
        "    e = text.rfind(\"}\")\n",
        "    if e == -1 or e <= s:\n",
        "        raise ValueError(\"No closing brace found for JSON object\")\n",
        "    return text[s:e+1]\n",
        "\n",
        "def to_bool(x):\n",
        "    \"\"\"Robust boolean conversion.\"\"\"\n",
        "    if isinstance(x, bool):\n",
        "        return x\n",
        "    if isinstance(x, (int, float)):\n",
        "        return bool(int(x))\n",
        "    if isinstance(x, str):\n",
        "        return x.strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
        "    return False\n",
        "\n",
        "def derive_label_from_score(score: float) -> str:\n",
        "    \"\"\"Derive label from hate_speech_score.\"\"\"\n",
        "    if score > 0.5:\n",
        "        return \"hateful\"\n",
        "    elif score < -1.0:\n",
        "        return \"supportive\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "def normalize_schema(data):\n",
        "    \"\"\"Normalize data to match expected schema, handling dot notation.\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    # Handle flat dot notation (e.g., \"overall.label\" -> nested structure)\n",
        "    if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "        flat_data = data.copy()\n",
        "        data = {\"overall\": {}, \"facets\": {}, \"targets\": {}}\n",
        "\n",
        "        for key, value in flat_data.items():\n",
        "            if key.startswith(\"overall.\"):\n",
        "                field = key.replace(\"overall.\", \"\")\n",
        "                data[\"overall\"][field] = value\n",
        "            elif key == \"facets\":\n",
        "                data[\"facets\"] = value\n",
        "            elif key == \"targets\":\n",
        "                data[\"targets\"] = value\n",
        "            elif not key.startswith(\"target_\"):\n",
        "                # Unknown field, skip\n",
        "                pass\n",
        "\n",
        "    overall = data.get(\"overall\", {})\n",
        "\n",
        "    # Handle score field variations\n",
        "    if \"hate_speech_score\" not in overall and \"score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall.pop(\"score\"))\n",
        "    if \"hate_speech_score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Fix wrong label values\n",
        "    if \"label\" in overall:\n",
        "        label = overall[\"label\"]\n",
        "        if label == \"hate_speech\":  # Base model sometimes uses this\n",
        "            overall[\"label\"] = \"hateful\"\n",
        "        elif label not in [\"hateful\", \"neutral\", \"supportive\"]:\n",
        "            # Derive from score if label is invalid\n",
        "            if \"hate_speech_score\" in overall:\n",
        "                overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Derive label if missing\n",
        "    if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "        overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    data[\"overall\"] = overall\n",
        "\n",
        "    # Normalize facets to integers 0-4\n",
        "    facets = data.get(\"facets\", {})\n",
        "    fixed_facets = {}\n",
        "    for key, value in facets.items():\n",
        "        try:\n",
        "            fixed_facets[key] = max(0, min(4, int(float(value))))\n",
        "        except Exception:\n",
        "            fixed_facets[key] = 0\n",
        "    data[\"facets\"] = fixed_facets\n",
        "\n",
        "    # Normalize targets to booleans\n",
        "    targets = data.get(\"targets\", {})\n",
        "    data[\"targets\"] = {key: to_bool(value) for key, value in targets.items()}\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def run_base_model_benchmark():\n",
        "    \"\"\"Benchmark base Gemma-3-1B-it model using vLLM.\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"BENCHMARKING BASE GEMMA-3-1B-IT MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load base model tokenizer\n",
        "    print(\"\\n📥 Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
        "\n",
        "    # Initialize vLLM with base model\n",
        "    print(\"🚀 Loading base model with vLLM...\")\n",
        "    llm = LLM(\n",
        "        model=\"google/gemma-3-1b-it\",\n",
        "        dtype=\"bfloat16\",\n",
        "        gpu_memory_utilization=0.9,\n",
        "        max_model_len=4096,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Sampling parameters - FIXED max_tokens\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.0,\n",
        "        max_tokens=2048,  # ✅ FIXED: Was 1024, now 2048 for full JSON output\n",
        "        stop=[\"<end_of_turn>\", \"</s>\", \"\\n}\\n\", \"}\\n\\n\"],\n",
        "        stop_token_ids=[tokenizer.eos_token_id],\n",
        "    )\n",
        "\n",
        "    # Load test data\n",
        "    print(\"📂 Loading test data...\")\n",
        "    test_data = load_dataset(\"json\", data_files=\"test_aggregated.jsonl\", split=\"train\")\n",
        "    print(f\"   Test dataset size: {len(test_data)}\")\n",
        "\n",
        "    # Prepare prompts\n",
        "    print(\"📝 Preparing prompts...\")\n",
        "    prompts = []\n",
        "    for sample in test_data:\n",
        "        prompt_text = INSTRUCTION.format(text=sample['text'])\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            [{\"role\": \"user\", \"content\": prompt_text}],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(formatted_prompt)\n",
        "\n",
        "    # Run inference\n",
        "    print(f\"⚡ Running inference on {len(prompts)} samples...\")\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "    # Process results\n",
        "    print(\"🔄 Processing outputs...\")\n",
        "    predictions = []\n",
        "    failed_samples = []\n",
        "\n",
        "    for i, output in enumerate(tqdm(outputs, desc=\"Processing\")):\n",
        "        sample = test_data[i]\n",
        "        generated_text = output.outputs[0].text.strip()\n",
        "\n",
        "        try:\n",
        "            predicted_json_str = extract_outer_json(generated_text)\n",
        "            normalized_prediction = normalize_schema(predicted_json_str)\n",
        "            normalized_expected = normalize_schema(sample)\n",
        "\n",
        "            predictions.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": normalized_expected,\n",
        "                \"predicted\": normalized_prediction,\n",
        "                \"raw_output\": generated_text,\n",
        "                \"success\": True\n",
        "            })\n",
        "        except Exception as e:\n",
        "            failed_samples.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": normalize_schema(sample),\n",
        "                \"raw_output\": generated_text,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "            predictions.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"success\": False\n",
        "            })\n",
        "\n",
        "    # Save results\n",
        "    print(\"💾 Saving results...\")\n",
        "    output_file = \"base_gemma_predictions.jsonl\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for pred in predictions:\n",
        "            f.write(json.dumps(pred) + \"\\n\")\n",
        "\n",
        "    if failed_samples:\n",
        "        failed_file = \"base_gemma_failed.jsonl\"\n",
        "        with open(failed_file, \"w\") as f:\n",
        "            for failed in failed_samples:\n",
        "                f.write(json.dumps(failed) + \"\\n\")\n",
        "        print(f\"   Failed samples saved to: {failed_file}\")\n",
        "\n",
        "    # Print summary\n",
        "    success_rate = (len(predictions) - len(failed_samples)) / len(predictions)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ BENCHMARK COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"   Total samples:    {len(predictions)}\")\n",
        "    print(f\"   Successful:       {len(predictions) - len(failed_samples)} ({success_rate:.1%})\")\n",
        "    print(f\"   Failed:           {len(failed_samples)}\")\n",
        "    print(f\"   Results saved to: {output_file}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_base_model_benchmark()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def aggressive_json_fix(raw_output):\n",
        "    \"\"\"Try to salvage JSON from base model output with aggressive fixes.\"\"\"\n",
        "    try:\n",
        "        # Remove markdown\n",
        "        text = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        # Remove duplicate fields (keep first occurrence)\n",
        "        lines = text.split('\\n')\n",
        "        seen_keys = set()\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'\\s*\"([^\"]+)\":', line)\n",
        "            if match:\n",
        "                key = match.group(1)\n",
        "                if key not in seen_keys:\n",
        "                    seen_keys.add(key)\n",
        "                    cleaned_lines.append(line)\n",
        "            else:\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        # Parse JSON\n",
        "        data = json.loads(text)\n",
        "\n",
        "        # Convert dot notation to nested structure\n",
        "        if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "            flat_data = data.copy()\n",
        "            data = {\"overall\": {}, \"facets\": flat_data.get(\"facets\", {}), \"targets\": flat_data.get(\"targets\", {})}\n",
        "\n",
        "            for key, value in flat_data.items():\n",
        "                if key.startswith(\"overall.\"):\n",
        "                    field = key.replace(\"overall.\", \"\")\n",
        "                    data[\"overall\"][field] = value\n",
        "\n",
        "        # Fix label\n",
        "        if \"label\" in data.get(\"overall\", {}):\n",
        "            label = data[\"overall\"][\"label\"]\n",
        "            if label == \"hate_speech\":\n",
        "                data[\"overall\"][\"label\"] = \"hateful\"\n",
        "\n",
        "        # Derive label from score if missing\n",
        "        overall = data.get(\"overall\", {})\n",
        "        if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "            score = float(overall[\"hate_speech_score\"])\n",
        "            if score > 0.5:\n",
        "                overall[\"label\"] = \"hateful\"\n",
        "            elif score < -1.0:\n",
        "                overall[\"label\"] = \"supportive\"\n",
        "            else:\n",
        "                overall[\"label\"] = \"neutral\"\n",
        "            data[\"overall\"] = overall\n",
        "\n",
        "        return data, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"RECOVERY: Parsing Failed Base Model Outputs\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if failed file exists\n",
        "    try:\n",
        "        with open(\"base_gemma_failed.jsonl\", \"r\") as f:\n",
        "            failed_samples = [json.loads(line) for line in f]\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n❌ base_gemma_failed.jsonl not found!\")\n",
        "        print(\"   Run benchmark_base_gemma.py first\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n📁 Found {len(failed_samples)} failed samples\")\n",
        "    print(\"🔧 Attempting aggressive recovery...\\n\")\n",
        "\n",
        "    recovered = []\n",
        "    still_failed = []\n",
        "\n",
        "    for sample in tqdm(failed_samples, desc=\"Recovering\"):\n",
        "        raw_output = sample.get(\"raw_output\", \"\")\n",
        "        fixed_data, error = aggressive_json_fix(raw_output)\n",
        "\n",
        "        if fixed_data:\n",
        "            recovered.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": sample.get(\"expected\"),\n",
        "                \"predicted\": fixed_data,\n",
        "                \"raw_output\": raw_output,\n",
        "                \"success\": True\n",
        "            })\n",
        "        else:\n",
        "            still_failed.append({\n",
        "                **sample,\n",
        "                \"recovery_error\": error\n",
        "            })\n",
        "\n",
        "    # Save recovered samples\n",
        "    if recovered:\n",
        "        with open(\"base_gemma_recovered.jsonl\", \"w\") as f:\n",
        "            for item in recovered:\n",
        "                f.write(json.dumps(item) + \"\\n\")\n",
        "        print(f\"\\n✅ Recovered {len(recovered)} samples → base_gemma_recovered.jsonl\")\n",
        "\n",
        "    if still_failed:\n",
        "        with open(\"base_gemma_still_failed.jsonl\", \"w\") as f:\n",
        "            for item in still_failed:\n",
        "                f.write(json.dumps(item) + \"\\n\")\n",
        "        print(f\"⚠️  Still failed: {len(still_failed)} samples → base_gemma_still_failed.jsonl\")\n",
        "\n",
        "    # Calculate recovery rate\n",
        "    recovery_rate = len(recovered) / len(failed_samples) if failed_samples else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RECOVERY SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Original failures:    {len(failed_samples)}\")\n",
        "    print(f\"Successfully recovered: {len(recovered)} ({recovery_rate:.1%})\")\n",
        "    print(f\"Still failed:         {len(still_failed)} ({(1-recovery_rate):.1%})\")\n",
        "\n",
        "    if recovery_rate < 0.5:\n",
        "        print(\"\\n⚠️  Recovery rate < 50%\")\n",
        "        print(\"   The base model struggles with this JSON format.\")\n",
        "        print(\"   Consider skipping base model benchmark.\")\n",
        "    elif recovery_rate < 0.9:\n",
        "        print(\"\\n✓ Partial recovery successful\")\n",
        "        print(\"  You can combine base_gemma_predictions.jsonl + base_gemma_recovered.jsonl\")\n",
        "    else:\n",
        "        print(\"\\n✅ High recovery rate!\")\n",
        "        print(\"   Merge recovered samples with predictions for evaluation\")\n",
        "\n",
        "    # If recovery rate is decent, create merged file\n",
        "    if recovery_rate >= 0.5:\n",
        "        print(\"\\n🔗 Creating merged predictions file...\")\n",
        "        try:\n",
        "            # Load original predictions\n",
        "            with open(\"base_gemma_predictions.jsonl\", \"r\") as f:\n",
        "                original = [json.loads(line) for line in f]\n",
        "\n",
        "            # Merge: keep successful from original, add recovered\n",
        "            merged = [p for p in original if p.get(\"success\")] + recovered\n",
        "\n",
        "            with open(\"base_gemma_predictions_merged.jsonl\", \"w\") as f:\n",
        "                for item in merged:\n",
        "                    f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "            print(f\"✅ Created base_gemma_predictions_merged.jsonl ({len(merged)} samples)\")\n",
        "            print(\"   Use this file for evaluation\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not create merged file: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xalGxz0u-Pd-",
        "outputId": "7352a38f-f8d6-4a44-9596-d008a471005e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RECOVERY: Parsing Failed Base Model Outputs\n",
            "============================================================\n",
            "\n",
            "📁 Found 3932 failed samples\n",
            "🔧 Attempting aggressive recovery...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Recovering: 100%|██████████| 3932/3932 [00:00<00:00, 9754.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Still failed: 3932 samples → base_gemma_still_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "RECOVERY SUMMARY\n",
            "============================================================\n",
            "Original failures:    3932\n",
            "Successfully recovered: 0 (0.0%)\n",
            "Still failed:         3932 (100.0%)\n",
            "\n",
            "⚠️  Recovery rate < 50%\n",
            "   The base model struggles with this JSON format.\n",
            "   Consider skipping base model benchmark.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_score, recall_score,\n",
        "    mean_absolute_error, mean_squared_error,\n",
        "    hamming_loss, accuracy_score, classification_report\n",
        ")\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# ============================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def to_bool(x):\n",
        "    \"\"\"Robust boolean coercion.\"\"\"\n",
        "    if isinstance(x, bool): return x\n",
        "    if isinstance(x, (int, float)): return bool(int(x))\n",
        "    if isinstance(x, str): return x.strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
        "    return False\n",
        "\n",
        "def derive_label_from_score(score: float) -> str:\n",
        "    \"\"\"Derive label using the same logic as training data.\"\"\"\n",
        "    if score > 0.5: return \"hateful\"\n",
        "    if score < -1.0: return \"supportive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def normalize_schema(data):\n",
        "    \"\"\"Ensure data conforms to required types, handling dot notation.\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    # Handle flat dot notation (e.g., \"overall.label\" -> nested structure)\n",
        "    if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "        flat_data = data.copy()\n",
        "        data = {\"overall\": {}, \"facets\": {}, \"targets\": {}}\n",
        "\n",
        "        for key, value in flat_data.items():\n",
        "            if key.startswith(\"overall.\"):\n",
        "                field = key.replace(\"overall.\", \"\")\n",
        "                data[\"overall\"][field] = value\n",
        "            elif key == \"facets\":\n",
        "                data[\"facets\"] = value\n",
        "            elif key == \"targets\":\n",
        "                data[\"targets\"] = value\n",
        "\n",
        "    overall = data.get(\"overall\", {})\n",
        "    if \"score\" in overall and \"hate_speech_score\" not in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall.pop(\"score\"))\n",
        "    if \"hate_speech_score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Fix wrong label values\n",
        "    if \"label\" in overall:\n",
        "        label = overall[\"label\"]\n",
        "        if label == \"hate_speech\":\n",
        "            overall[\"label\"] = \"hateful\"\n",
        "        elif label not in [\"hateful\", \"neutral\", \"supportive\"]:\n",
        "            if \"hate_speech_score\" in overall:\n",
        "                overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "        overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "    data[\"overall\"] = overall\n",
        "\n",
        "    facets = data.get(\"facets\", {})\n",
        "    fixed_facets = {}\n",
        "    for key, value in facets.items():\n",
        "        try:\n",
        "            fixed_facets[key] = max(0, min(4, int(float(value))))\n",
        "        except (ValueError, TypeError):\n",
        "            fixed_facets[key] = 0\n",
        "    data[\"facets\"] = fixed_facets\n",
        "\n",
        "    targets = data.get(\"targets\", {})\n",
        "    data[\"targets\"] = {key: to_bool(value) for key, value in targets.items()}\n",
        "\n",
        "    return data\n",
        "\n",
        "def safe_get_score(obj):\n",
        "    \"\"\"Safely extract score from overall.\"\"\"\n",
        "    overall = obj.get(\"overall\", {})\n",
        "    if \"hate_speech_score\" in overall: return float(overall[\"hate_speech_score\"])\n",
        "    if \"score\" in overall: return float(overall[\"score\"])\n",
        "    raise KeyError(\"Neither 'hate_speech_score' nor 'score' found in overall\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# LOAD DATA\n",
        "# ============================================\n",
        "\n",
        "# Auto-detect which predictions file to use\n",
        "import os\n",
        "if os.path.exists(\"base_gemma_predictions_merged.jsonl\"):\n",
        "    PREDICTIONS_FILE = \"base_gemma_predictions_merged.jsonl\"\n",
        "    print(\"🔗 Using merged predictions (includes recovered samples)\")\n",
        "elif os.path.exists(\"base_gemma_predictions.jsonl\"):\n",
        "    PREDICTIONS_FILE = \"base_gemma_predictions.jsonl\"\n",
        "else:\n",
        "    print(\"❌ No predictions file found!\")\n",
        "    print(\"   Run benchmark_base_gemma.py first\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BASE GEMMA-3-1B-IT EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n📁 Loading predictions from: {PREDICTIONS_FILE}\")\n",
        "\n",
        "predictions = []\n",
        "with open(PREDICTIONS_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        predictions.append(json.loads(line))\n",
        "\n",
        "# Separate successful and failed predictions\n",
        "valid_preds = [p for p in predictions if p.get(\"success\")]\n",
        "failed_samples = [p for p in predictions if not p.get(\"success\")]\n",
        "\n",
        "# Re-normalize both expected and predicted data\n",
        "for p in valid_preds:\n",
        "    p[\"expected\"] = normalize_schema(p[\"expected\"])\n",
        "    p[\"predicted\"] = normalize_schema(p[\"predicted\"])\n",
        "\n",
        "print(f\"\\n📊 Data Summary:\")\n",
        "print(f\"   Total samples:        {len(predictions)}\")\n",
        "print(f\"   Valid predictions:    {len(valid_preds)} ({100*len(valid_preds)/len(predictions):.1f}%)\")\n",
        "print(f\"   Failed predictions:   {len(failed_samples)} ({100*len(failed_samples)/len(predictions):.1f}%)\")\n",
        "\n",
        "if not valid_preds:\n",
        "    print(\"\\n❌ No valid predictions found. Cannot evaluate.\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 1. OVERALL LABEL CLASSIFICATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OVERALL: Label Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_true_labels = [p[\"expected\"][\"overall\"][\"label\"] for p in valid_preds]\n",
        "y_pred_labels = [p[\"predicted\"][\"overall\"][\"label\"] for p in valid_preds]\n",
        "\n",
        "overall_accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "overall_micro_f1 = f1_score(y_true_labels, y_pred_labels, average=\"micro\", zero_division=0)\n",
        "overall_macro_f1 = f1_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "overall_precision = precision_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "overall_recall = recall_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"Accuracy:        {overall_accuracy:.4f}\")\n",
        "print(f\"Macro F1:        {overall_macro_f1:.4f}\")\n",
        "print(f\"Micro F1:        {overall_micro_f1:.4f}\")\n",
        "print(f\"Macro Precision: {overall_precision:.4f}\")\n",
        "print(f\"Macro Recall:    {overall_recall:.4f}\")\n",
        "print(\"\\nPer-class breakdown:\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n",
        "\n",
        "# Score correlation\n",
        "y_true_scores = [safe_get_score(p[\"expected\"]) for p in valid_preds]\n",
        "y_pred_scores = [safe_get_score(p[\"predicted\"]) for p in valid_preds]\n",
        "score_corr = spearmanr(y_true_scores, y_pred_scores).correlation\n",
        "print(f\"Score Spearman correlation: {score_corr:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. FACETS EVALUATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FACETS: Ordinal Ratings (0-4 scale)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "facet_names = list(valid_preds[0][\"expected\"][\"facets\"].keys())\n",
        "facet_results = {}\n",
        "\n",
        "for facet in facet_names:\n",
        "    y_true = np.array([p[\"expected\"][\"facets\"].get(facet, 0) for p in valid_preds])\n",
        "    y_pred = np.array([p[\"predicted\"][\"facets\"].get(facet, 0) for p in valid_preds])\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    spearman = spearmanr(y_true, y_pred).correlation\n",
        "    exact_match = accuracy_score(y_true, y_pred)\n",
        "    within_1 = np.mean(np.abs(y_true - y_pred) <= 1)\n",
        "\n",
        "    facet_results[facet] = {\n",
        "        \"mae\": mae, \"mse\": mse, \"spearman\": spearman,\n",
        "        \"exact_match\": exact_match, \"within_1_accuracy\": within_1\n",
        "    }\n",
        "\n",
        "mean_mae = np.mean([r[\"mae\"] for r in facet_results.values()])\n",
        "mean_mse = np.mean([r[\"mse\"] for r in facet_results.values()])\n",
        "mean_spearman = np.mean([r[\"spearman\"] for r in facet_results.values()])\n",
        "mean_exact = np.mean([r[\"exact_match\"] for r in facet_results.values()])\n",
        "mean_within_1 = np.mean([r[\"within_1_accuracy\"] for r in facet_results.values()])\n",
        "\n",
        "print(f\"Mean MAE:               {mean_mae:.4f}\")\n",
        "print(f\"Mean MSE:               {mean_mse:.4f}\")\n",
        "print(f\"Mean Spearman:          {mean_spearman:.4f}\")\n",
        "print(f\"Mean Exact Match:       {mean_exact:.4f}\")\n",
        "print(f\"Mean Within-1 Accuracy: {mean_within_1:.4f}\")\n",
        "\n",
        "print(\"\\nPer-facet breakdown:\")\n",
        "print(f\"{'Facet':<20} {'MAE':<8} {'Exact':<8} {'Within-1':<10} {'Spearman':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for facet in facet_names:\n",
        "    r = facet_results[facet]\n",
        "    print(f\"{facet:<20} {r['mae']:<8.3f} {r['exact_match']:<8.3f} {r['within_1_accuracy']:<10.3f} {r['spearman']:<10.3f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3. TARGETS EVALUATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TARGETS: Multi-label Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "target_names = list(valid_preds[0][\"expected\"][\"targets\"].keys())\n",
        "\n",
        "y_true_targets = np.array([[int(p[\"expected\"][\"targets\"].get(t, False)) for t in target_names] for p in valid_preds])\n",
        "y_pred_targets = np.array([[int(p[\"predicted\"][\"targets\"].get(t, False)) for t in target_names] for p in valid_preds])\n",
        "\n",
        "targets_micro_f1 = f1_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_macro_f1 = f1_score(y_true_targets, y_pred_targets, average=\"macro\", zero_division=0)\n",
        "targets_micro_precision = precision_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_micro_recall = recall_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_hamming = hamming_loss(y_true_targets, y_pred_targets)\n",
        "exact_match_ratio = np.mean(np.all(y_true_targets == y_pred_targets, axis=1))\n",
        "\n",
        "print(f\"Micro F1:          {targets_micro_f1:.4f}\")\n",
        "print(f\"Macro F1:          {targets_macro_f1:.4f}\")\n",
        "print(f\"Micro Precision:   {targets_micro_precision:.4f}\")\n",
        "print(f\"Micro Recall:      {targets_micro_recall:.4f}\")\n",
        "print(f\"Hamming Loss:      {targets_hamming:.4f}\")\n",
        "print(f\"Exact Match Ratio: {exact_match_ratio:.4f} ({int(exact_match_ratio*len(valid_preds))}/{len(valid_preds)})\")\n",
        "\n",
        "print(\"\\nPer-target F1 scores (bottom 10):\")\n",
        "per_target_f1 = {target: f1_score(y_true_targets[:, i], y_pred_targets[:, i], zero_division=0) for i, target in enumerate(target_names)}\n",
        "sorted_targets = sorted(per_target_f1.items(), key=lambda x: x[1])\n",
        "for target, f1 in sorted_targets[:10]:\n",
        "    print(f\"  {target:<40} {f1:.3f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# SAVE EVALUATION SUMMARY\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "eval_summary = {\n",
        "    \"metadata\": {\n",
        "        \"model\": \"google/gemma-3-1b-it (base)\",\n",
        "        \"prediction_file\": PREDICTIONS_FILE,\n",
        "        \"total_samples\": len(predictions),\n",
        "        \"valid_predictions\": len(valid_preds),\n",
        "        \"failed_predictions\": len(failed_samples),\n",
        "        \"success_rate\": len(valid_preds) / len(predictions) if predictions else 0\n",
        "    },\n",
        "    \"overall\": {\n",
        "        \"accuracy\": overall_accuracy,\n",
        "        \"macro_f1\": overall_macro_f1,\n",
        "        \"micro_f1\": overall_micro_f1,\n",
        "        \"precision\": overall_precision,\n",
        "        \"recall\": overall_recall,\n",
        "        \"score_spearman\": score_corr\n",
        "    },\n",
        "    \"facets\": {\n",
        "        \"mean_mae\": mean_mae,\n",
        "        \"mean_mse\": mean_mse,\n",
        "        \"mean_spearman\": mean_spearman,\n",
        "        \"mean_exact_match\": mean_exact,\n",
        "        \"mean_within_1_accuracy\": mean_within_1,\n",
        "        \"per_facet\": facet_results\n",
        "    },\n",
        "    \"targets\": {\n",
        "        \"micro_f1\": targets_micro_f1,\n",
        "        \"macro_f1\": targets_macro_f1,\n",
        "        \"precision\": targets_micro_precision,\n",
        "        \"recall\": targets_micro_recall,\n",
        "        \"hamming_loss\": targets_hamming,\n",
        "        \"exact_match_ratio\": exact_match_ratio,\n",
        "        \"per_target_f1\": per_target_f1\n",
        "    }\n",
        "}\n",
        "\n",
        "output_file = \"base_gemma_evaluation.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(eval_summary, f, indent=2)\n",
        "\n",
        "print(f\"✅ Summary saved to: {output_file}\")\n",
        "if failed_samples:\n",
        "    print(f\"⚠️  {len(failed_samples)} samples failed - check base_gemma_failed.jsonl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Print key metrics summary\n",
        "print(\"\\n📈 KEY METRICS SUMMARY:\")\n",
        "print(f\"   Overall Accuracy:  {overall_accuracy:.2%}\")\n",
        "print(f\"   Overall Macro F1:  {overall_macro_f1:.4f}\")\n",
        "print(f\"   Facets Mean MAE:   {mean_mae:.4f}\")\n",
        "print(f\"   Targets Micro F1:  {targets_micro_f1:.4f}\")\n",
        "print(f\"   Success Rate:      {len(valid_preds)/len(predictions):.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_RUHS-A58Gd",
        "outputId": "33cee055-2827-41dd-fd09-dda55fdff649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASE GEMMA-3-1B-IT EVALUATION\n",
            "============================================================\n",
            "\n",
            "📁 Loading predictions from: base_gemma_predictions.jsonl\n",
            "\n",
            "📊 Data Summary:\n",
            "   Total samples:        3957\n",
            "   Valid predictions:    25 (0.6%)\n",
            "   Failed predictions:   3932 (99.4%)\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "OVERALL: Label Classification\n",
            "============================================================\n",
            "Accuracy:        0.2800\n",
            "Macro F1:        0.2169\n",
            "Micro F1:        0.2800\n",
            "Macro Precision: 0.2020\n",
            "Macro Recall:    0.3889\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     hateful       0.27      1.00      0.43         6\n",
            "     neutral       0.33      0.17      0.22         6\n",
            "  supportive       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.28        25\n",
            "   macro avg       0.20      0.39      0.22        25\n",
            "weighted avg       0.15      0.28      0.16        25\n",
            "\n",
            "Score Spearman correlation: 0.4216\n",
            "\n",
            "============================================================\n",
            "FACETS: Ordinal Ratings (0-4 scale)\n",
            "============================================================\n",
            "Mean MAE:               1.6960\n",
            "Mean MSE:               4.5040\n",
            "Mean Spearman:          nan\n",
            "Mean Exact Match:       0.2360\n",
            "Mean Within-1 Accuracy: 0.4320\n",
            "\n",
            "Per-facet breakdown:\n",
            "Facet                MAE      Exact    Within-1   Spearman  \n",
            "------------------------------------------------------------\n",
            "sentiment            2.920    0.040    0.040      0.225     \n",
            "respect              2.840    0.040    0.040      0.015     \n",
            "insult               1.440    0.200    0.520      0.073     \n",
            "humiliate            1.480    0.160    0.600      0.239     \n",
            "status               2.360    0.000    0.080      0.207     \n",
            "dehumanize           1.640    0.160    0.400      nan       \n",
            "violence             1.040    0.280    0.760      nan       \n",
            "genocide             0.480    0.720    0.880      nan       \n",
            "attack_defend        2.440    0.000    0.080      nan       \n",
            "hatespeech           0.320    0.760    0.920      nan       \n",
            "\n",
            "============================================================\n",
            "TARGETS: Multi-label Classification\n",
            "============================================================\n",
            "Micro F1:          0.0000\n",
            "Macro F1:          0.0000\n",
            "Micro Precision:   0.0000\n",
            "Micro Recall:      0.0000\n",
            "Hamming Loss:      0.0539\n",
            "Exact Match Ratio: 0.0000 (0/25)\n",
            "\n",
            "Per-target F1 scores (bottom 10):\n",
            "  target_race_asian                        0.000\n",
            "  target_race_black                        0.000\n",
            "  target_race_latinx                       0.000\n",
            "  target_race_middle_eastern               0.000\n",
            "  target_race_native_american              0.000\n",
            "  target_race_pacific_islander             0.000\n",
            "  target_race_white                        0.000\n",
            "  target_race_other                        0.000\n",
            "  target_religion_atheist                  0.000\n",
            "  target_religion_buddhist                 0.000\n",
            "\n",
            "============================================================\n",
            "SAVING EVALUATION SUMMARY\n",
            "============================================================\n",
            "✅ Summary saved to: base_gemma_evaluation.json\n",
            "⚠️  3932 samples failed - check base_gemma_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "EVALUATION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "📈 KEY METRICS SUMMARY:\n",
            "   Overall Accuracy:  28.00%\n",
            "   Overall Macro F1:  0.2169\n",
            "   Facets Mean MAE:   1.6960\n",
            "   Targets Micro F1:  0.0000\n",
            "   Success Rate:      0.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-510175953.py:185: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman = spearmanr(y_true, y_pred).correlation\n",
            "/tmp/ipython-input-510175953.py:185: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman = spearmanr(y_true, y_pred).correlation\n",
            "/tmp/ipython-input-510175953.py:185: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman = spearmanr(y_true, y_pred).correlation\n",
            "/tmp/ipython-input-510175953.py:185: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman = spearmanr(y_true, y_pred).correlation\n",
            "/tmp/ipython-input-510175953.py:185: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  spearman = spearmanr(y_true, y_pred).correlation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from vllm import LLM, SamplingParams\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Your instruction template\n",
        "INSTRUCTION = \"\"\"\n",
        "You are an expert hate speech analyst. Your task is to analyze the provided text and return ONLY a valid JSON object that strictly follows the schema below.\n",
        "Do not include any explanations, markdown formatting, or text outside of the JSON object.\n",
        "\n",
        "=========================\n",
        "IMPORTANT INSTRUCTIONS\n",
        "=========================\n",
        "1. Output must be **valid JSON only** - no markdown, no commentary.\n",
        "2. Use exact field names and types from the schema.\n",
        "3. Type requirements:\n",
        "   - `overall.label` → string (\"hateful\", \"supportive\", or \"neutral\")\n",
        "   - `overall.hate_speech_score` → float (e.g., `1.52`, `-1.35`, `0.12`)\n",
        "   - All `facets` values → integers 0-4 only (e.g., `3`, not `3.0`)\n",
        "   - All `targets` values → booleans (`true` or `false`)\n",
        "\n",
        "=========================\n",
        "OVERALL ASSESSMENT\n",
        "=========================\n",
        "Provide two values in the `\"overall\"` object:\n",
        "\n",
        "1. **`\"hate_speech_score\"`** - A float representing hate speech intensity:\n",
        "   - **Hateful content** → positive float > 0.5 (e.g., `0.8`, `1.52`, `2.3`)\n",
        "   - **Neutral content** → float between -1.0 and 0.5 inclusive (e.g., `-0.8`, `0.12`, `0.4`)\n",
        "   - **Supportive content** → negative float < -1.0 (e.g., `-1.35`, `-2.1`)\n",
        "\n",
        "2. **`\"label\"`** - A categorical string derived from the hate_speech_score:\n",
        "   - `\"hateful\"` if hate_speech_score > 0.5\n",
        "   - `\"neutral\"` if hate_speech_score is between -1.0 and 0.5 (inclusive)\n",
        "   - `\"supportive\"` if hate_speech_score < -1.0\n",
        "\n",
        "=========================\n",
        "FACETS (0-4 SCALE)\n",
        "=========================\n",
        "Each facet is an **integer** from 0 to 4:\n",
        "- 0 = Absent\n",
        "- 1 = Mild\n",
        "- 2 = Clear\n",
        "- 3 = Severe\n",
        "- 4 = Extreme\n",
        "\n",
        "Example: `\"insult\": 2` (not `2.0` or `\"2\"`)\n",
        "\n",
        "=========================\n",
        "TARGETS (BOOLEAN FLAGS)\n",
        "=========================\n",
        "Set to `true` only if that group is explicitly targeted in the text.\n",
        "\n",
        "=========================\n",
        "JSON SCHEMA (MUST MATCH EXACTLY)\n",
        "=========================\n",
        "{{\n",
        "  \"overall\": {{\n",
        "    \"label\": \"neutral\",\n",
        "    \"hate_speech_score\": 0.00\n",
        "  }},\n",
        "  \"facets\": {{\n",
        "    \"sentiment\": 0,\n",
        "    \"respect\": 0,\n",
        "    \"insult\": 0,\n",
        "    \"humiliate\": 0,\n",
        "    \"status\": 0,\n",
        "    \"dehumanize\": 0,\n",
        "    \"violence\": 0,\n",
        "    \"genocide\": 0,\n",
        "    \"attack_defend\": 0,\n",
        "    \"hatespeech\": 0\n",
        "  }},\n",
        "  \"targets\": {{\n",
        "    \"target_race_asian\": false,\n",
        "    \"target_race_black\": false,\n",
        "    \"target_race_latinx\": false,\n",
        "    \"target_race_middle_eastern\": false,\n",
        "    \"target_race_native_american\": false,\n",
        "    \"target_race_pacific_islander\": false,\n",
        "    \"target_race_white\": false,\n",
        "    \"target_race_other\": false,\n",
        "    \"target_religion_atheist\": false,\n",
        "    \"target_religion_buddhist\": false,\n",
        "    \"target_religion_christian\": false,\n",
        "    \"target_religion_hindu\": false,\n",
        "    \"target_religion_jewish\": false,\n",
        "    \"target_religion_mormon\": false,\n",
        "    \"target_religion_muslim\": false,\n",
        "    \"target_religion_other\": false,\n",
        "    \"target_origin_immigrant\": false,\n",
        "    \"target_origin_migrant_worker\": false,\n",
        "    \"target_origin_specific_country\": false,\n",
        "    \"target_origin_undocumented\": false,\n",
        "    \"target_origin_other\": false,\n",
        "    \"target_gender_men\": false,\n",
        "    \"target_gender_non_binary\": false,\n",
        "    \"target_gender_transgender_men\": false,\n",
        "    \"target_gender_transgender_unspecified\": false,\n",
        "    \"target_gender_transgender_women\": false,\n",
        "    \"target_gender_women\": false,\n",
        "    \"target_gender_other\": false,\n",
        "    \"target_sexuality_bisexual\": false,\n",
        "    \"target_sexuality_gay\": false,\n",
        "    \"target_sexuality_lesbian\": false,\n",
        "    \"target_sexuality_straight\": false,\n",
        "    \"target_sexuality_other\": false,\n",
        "    \"target_age_children\": false,\n",
        "    \"target_age_teenagers\": false,\n",
        "    \"target_age_young_adults\": false,\n",
        "    \"target_age_middle_aged\": false,\n",
        "    \"target_age_seniors\": false,\n",
        "    \"target_age_other\": false,\n",
        "    \"target_disability_physical\": false,\n",
        "    \"target_disability_cognitive\": false,\n",
        "    \"target_disability_neurological\": false,\n",
        "    \"target_disability_visually_impaired\": false,\n",
        "    \"target_disability_hearing_impaired\": false,\n",
        "    \"target_disability_unspecific\": false,\n",
        "    \"target_disability_other\": false\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def extract_outer_json(text: str) -> str:\n",
        "    \"\"\"Extract JSON object from model output, stripping markdown and handling malformed JSON.\"\"\"\n",
        "    # Remove markdown code fences\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    s = text.find(\"{\")\n",
        "    if s == -1:\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "\n",
        "    # Find matching closing brace\n",
        "    brace_count = 0\n",
        "    for i in range(s, len(text)):\n",
        "        if text[i] == '{':\n",
        "            brace_count += 1\n",
        "        elif text[i] == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                return text[s:i+1]\n",
        "\n",
        "    # Fallback to rfind if no matching brace found\n",
        "    e = text.rfind(\"}\")\n",
        "    if e == -1 or e <= s:\n",
        "        raise ValueError(\"No closing brace found for JSON object\")\n",
        "    return text[s:e+1]\n",
        "\n",
        "def to_bool(x):\n",
        "    \"\"\"Robust boolean conversion.\"\"\"\n",
        "    if isinstance(x, bool):\n",
        "        return x\n",
        "    if isinstance(x, (int, float)):\n",
        "        return bool(int(x))\n",
        "    if isinstance(x, str):\n",
        "        return x.strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
        "    return False\n",
        "\n",
        "def derive_label_from_score(score: float) -> str:\n",
        "    \"\"\"Derive label from hate_speech_score.\"\"\"\n",
        "    if score > 0.5:\n",
        "        return \"hateful\"\n",
        "    elif score < -1.0:\n",
        "        return \"supportive\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "def normalize_schema(data):\n",
        "    \"\"\"Normalize data to match expected schema, handling dot notation and variations.\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    # Handle flat dot notation (e.g., \"overall.label\" -> nested structure)\n",
        "    if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "        flat_data = data.copy()\n",
        "        data = {\"overall\": {}, \"facets\": {}, \"targets\": {}}\n",
        "\n",
        "        for key, value in flat_data.items():\n",
        "            if key.startswith(\"overall.\"):\n",
        "                field = key.replace(\"overall.\", \"\")\n",
        "                data[\"overall\"][field] = value\n",
        "            elif key == \"facets\":\n",
        "                data[\"facets\"] = value\n",
        "            elif key == \"targets\":\n",
        "                data[\"targets\"] = value\n",
        "\n",
        "    overall = data.get(\"overall\", {})\n",
        "\n",
        "    # Handle score field variations\n",
        "    if \"hate_speech_score\" not in overall and \"score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall.pop(\"score\"))\n",
        "    if \"hate_speech_score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Fix wrong label values\n",
        "    if \"label\" in overall:\n",
        "        label = overall[\"label\"]\n",
        "        if label == \"hate_speech\":  # Base model sometimes uses this\n",
        "            overall[\"label\"] = \"hateful\"\n",
        "        elif label not in [\"hateful\", \"neutral\", \"supportive\"]:\n",
        "            if \"hate_speech_score\" in overall:\n",
        "                overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Derive label if missing\n",
        "    if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "        overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    data[\"overall\"] = overall\n",
        "\n",
        "    # Normalize facets to integers 0-4\n",
        "    facets = data.get(\"facets\", {})\n",
        "    fixed_facets = {}\n",
        "    for key, value in facets.items():\n",
        "        try:\n",
        "            fixed_facets[key] = max(0, min(4, int(float(value))))\n",
        "        except Exception:\n",
        "            fixed_facets[key] = 0\n",
        "    data[\"facets\"] = fixed_facets\n",
        "\n",
        "    # Normalize targets to booleans\n",
        "    targets = data.get(\"targets\", {})\n",
        "    data[\"targets\"] = {key: to_bool(value) for key, value in targets.items()}\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def run_base_llama_benchmark():\n",
        "    \"\"\"Benchmark base Llama-3.2-1B-Instruct model using vLLM.\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"BENCHMARKING BASE LLAMA-3.2-1B-INSTRUCT MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load base model tokenizer\n",
        "    print(\"\\n📥 Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "\n",
        "    # Initialize vLLM with base model\n",
        "    print(\"🚀 Loading base model with vLLM...\")\n",
        "    llm = LLM(\n",
        "        model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        dtype=\"bfloat16\",\n",
        "        gpu_memory_utilization=0.9,\n",
        "        max_model_len=4096,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Sampling parameters\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.0,\n",
        "        max_tokens=2048,  # Sufficient for full JSON output\n",
        "        stop=[\"<|eot_id|>\", \"</s>\", \"\\n}\\n\", \"}\\n\\n\"],  # Llama stop tokens\n",
        "        stop_token_ids=[tokenizer.eos_token_id],\n",
        "    )\n",
        "\n",
        "    # Load test data\n",
        "    print(\"📂 Loading test data...\")\n",
        "    test_data = load_dataset(\"json\", data_files=\"test_aggregated.jsonl\", split=\"train\")\n",
        "    print(f\"   Test dataset size: {len(test_data)}\")\n",
        "\n",
        "    # Prepare prompts with Llama chat template\n",
        "    print(\"📝 Preparing prompts...\")\n",
        "    prompts = []\n",
        "    for sample in test_data:\n",
        "        # Use system + user message format for Llama\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": INSTRUCTION.strip()},\n",
        "            {\"role\": \"user\", \"content\": sample['text']}\n",
        "        ]\n",
        "\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        prompts.append(formatted_prompt)\n",
        "\n",
        "    # Run inference\n",
        "    print(f\"⚡ Running inference on {len(prompts)} samples...\")\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "    # Process results\n",
        "    print(\"🔄 Processing outputs...\")\n",
        "    predictions = []\n",
        "    failed_samples = []\n",
        "\n",
        "    for i, output in enumerate(tqdm(outputs, desc=\"Processing\")):\n",
        "        sample = test_data[i]\n",
        "        generated_text = output.outputs[0].text.strip()\n",
        "\n",
        "        try:\n",
        "            predicted_json_str = extract_outer_json(generated_text)\n",
        "            normalized_prediction = normalize_schema(predicted_json_str)\n",
        "            normalized_expected = normalize_schema(sample)\n",
        "\n",
        "            predictions.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": normalized_expected,\n",
        "                \"predicted\": normalized_prediction,\n",
        "                \"raw_output\": generated_text,\n",
        "                \"success\": True\n",
        "            })\n",
        "        except Exception as e:\n",
        "            failed_samples.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": normalize_schema(sample),\n",
        "                \"raw_output\": generated_text,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "            predictions.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"success\": False\n",
        "            })\n",
        "\n",
        "    # Save results\n",
        "    print(\"💾 Saving results...\")\n",
        "    output_file = \"base_llama_predictions.jsonl\"\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for pred in predictions:\n",
        "            f.write(json.dumps(pred) + \"\\n\")\n",
        "\n",
        "    if failed_samples:\n",
        "        failed_file = \"base_llama_failed.jsonl\"\n",
        "        with open(failed_file, \"w\") as f:\n",
        "            for failed in failed_samples:\n",
        "                f.write(json.dumps(failed) + \"\\n\")\n",
        "        print(f\"   Failed samples saved to: {failed_file}\")\n",
        "\n",
        "    # Print summary\n",
        "    success_rate = (len(predictions) - len(failed_samples)) / len(predictions)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ BENCHMARK COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"   Total samples:    {len(predictions)}\")\n",
        "    print(f\"   Successful:       {len(predictions) - len(failed_samples)} ({success_rate:.1%})\")\n",
        "    print(f\"   Failed:           {len(failed_samples)}\")\n",
        "    print(f\"   Results saved to: {output_file}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_base_llama_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772,
          "referenced_widgets": [
            "5a7d1430541847eba80b5af5ae292962",
            "cea4b9da1f3549fe9c5c90e9782a56bd",
            "37a53060315d4a2cb7f7a136490ad844",
            "0d5153125d8749fe952d415630d023f3",
            "648fef65c7de4d4480d9f02b752561fd",
            "eaa3269f5b3345e3a36433bc5327a970",
            "9580f242f46e47ca9fd77142e7b3abfc",
            "c5458adfe12e484db9c91380c2920c83",
            "6bc47a5f4fcb4fc0b9186268194ea555",
            "d84bdf5d2ad14c40885a28653306b0b5",
            "97aac808f8ea460ebab0dadd1d8245bd",
            "8cd623d34b7d4321aa94c2d12a51c1a1",
            "8d6778dfc99d4374af29f684d8ba6f52",
            "748ef0f7020640ef98382ef2c60dc5cb",
            "d73d83a09a3f4b0dabeec4c454556aec",
            "6a6d7192145e42728b608e9493b33dcd",
            "a3f4e0cf67cd44af8157cc33030c0e77",
            "1c283be0a674423e824009c22324f671",
            "7c739c101f594956923c539d44ec8529",
            "b3b1e7e8636842038f0b873b5bd46abd",
            "42aa3f09a50a43778a6aff68c0e1e35c",
            "e528c733360e4aa7b75e7f0c7f16c33f",
            "4f5f5151c5b04fa4a125a4f6fcd5a445",
            "1ab569703d1842e3b3a23222c44705dc",
            "68f298b63d2640499870a6762e556edd",
            "089cef4e3aa24da38dcf5e9124d64ebb",
            "71a1615e829c49bc8061ffdb9eb6fa0a",
            "148bcdad4d364d5197d242f98aa5c7c6",
            "f0e82ac7ffb64c888caa5144b81b4ef9",
            "306194d117974ac4a16301a62cf6f7c4",
            "438c324e4da4466d86b7a5c83d98ddb8",
            "d29eb2b460794ab396a0e124e42c7ded",
            "a110ef07c8b94c8880099d65703626f3",
            "c61ef7b5898e4cf48f7bec4c0e0fc979",
            "7aa294b9040d41778f69fcc911458a0f",
            "be2a58cea3b54cfd8d09fecb7ba0e05c",
            "3f5d2306fc554e5ebe2d46c1256f7569",
            "05992e20521a45e19e952a473a9fe263",
            "d22e62cd3cc24ee08a5eb71d5f823966",
            "7c0f76b0e51748ab906f2ca82606eae0",
            "783fd3eb7d514fe08b17472075236d9e",
            "2d74bb7708044412824bd0681ff549bb",
            "c92def963f6647edbcdc2413061db075",
            "d92e2fa6ed674f708582bc1a952cd1fa",
            "f8a3904783484be0a5a4bb168bdf02d2",
            "ba8fd6686699459593dda35a00816e70",
            "7dc5937141d54f46a1e20dda119b8b3a",
            "1a87d3db69f24ac6803f600177e12fd8",
            "11447d610767440ba31f48654c6e47ac",
            "04d76158e0e643aeacfbff847965e25e",
            "a518fd5b153a457aae632da75c2d6cf4",
            "33f094fda753417d8c72f33ad11718a2",
            "cec14cd50402479dad5767dd331aa39f",
            "a4874d4958a247bb9ae41de2fb27bc02",
            "d69f87e571ad453193cafc3e0fdb1e91",
            "816d35dfb4784f78b39a3176345ddc8c",
            "3b59cd5e2ee94dd9b9f23a4cea9f2136",
            "228271766ef44df3bb7364bd267c54cb",
            "bb8c7266b8f741a2bcdecba045b1c31e",
            "ee736b489a38435ea12e082452b045fc",
            "973953d9794547fc81095d9bb914529f",
            "60a8c5be8973402ba3f1fb998708c7c5",
            "5e174881d681453f80f34671f76dae14",
            "b30b7fc423254bd6a3ecfbdf3f6d1db7",
            "fd09e5a11dac4a2494a6189ce9399274",
            "55627cfd965f4e9fb7221b90b1c64ec2",
            "38a922ab906a4ca3ab17368ebe601b7f",
            "82e79bafa7ae4b3ea997efbddf150734",
            "3f5cec9cda844fe08dd761729eaebc46",
            "8da40534e08141358481fa95c2809b1d",
            "03c1ae23eb2b4e7181bfdf69caf7bbb6",
            "62805cc9cccd42a79bb85ee78b133742",
            "110a74af1edc4116baf31b2260a1e6d4",
            "6eac02c4bd2f4151a20d43a1f709c976",
            "cf0ed0a6c9c5497ba474126974a2f7c8",
            "5fc94f2b03b444ca8a1f9086d2572812",
            "e535e4cacd724975887f98b20ff31d83"
          ]
        },
        "id": "o_8x8lJb6TeB",
        "outputId": "4bc97717-e1af-4ab3-92f3-3ac216ba79c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BENCHMARKING BASE LLAMA-3.2-1B-INSTRUCT MODEL\n",
            "============================================================\n",
            "\n",
            "📥 Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7d1430541847eba80b5af5ae292962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cd623d34b7d4321aa94c2d12a51c1a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f5f5151c5b04fa4a125a4f6fcd5a445"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Loading base model with vLLM...\n",
            "INFO 11-08 06:02:57 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.2-1B-Instruct'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c61ef7b5898e4cf48f7bec4c0e0fc979"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-08 06:03:14 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
            "INFO 11-08 06:03:14 [model.py:1510] Using max model len 4096\n",
            "INFO 11-08 06:03:14 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8a3904783484be0a5a4bb168bdf02d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-08 06:04:08 [llm.py:306] Supported_tasks: ['generate']\n",
            "📂 Loading test data...\n",
            "   Test dataset size: 3957\n",
            "📝 Preparing prompts...\n",
            "⚡ Running inference on 3957 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/3957 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "816d35dfb4784f78b39a3176345ddc8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/3957 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38a922ab906a4ca3ab17368ebe601b7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Processing outputs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 3957/3957 [00:01<00:00, 2427.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving results...\n",
            "   Failed samples saved to: base_llama_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "✅ BENCHMARK COMPLETE!\n",
            "============================================================\n",
            "   Total samples:    3957\n",
            "   Successful:       1617 (40.9%)\n",
            "   Failed:           2340\n",
            "   Results saved to: base_llama_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def aggressive_json_fix(raw_output):\n",
        "    \"\"\"Try to salvage JSON from base model output with aggressive fixes.\"\"\"\n",
        "    try:\n",
        "        # Remove markdown\n",
        "        text = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        # Remove duplicate fields (keep first occurrence)\n",
        "        lines = text.split('\\n')\n",
        "        seen_keys = set()\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'\\s*\"([^\"]+)\":', line)\n",
        "            if match:\n",
        "                key = match.group(1)\n",
        "                if key not in seen_keys:\n",
        "                    seen_keys.add(key)\n",
        "                    cleaned_lines.append(line)\n",
        "            else:\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        # Parse JSON\n",
        "        data = json.loads(text)\n",
        "\n",
        "        # Convert dot notation to nested structure\n",
        "        if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "            flat_data = data.copy()\n",
        "            data = {\"overall\": {}, \"facets\": flat_data.get(\"facets\", {}), \"targets\": flat_data.get(\"targets\", {})}\n",
        "\n",
        "            for key, value in flat_data.items():\n",
        "                if key.startswith(\"overall.\"):\n",
        "                    field = key.replace(\"overall.\", \"\")\n",
        "                    data[\"overall\"][field] = value\n",
        "\n",
        "        # Fix label\n",
        "        if \"label\" in data.get(\"overall\", {}):\n",
        "            label = data[\"overall\"][\"label\"]\n",
        "            if label == \"hate_speech\":\n",
        "                data[\"overall\"][\"label\"] = \"hateful\"\n",
        "\n",
        "        # Derive label from score if missing\n",
        "        overall = data.get(\"overall\", {})\n",
        "        if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "            score = float(overall[\"hate_speech_score\"])\n",
        "            if score > 0.5:\n",
        "                overall[\"label\"] = \"hateful\"\n",
        "            elif score < -1.0:\n",
        "                overall[\"label\"] = \"supportive\"\n",
        "            else:\n",
        "                overall[\"label\"] = \"neutral\"\n",
        "            data[\"overall\"] = overall\n",
        "\n",
        "        return data, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"RECOVERY: Parsing Failed Base Llama Outputs\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Check if failed file exists\n",
        "    try:\n",
        "        with open(\"base_llama_failed.jsonl\", \"r\") as f:\n",
        "            failed_samples = [json.loads(line) for line in f]\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n❌ base_llama_failed.jsonl not found!\")\n",
        "        print(\"   Run benchmark_base_llama.py first\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n📁 Found {len(failed_samples)} failed samples\")\n",
        "    print(\"🔧 Attempting aggressive recovery...\\n\")\n",
        "\n",
        "    recovered = []\n",
        "    still_failed = []\n",
        "\n",
        "    for sample in tqdm(failed_samples, desc=\"Recovering\"):\n",
        "        raw_output = sample.get(\"raw_output\", \"\")\n",
        "        fixed_data, error = aggressive_json_fix(raw_output)\n",
        "\n",
        "        if fixed_data:\n",
        "            recovered.append({\n",
        "                \"comment_id\": sample.get(\"comment_id\"),\n",
        "                \"text\": sample.get(\"text\"),\n",
        "                \"expected\": sample.get(\"expected\"),\n",
        "                \"predicted\": fixed_data,\n",
        "                \"raw_output\": raw_output,\n",
        "                \"success\": True\n",
        "            })\n",
        "        else:\n",
        "            still_failed.append({\n",
        "                **sample,\n",
        "                \"recovery_error\": error\n",
        "            })\n",
        "\n",
        "    # Save recovered samples\n",
        "    if recovered:\n",
        "        with open(\"base_llama_recovered.jsonl\", \"w\") as f:\n",
        "            for item in recovered:\n",
        "                f.write(json.dumps(item) + \"\\n\")\n",
        "        print(f\"\\n✅ Recovered {len(recovered)} samples → base_llama_recovered.jsonl\")\n",
        "\n",
        "    if still_failed:\n",
        "        with open(\"base_llama_still_failed.jsonl\", \"w\") as f:\n",
        "            for item in still_failed:\n",
        "                f.write(json.dumps(item) + \"\\n\")\n",
        "        print(f\"⚠️  Still failed: {len(still_failed)} samples → base_llama_still_failed.jsonl\")\n",
        "\n",
        "    # Calculate recovery rate\n",
        "    recovery_rate = len(recovered) / len(failed_samples) if failed_samples else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RECOVERY SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Original failures:      {len(failed_samples)}\")\n",
        "    print(f\"Successfully recovered: {len(recovered)} ({recovery_rate:.1%})\")\n",
        "    print(f\"Still failed:           {len(still_failed)} ({(1-recovery_rate):.1%})\")\n",
        "\n",
        "    if recovery_rate < 0.5:\n",
        "        print(\"\\n⚠️  Recovery rate < 50%\")\n",
        "        print(\"   The base Llama model struggles with this JSON format.\")\n",
        "        print(\"   Consider skipping base model benchmark.\")\n",
        "    elif recovery_rate < 0.9:\n",
        "        print(\"\\n✓ Partial recovery successful\")\n",
        "        print(\"  You can combine base_llama_predictions.jsonl + base_llama_recovered.jsonl\")\n",
        "    else:\n",
        "        print(\"\\n✅ High recovery rate!\")\n",
        "        print(\"   Merge recovered samples with predictions for evaluation\")\n",
        "\n",
        "    # If recovery rate is decent, create merged file\n",
        "    if recovery_rate >= 0.5:\n",
        "        print(\"\\n🔗 Creating merged predictions file...\")\n",
        "        try:\n",
        "            # Load original predictions\n",
        "            with open(\"base_llama_predictions.jsonl\", \"r\") as f:\n",
        "                original = [json.loads(line) for line in f]\n",
        "\n",
        "            # Merge: keep successful from original, add recovered\n",
        "            merged = [p for p in original if p.get(\"success\")] + recovered\n",
        "\n",
        "            with open(\"base_llama_predictions_merged.jsonl\", \"w\") as f:\n",
        "                for item in merged:\n",
        "                    f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "            print(f\"✅ Created base_llama_predictions_merged.jsonl ({len(merged)} samples)\")\n",
        "            print(\"   Use this file for evaluation\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Could not create merged file: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTHXcuOZD3fB",
        "outputId": "437c1bc4-9d8f-44e5-eb81-b9482563c7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RECOVERY: Parsing Failed Base Llama Outputs\n",
            "============================================================\n",
            "\n",
            "📁 Found 2340 failed samples\n",
            "🔧 Attempting aggressive recovery...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Recovering: 100%|██████████| 2340/2340 [00:00<00:00, 37369.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Still failed: 2340 samples → base_llama_still_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "RECOVERY SUMMARY\n",
            "============================================================\n",
            "Original failures:      2340\n",
            "Successfully recovered: 0 (0.0%)\n",
            "Still failed:           2340 (100.0%)\n",
            "\n",
            "⚠️  Recovery rate < 50%\n",
            "   The base Llama model struggles with this JSON format.\n",
            "   Consider skipping base model benchmark.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_score, recall_score,\n",
        "    mean_absolute_error, mean_squared_error,\n",
        "    hamming_loss, accuracy_score, classification_report\n",
        ")\n",
        "from scipy.stats import spearmanr\n",
        "import os\n",
        "\n",
        "# ============================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def to_bool(x):\n",
        "    \"\"\"Robust boolean coercion.\"\"\"\n",
        "    if isinstance(x, bool): return x\n",
        "    if isinstance(x, (int, float)): return bool(int(x))\n",
        "    if isinstance(x, str): return x.strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"t\"}\n",
        "    return False\n",
        "\n",
        "def derive_label_from_score(score: float) -> str:\n",
        "    \"\"\"Derive label using the same logic as training data.\"\"\"\n",
        "    if score > 0.5: return \"hateful\"\n",
        "    if score < -1.0: return \"supportive\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def normalize_schema(data):\n",
        "    \"\"\"Ensure data conforms to required types, handling dot notation.\"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    # Handle flat dot notation\n",
        "    if \"overall.label\" in data or \"overall.hate_speech_score\" in data:\n",
        "        flat_data = data.copy()\n",
        "        data = {\"overall\": {}, \"facets\": {}, \"targets\": {}}\n",
        "\n",
        "        for key, value in flat_data.items():\n",
        "            if key.startswith(\"overall.\"):\n",
        "                field = key.replace(\"overall.\", \"\")\n",
        "                data[\"overall\"][field] = value\n",
        "            elif key == \"facets\":\n",
        "                data[\"facets\"] = value\n",
        "            elif key == \"targets\":\n",
        "                data[\"targets\"] = value\n",
        "\n",
        "    overall = data.get(\"overall\", {})\n",
        "    if \"score\" in overall and \"hate_speech_score\" not in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall.pop(\"score\"))\n",
        "    if \"hate_speech_score\" in overall:\n",
        "        overall[\"hate_speech_score\"] = float(overall[\"hate_speech_score\"])\n",
        "\n",
        "    # Fix wrong label values\n",
        "    if \"label\" in overall:\n",
        "        label = overall[\"label\"]\n",
        "        if label == \"hate_speech\":\n",
        "            overall[\"label\"] = \"hateful\"\n",
        "        elif label not in [\"hateful\", \"neutral\", \"supportive\"]:\n",
        "            if \"hate_speech_score\" in overall:\n",
        "                overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "\n",
        "    if \"label\" not in overall and \"hate_speech_score\" in overall:\n",
        "        overall[\"label\"] = derive_label_from_score(overall[\"hate_speech_score\"])\n",
        "    data[\"overall\"] = overall\n",
        "\n",
        "    facets = data.get(\"facets\", {})\n",
        "    fixed_facets = {}\n",
        "    for key, value in facets.items():\n",
        "        try:\n",
        "            fixed_facets[key] = max(0, min(4, int(float(value))))\n",
        "        except (ValueError, TypeError):\n",
        "            fixed_facets[key] = 0\n",
        "    data[\"facets\"] = fixed_facets\n",
        "\n",
        "    targets = data.get(\"targets\", {})\n",
        "    data[\"targets\"] = {key: to_bool(value) for key, value in targets.items()}\n",
        "\n",
        "    return data\n",
        "\n",
        "def safe_get_score(obj):\n",
        "    \"\"\"Safely extract score from overall.\"\"\"\n",
        "    overall = obj.get(\"overall\", {})\n",
        "    if \"hate_speech_score\" in overall: return float(overall[\"hate_speech_score\"])\n",
        "    if \"score\" in overall: return float(overall[\"score\"])\n",
        "    raise KeyError(\"Neither 'hate_speech_score' nor 'score' found in overall\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# LOAD DATA\n",
        "# ============================================\n",
        "\n",
        "# Auto-detect which predictions file to use\n",
        "if os.path.exists(\"base_llama_predictions_merged.jsonl\"):\n",
        "    PREDICTIONS_FILE = \"base_llama_predictions_merged.jsonl\"\n",
        "    print(\"🔗 Using merged predictions (includes recovered samples)\")\n",
        "elif os.path.exists(\"base_llama_predictions.jsonl\"):\n",
        "    PREDICTIONS_FILE = \"base_llama_predictions.jsonl\"\n",
        "else:\n",
        "    print(\"❌ No predictions file found!\")\n",
        "    print(\"   Run benchmark_base_llama.py first\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BASE LLAMA-3.2-1B-INSTRUCT EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n📁 Loading predictions from: {PREDICTIONS_FILE}\")\n",
        "\n",
        "predictions = []\n",
        "with open(PREDICTIONS_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        predictions.append(json.loads(line))\n",
        "\n",
        "# Separate successful and failed predictions\n",
        "valid_preds = [p for p in predictions if p.get(\"success\")]\n",
        "failed_samples = [p for p in predictions if not p.get(\"success\")]\n",
        "\n",
        "# Re-normalize both expected and predicted data\n",
        "for p in valid_preds:\n",
        "    p[\"expected\"] = normalize_schema(p[\"expected\"])\n",
        "    p[\"predicted\"] = normalize_schema(p[\"predicted\"])\n",
        "\n",
        "print(f\"\\n📊 Data Summary:\")\n",
        "print(f\"   Total samples:        {len(predictions)}\")\n",
        "print(f\"   Valid predictions:    {len(valid_preds)} ({100*len(valid_preds)/len(predictions):.1f}%)\")\n",
        "print(f\"   Failed predictions:   {len(failed_samples)} ({100*len(failed_samples)/len(predictions):.1f}%)\")\n",
        "\n",
        "if not valid_preds:\n",
        "    print(\"\\n❌ No valid predictions found. Cannot evaluate.\")\n",
        "    exit(1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 1. OVERALL LABEL CLASSIFICATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"OVERALL: Label Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_true_labels = [p[\"expected\"][\"overall\"][\"label\"] for p in valid_preds]\n",
        "y_pred_labels = [p[\"predicted\"][\"overall\"][\"label\"] for p in valid_preds]\n",
        "\n",
        "overall_accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "overall_micro_f1 = f1_score(y_true_labels, y_pred_labels, average=\"micro\", zero_division=0)\n",
        "overall_macro_f1 = f1_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "overall_precision = precision_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "overall_recall = recall_score(y_true_labels, y_pred_labels, average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"Accuracy:        {overall_accuracy:.4f}\")\n",
        "print(f\"Macro F1:        {overall_macro_f1:.4f}\")\n",
        "print(f\"Micro F1:        {overall_micro_f1:.4f}\")\n",
        "print(f\"Macro Precision: {overall_precision:.4f}\")\n",
        "print(f\"Macro Recall:    {overall_recall:.4f}\")\n",
        "print(\"\\nPer-class breakdown:\")\n",
        "print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n",
        "\n",
        "# Score correlation\n",
        "y_true_scores = [safe_get_score(p[\"expected\"]) for p in valid_preds]\n",
        "y_pred_scores = [safe_get_score(p[\"predicted\"]) for p in valid_preds]\n",
        "score_corr = spearmanr(y_true_scores, y_pred_scores).correlation\n",
        "print(f\"Score Spearman correlation: {score_corr:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. FACETS EVALUATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FACETS: Ordinal Ratings (0-4 scale)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "facet_names = list(valid_preds[0][\"expected\"][\"facets\"].keys())\n",
        "facet_results = {}\n",
        "\n",
        "for facet in facet_names:\n",
        "    y_true = np.array([p[\"expected\"][\"facets\"].get(facet, 0) for p in valid_preds])\n",
        "    y_pred = np.array([p[\"predicted\"][\"facets\"].get(facet, 0) for p in valid_preds])\n",
        "\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    spearman = spearmanr(y_true, y_pred).correlation\n",
        "    exact_match = accuracy_score(y_true, y_pred)\n",
        "    within_1 = np.mean(np.abs(y_true - y_pred) <= 1)\n",
        "\n",
        "    facet_results[facet] = {\n",
        "        \"mae\": mae, \"mse\": mse, \"spearman\": spearman,\n",
        "        \"exact_match\": exact_match, \"within_1_accuracy\": within_1\n",
        "    }\n",
        "\n",
        "mean_mae = np.mean([r[\"mae\"] for r in facet_results.values()])\n",
        "mean_mse = np.mean([r[\"mse\"] for r in facet_results.values()])\n",
        "mean_spearman = np.mean([r[\"spearman\"] for r in facet_results.values()])\n",
        "mean_exact = np.mean([r[\"exact_match\"] for r in facet_results.values()])\n",
        "mean_within_1 = np.mean([r[\"within_1_accuracy\"] for r in facet_results.values()])\n",
        "\n",
        "print(f\"Mean MAE:               {mean_mae:.4f}\")\n",
        "print(f\"Mean MSE:               {mean_mse:.4f}\")\n",
        "print(f\"Mean Spearman:          {mean_spearman:.4f}\")\n",
        "print(f\"Mean Exact Match:       {mean_exact:.4f}\")\n",
        "print(f\"Mean Within-1 Accuracy: {mean_within_1:.4f}\")\n",
        "\n",
        "print(\"\\nPer-facet breakdown:\")\n",
        "print(f\"{'Facet':<20} {'MAE':<8} {'Exact':<8} {'Within-1':<10} {'Spearman':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for facet in facet_names:\n",
        "    r = facet_results[facet]\n",
        "    print(f\"{facet:<20} {r['mae']:<8.3f} {r['exact_match']:<8.3f} {r['within_1_accuracy']:<10.3f} {r['spearman']:<10.3f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3. TARGETS EVALUATION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TARGETS: Multi-label Classification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "target_names = list(valid_preds[0][\"expected\"][\"targets\"].keys())\n",
        "\n",
        "y_true_targets = np.array([[int(p[\"expected\"][\"targets\"].get(t, False)) for t in target_names] for p in valid_preds])\n",
        "y_pred_targets = np.array([[int(p[\"predicted\"][\"targets\"].get(t, False)) for t in target_names] for p in valid_preds])\n",
        "\n",
        "targets_micro_f1 = f1_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_macro_f1 = f1_score(y_true_targets, y_pred_targets, average=\"macro\", zero_division=0)\n",
        "targets_micro_precision = precision_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_micro_recall = recall_score(y_true_targets, y_pred_targets, average=\"micro\", zero_division=0)\n",
        "targets_hamming = hamming_loss(y_true_targets, y_pred_targets)\n",
        "exact_match_ratio = np.mean(np.all(y_true_targets == y_pred_targets, axis=1))\n",
        "\n",
        "print(f\"Micro F1:          {targets_micro_f1:.4f}\")\n",
        "print(f\"Macro F1:          {targets_macro_f1:.4f}\")\n",
        "print(f\"Micro Precision:   {targets_micro_precision:.4f}\")\n",
        "print(f\"Micro Recall:      {targets_micro_recall:.4f}\")\n",
        "print(f\"Hamming Loss:      {targets_hamming:.4f}\")\n",
        "print(f\"Exact Match Ratio: {exact_match_ratio:.4f} ({int(exact_match_ratio*len(valid_preds))}/{len(valid_preds)})\")\n",
        "\n",
        "print(\"\\nPer-target F1 scores (bottom 10):\")\n",
        "per_target_f1 = {target: f1_score(y_true_targets[:, i], y_pred_targets[:, i], zero_division=0) for i, target in enumerate(target_names)}\n",
        "sorted_targets = sorted(per_target_f1.items(), key=lambda x: x[1])\n",
        "for target, f1 in sorted_targets[:10]:\n",
        "    print(f\"  {target:<40} {f1:.3f}\")\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# SAVE EVALUATION SUMMARY\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "eval_summary = {\n",
        "    \"metadata\": {\n",
        "        \"model\": \"meta-llama/Llama-3.2-1B-Instruct (base)\",\n",
        "        \"prediction_file\": PREDICTIONS_FILE,\n",
        "        \"total_samples\": len(predictions),\n",
        "        \"valid_predictions\": len(valid_preds),\n",
        "        \"failed_predictions\": len(failed_samples),\n",
        "        \"success_rate\": len(valid_preds) / len(predictions) if predictions else 0\n",
        "    },\n",
        "    \"overall\": {\n",
        "        \"accuracy\": overall_accuracy,\n",
        "        \"macro_f1\": overall_macro_f1,\n",
        "        \"micro_f1\": overall_micro_f1,\n",
        "        \"precision\": overall_precision,\n",
        "        \"recall\": overall_recall,\n",
        "        \"score_spearman\": score_corr\n",
        "    },\n",
        "    \"facets\": {\n",
        "        \"mean_mae\": mean_mae,\n",
        "        \"mean_mse\": mean_mse,\n",
        "        \"mean_spearman\": mean_spearman,\n",
        "        \"mean_exact_match\": mean_exact,\n",
        "        \"mean_within_1_accuracy\": mean_within_1,\n",
        "        \"per_facet\": facet_results\n",
        "    },\n",
        "    \"targets\": {\n",
        "        \"micro_f1\": targets_micro_f1,\n",
        "        \"macro_f1\": targets_macro_f1,\n",
        "        \"precision\": targets_micro_precision,\n",
        "        \"recall\": targets_micro_recall,\n",
        "        \"hamming_loss\": targets_hamming,\n",
        "        \"exact_match_ratio\": exact_match_ratio,\n",
        "        \"per_target_f1\": per_target_f1\n",
        "    }\n",
        "}\n",
        "\n",
        "output_file = \"base_llama_evaluation.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(eval_summary, f, indent=2)\n",
        "\n",
        "print(f\"✅ Summary saved to: {output_file}\")\n",
        "if failed_samples:\n",
        "    print(f\"⚠️  {len(failed_samples)} samples failed - check base_llama_failed.jsonl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Print key metrics summary\n",
        "print(\"\\n📈 KEY METRICS SUMMARY:\")\n",
        "print(f\"   Overall Accuracy:  {overall_accuracy:.2%}\")\n",
        "print(f\"   Overall Macro F1:  {overall_macro_f1:.4f}\")\n",
        "print(f\"   Facets Mean MAE:   {mean_mae:.4f}\")\n",
        "print(f\"   Targets Micro F1:  {targets_micro_f1:.4f}\")\n",
        "print(f\"   Success Rate:      {len(valid_preds)/len(predictions):.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXHiuXf7EGyK",
        "outputId": "1be91035-988c-4841-ad80-4255d6900ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASE LLAMA-3.2-1B-INSTRUCT EVALUATION\n",
            "============================================================\n",
            "\n",
            "📁 Loading predictions from: base_llama_predictions.jsonl\n",
            "\n",
            "📊 Data Summary:\n",
            "   Total samples:        3957\n",
            "   Valid predictions:    1617 (40.9%)\n",
            "   Failed predictions:   2340 (59.1%)\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "OVERALL: Label Classification\n",
            "============================================================\n",
            "Accuracy:        0.3531\n",
            "Macro F1:        0.2767\n",
            "Micro F1:        0.3531\n",
            "Macro Precision: 0.3851\n",
            "Macro Recall:    0.4295\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     hateful       0.23      0.99      0.38       310\n",
            "     neutral       0.00      0.00      0.00       428\n",
            "  supportive       0.92      0.30      0.45       879\n",
            "\n",
            "    accuracy                           0.35      1617\n",
            "   macro avg       0.39      0.43      0.28      1617\n",
            "weighted avg       0.55      0.35      0.32      1617\n",
            "\n",
            "Score Spearman correlation: 0.3844\n",
            "\n",
            "============================================================\n",
            "FACETS: Ordinal Ratings (0-4 scale)\n",
            "============================================================\n",
            "Mean MAE:               1.5341\n",
            "Mean MSE:               3.9827\n",
            "Mean Spearman:          0.1247\n",
            "Mean Exact Match:       0.2854\n",
            "Mean Within-1 Accuracy: 0.5027\n",
            "\n",
            "Per-facet breakdown:\n",
            "Facet                MAE      Exact    Within-1   Spearman  \n",
            "------------------------------------------------------------\n",
            "sentiment            2.184    0.097    0.278      0.117     \n",
            "respect              2.549    0.082    0.181      -0.192    \n",
            "insult               1.050    0.339    0.735      0.481     \n",
            "humiliate            1.853    0.179    0.378      0.099     \n",
            "status               2.427    0.006    0.041      0.073     \n",
            "dehumanize           1.439    0.250    0.523      0.085     \n",
            "violence             0.627    0.604    0.837      0.080     \n",
            "genocide             0.361    0.768    0.912      0.075     \n",
            "attack_defend        2.292    0.052    0.168      0.089     \n",
            "hatespeech           0.558    0.477    0.975      0.340     \n",
            "\n",
            "============================================================\n",
            "TARGETS: Multi-label Classification\n",
            "============================================================\n",
            "Micro F1:          0.0904\n",
            "Macro F1:          0.0565\n",
            "Micro Precision:   0.7736\n",
            "Micro Recall:      0.0480\n",
            "Hamming Loss:      0.0555\n",
            "Exact Match Ratio: 0.0501 (81/1617)\n",
            "\n",
            "Per-target F1 scores (bottom 10):\n",
            "  target_race_pacific_islander             0.000\n",
            "  target_race_other                        0.000\n",
            "  target_religion_atheist                  0.000\n",
            "  target_religion_buddhist                 0.000\n",
            "  target_religion_other                    0.000\n",
            "  target_origin_migrant_worker             0.000\n",
            "  target_origin_specific_country           0.000\n",
            "  target_origin_undocumented               0.000\n",
            "  target_origin_other                      0.000\n",
            "  target_gender_non_binary                 0.000\n",
            "\n",
            "============================================================\n",
            "SAVING EVALUATION SUMMARY\n",
            "============================================================\n",
            "✅ Summary saved to: base_llama_evaluation.json\n",
            "⚠️  2340 samples failed - check base_llama_failed.jsonl\n",
            "\n",
            "============================================================\n",
            "EVALUATION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "📈 KEY METRICS SUMMARY:\n",
            "   Overall Accuracy:  35.31%\n",
            "   Overall Macro F1:  0.2767\n",
            "   Facets Mean MAE:   1.5341\n",
            "   Targets Micro F1:  0.0904\n",
            "   Success Rate:      40.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpJ5gBk3Euac"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}