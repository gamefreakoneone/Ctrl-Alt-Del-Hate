(OPTMISED VERSION USING EPOCH2'S RESULTS)

 Starting Llama LoRA Testing Pipeline
============================================================
STEP 1: Merging LoRA adapter with base Llama model
============================================================

 Loading base model...

 Loading LoRA adapter...

 Merging adapter with base model...

 Saving merged model...

 Merged model saved to: llama-1B-merged-r32
   Now you can use this with vLLM!
   

============================================================
VLLM INFERENCE (MERGED LLAMA MODEL)
============================================================

 Loading merged model with vLLM...
INFO 11-18 01:59:53 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 4096, 'disable_log_stats': True, 'model': 'llama-1B-merged-r32'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 11-18 01:59:53 [model.py:547] Resolved architecture: LlamaForCausalLM
INFO 11-18 01:59:53 [model.py:1510] Using max model len 4096
INFO 11-18 01:59:58 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 11-18 01:59:59 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 11-18 02:00:45 [llm.py:306] Supported_tasks: ['generate']

 Loading test data...
Generating train split: 
 3957/0 [00:00<00:00, 116173.26 examples/s]
Test dataset size: 3957

 Preparing prompts...

 Running inference on 3957 samples...
Adding requests: 100%
 3957/3957 [00:12<00:00, 342.69it/s]
Processed prompts: 100%
 3957/3957 [02:29<00:00, 33.09it/s, est. speed input: 31442.78 toks/s, output: 14226.82 toks/s]

 Processing results...
Processing outputs: 100%|██████████| 3957/3957 [00:01<00:00, 2846.98it/s]

 Saving results...

 Inference complete!
   Total: 3957
   Success: 3957 (100.0%)
   Failed: 0

============================================================
 PIPELINE COMPLETE!
============================================================

 Output files created:
   - llama_test_predictions_vllm.jsonl
   - llama_failed_predictions_vllm.jsonl (if any failures)

 Next step: Run evaluation script on llama_test_predictions_vllm.jsonl
 
 
============================================================
LLAMA HATE SPEECH MODEL EVALUATION
============================================================

 Using predictions from: llama_test_predictions_vllm.jsonl

 Data Summary:
   Total samples: 3957
    Valid predictions: 3957 (100.0%)
    Failed predictions: 0 (0.0%)

============================================================
EVALUATION RESULTS
============================================================

============================================================
1. OVERALL: Label Classification
============================================================
Accuracy:        0.6737
Macro F1:        0.6284
Micro F1:        0.6737
Macro Precision: 0.6405
Macro Recall:    0.6357

Per-class breakdown:
              precision    recall  f1-score   support

     hateful       0.69      0.70      0.69      1053
     neutral       0.51      0.33      0.40      1137
  supportive       0.72      0.88      0.79      1767

    accuracy                           0.67      3957
   macro avg       0.64      0.64      0.63      3957
weighted avg       0.65      0.67      0.65      3957

Score Spearman correlation: 0.8056

============================================================
2. FACETS: Ordinal Ratings (0-4 scale)
============================================================
Mean MAE:               0.5462
Mean MSE:               0.7377
Mean Spearman:          0.5852
Mean Exact Match:       0.5429
Mean Within-1 Accuracy: 0.9169

Per-facet breakdown:
Facet                MAE      Exact    Within-1   Spearman  
------------------------------------------------------------
sentiment            0.493    0.561    0.948      0.717     
respect              0.566    0.511    0.928      0.707     
insult               0.700    0.434    0.879      0.669     
humiliate            0.739    0.405    0.865      0.616     
status               0.422    0.604    0.974      0.520     
dehumanize           0.855    0.348    0.804      0.454     
violence             0.536    0.567    0.909      0.512     
genocide             0.313    0.753    0.944      0.423     
attack_defend        0.499    0.555    0.949      0.604     
hatespeech           0.340    0.690    0.969      0.631     

============================================================
3. TARGETS: Multi-label Classification
============================================================
Micro F1:          0.5666
Macro F1:          0.3460
Micro Precision:   0.7723
Micro Recall:      0.4474
Hamming Loss:      0.0386
Exact Match Ratio: 0.3624 (1434/3957)

Per-target F1 scores (bottom 10):
  target_gender_other                      0.000
  target_age_other                         0.000
  target_disability_visually_impaired      0.000
  target_disability_hearing_impaired       0.000
  target_origin_other                      0.025
  target_disability_unspecific             0.030
  target_age_middle_aged                   0.060
  target_disability_other                  0.063
  target_age_young_adults                  0.082
  target_religion_buddhist                 0.136

============================================================
SAVING EVALUATION SUMMARY
============================================================
 Summary saved to: llama_evaluation_summary.json

============================================================
EVALUATION COMPLETE!
============================================================

 KEY METRICS SUMMARY:
   Overall Accuracy: 67.37%
   Overall Macro F1: 0.6284
   Facets Mean MAE:  0.5462
   Targets Micro F1: 0.5666