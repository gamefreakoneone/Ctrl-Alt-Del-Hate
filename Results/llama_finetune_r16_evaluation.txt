============================================================
LLAMA HATE SPEECH MODEL EVALUATION
============================================================
 Using predictions from: llama_test_predictions_vllm.jsonl
 Data Summary:
   Total samples: 3957
    Valid predictions: 3956 (100.0%)
    Failed predictions: 1 (0.0%)
============================================================
EVALUATION RESULTS
============================================================
============================================================
1. OVERALL: Label Classification
============================================================
Accuracy:        0.6729
Macro F1:        0.6441
Micro F1:        0.6729
Macro Precision: 0.6429
Macro Recall:    0.6481
Per-class breakdown:
              precision    recall  f1-score   support
     hateful       0.67      0.71      0.69      1052
     neutral       0.49      0.42      0.45      1137
  supportive       0.77      0.81      0.79      1767
    accuracy                           0.67      3956
   macro avg       0.64      0.65      0.64      3956
weighted avg       0.66      0.67      0.67      3956
Score Spearman correlation: 0.7979
============================================================
2. FACETS: Ordinal Ratings (0-4 scale)
============================================================
Mean MAE:               0.5321
Mean MSE:               0.7220
Mean Spearman:          0.5941
Mean Exact Match:       0.5549
Mean Within-1 Accuracy: 0.9204
Per-facet breakdown:
Facet                MAE      Exact    Within-1   Spearman
------------------------------------------------------------
sentiment            0.460    0.592    0.951      0.721
respect              0.538    0.533    0.935      0.703
insult               0.652    0.463    0.897      0.680
humiliate            0.683    0.434    0.891      0.644
status               0.447    0.588    0.965      0.503
dehumanize           0.820    0.369    0.826      0.530
violence             0.541    0.572    0.901      0.516
genocide             0.315    0.754    0.940      0.420
attack_defend        0.509    0.547    0.948      0.598
hatespeech           0.355    0.696    0.949      0.628
============================================================
3. TARGETS: Multi-label Classification
============================================================
Micro F1:          0.5931
Macro F1:          0.3856
Micro Precision:   0.7480
Micro Recall:      0.4913
Hamming Loss:      0.0380
Exact Match Ratio: 0.3587 (1419/3956)
Per-target F1 scores (bottom 10):
  target_origin_other                      0.000
  target_gender_other                      0.000
  target_age_other                         0.000
  target_disability_visually_impaired      0.000
  target_disability_hearing_impaired       0.000
  target_disability_other                  0.000
  target_disability_unspecific             0.079
  target_age_young_adults                  0.101
  target_age_middle_aged                   0.114
  target_disability_physical               0.160
============================================================
SAVING EVALUATION SUMMARY
============================================================
 Summary saved to: llama_evaluation_summary.json
  1 samples failed - check llama_failed_predictions_vllm.jsonl
============================================================
EVALUATION COMPLETE!
============================================================
 KEY METRICS SUMMARY:
   Overall Accuracy: 67.29%
   Overall Macro F1: 0.6441
   Facets Mean MAE:  0.5321
   Targets Micro F1: 0.5931